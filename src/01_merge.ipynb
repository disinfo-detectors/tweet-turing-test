{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Turing Test: Detecting Disinformation on Twitter  \n",
    "\n",
    "|          | Group #2 - Disinformation Detectors                     |\n",
    "|---------:|---------------------------------------------------------|\n",
    "| Members  | John Johnson, Katy Matulay, Justin Minnion, Jared Rubin |\n",
    "| Notebook | `01_merge.ipynb`                                        |\n",
    "| Purpose  | Merge together acquired data into a common format.      |\n",
    "\n",
    "Our acquired data is stored in the following directory structure:  \n",
    "- `/` (root project directory)\n",
    "    - `/data`\n",
    "        - `/data/raw`\n",
    "            - `/data/raw/tweets-govt_entities`\n",
    "            - `/data/raw/tweets-inidviduals`\n",
    "            - `/data/raw/tweets-news_orgs`\n",
    "            - `/data/raw/tweets-random`\n",
    "        - `/data/processed`\n",
    "            - `/data/processed/...`\n",
    "\n",
    "> (TODO - write more explaining notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from Python standard library\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# imports requiring installation\n",
    "#   connection to Google Cloud Storage\n",
    "from google.cloud import storage            # pip install google-cloud-storage\n",
    "from google.oauth2 import service_account   # pip install google-auth\n",
    "\n",
    "#  data science packages\n",
    "import demoji                               # pip install demoji\n",
    "import numpy as np                          # pip install numpy\n",
    "import pandas as pd                         # pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from tweet_turing.py\n",
    "from tweet_turing import get_json_files, merge_json_files, get_csv_files, merge_csv_files, \\\n",
    "    get_gcp_storage_client, get_gcp_bucket, list_gcp_objects, get_gcp_object_as_json, \\\n",
    "    get_gcp_object_as_text, merge_gcp_json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup location of data files\n",
    "#   files when loading local data\n",
    "local_data_paths: dict[str, str] = {\n",
    "    \"govt_entities\": \"../data/raw/tweets-govt_entities/\",\n",
    "    \"individuals\": \"../data/raw/tweets-individuals/\",\n",
    "    \"news_orgs\": \"../data/raw/tweets-news_orgs/\",\n",
    "    \"random\": \"../data/raw/tweets-random/\",\n",
    "    \"troll\": \"../data/raw/tweets-troll\"\n",
    "}\n",
    "\n",
    "local_snapshot_paths: dict[str, str] = {\n",
    "    \"json_snapshot\": \"../data/snapshot/\",\n",
    "    \"csv_snapshot\": \"../data/snapshot/\"\n",
    "}\n",
    "\n",
    "#   files when loading data from GCP bucket\n",
    "gcp_data_paths: dict[str, str] = {\n",
    "    \"govt_entities\": \"raw/govt/\",\n",
    "    \"individuals\": \"raw/individuals/\",\n",
    "    \"news_orgs\": \"raw/news/\",\n",
    "    \"random\": \"raw/random/\",\n",
    "    \"troll\": \"raw/troll/\"\n",
    "}\n",
    "\n",
    "gcp_snapshot_paths: dict[str, str] = {\n",
    "    \"json_snapshot\": \"snapshot/\",\n",
    "    \"csv_snapshot\": \"snapshot/\"\n",
    "}\n",
    "\n",
    "gcp_project_name: str = \"ds-capstone-jmmr\"\n",
    "gcp_bucket_name: str = \"disinfo-detector-tweet-turing-test\"\n",
    "gcp_key_file: str = \"../key/service_acct_key.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local or Cloud?\n",
    "Decide here whether to run notebook with local data or GCP bucket data\n",
    " - if the working directory of this notebook has a \"../data/\" folder with data loaded (e.g. working on local computer or have data files loaded to a cloud VM) then use the \"local files\" option and comment out the \"gcp bucket files\" option\n",
    " - if this notebook is being run from a GCP VM (preferrably in the `us-central1` location) then use the \"gcp bucket files\" option and comment out the \"local files\" option "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option: local files\n",
    "local_or_cloud: str = \"local\"   # comment/uncomment this line or next\n",
    "\n",
    "# option: gcp bucket files\n",
    "#local_or_cloud: str = \"cloud\"   # comment/uncomment this line or previous\n",
    "\n",
    "# don't comment/uncomment for remainder of cell\n",
    "if (local_or_cloud == \"local\"):\n",
    "    data_paths: dict[str, str] = local_data_paths\n",
    "    snapshot_paths: dict[str, str] = local_snapshot_paths\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    data_paths: dict[str, str] = gcp_data_paths\n",
    "    snapshot_paths: dict[str, str] = gcp_snapshot_paths\n",
    "else:\n",
    "    raise ValueError(\"Variable 'local_or_cloud' can only take on one of two values, 'local' or 'cloud'.\")\n",
    "    # subsequent cells will not do this final \"else\" check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell only needs to run its code if local_or_cloud==\"cloud\"\n",
    "gcp_storage_client: storage.Client = None\n",
    "gcp_bucket: storage.Bucket = None\n",
    "\n",
    "if (local_or_cloud == \"cloud\"):\n",
    "    gcp_storage_client = get_gcp_storage_client(project_name=gcp_project_name, key_file=gcp_key_file)\n",
    "    gcp_bucket = get_gcp_bucket(storage_client=gcp_storage_client, bucket_name=gcp_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Troll Tweet CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Justin\\OneDrive - Drexel University\\Git\\tweet-turing-test\\src\\tweet_turing.py:120: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  new_df: pd.DataFrame = pd.read_csv(file, encoding='utf-8')\n",
      "c:\\Users\\Justin\\OneDrive - Drexel University\\Git\\tweet-turing-test\\src\\tweet_turing.py:120: DtypeWarning: Columns (10,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  new_df: pd.DataFrame = pd.read_csv(file, encoding='utf-8')\n",
      "c:\\Users\\Justin\\OneDrive - Drexel University\\Git\\tweet-turing-test\\src\\tweet_turing.py:120: DtypeWarning: Columns (0,15,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  new_df: pd.DataFrame = pd.read_csv(file, encoding='utf-8')\n"
     ]
    }
   ],
   "source": [
    "csv_filename: str = \"csv_snapshot.csv\"\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    # load list of filenames/paths\n",
    "    csv_file_list: list[str] = get_csv_files(data_paths[\"troll\"])\n",
    "\n",
    "    # merge\n",
    "    csv_df: pd.DataFrame = merge_csv_files(csv_file_list)\n",
    "\n",
    "    # save result to a new file\n",
    "    #   resource warning: file size is ~1.08 GB\n",
    "    csv_path: str = f\"{snapshot_paths['csv_snapshot']}{csv_filename}\"\n",
    "    csv_df.to_csv(csv_path, encoding='utf-8')\n",
    "    \n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    pass    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Twitter API JSON Files\n",
    "\n",
    "> TODO - explain difference between `verified_user` / `verified_random`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load JSON files to memory - nonrandom verified users\n",
    "json_groups_nonrandom: list[str] = ['govt_entities', 'individuals', 'news_orgs']\n",
    "\n",
    "json_data_nonrandom: list[dict] = []\n",
    "\n",
    "for this_group in json_groups_nonrandom:\n",
    "    if (local_or_cloud == \"local\"):\n",
    "        this_file_list: list[str] = get_json_files(data_paths[this_group])\n",
    "        this_json_data: list[dict] = merge_json_files(this_file_list)\n",
    "    elif (local_or_cloud == \"cloud\"):\n",
    "        this_obj_list: list[str] = list_gcp_objects(\n",
    "            storage_client=gcp_storage_client, \n",
    "            bucket_name=gcp_bucket_name, \n",
    "            obj_prefix=data_paths[this_group]\n",
    "            )\n",
    "        this_json_data: list[dict] = merge_gcp_json_files(gcp_bucket, this_obj_list)\n",
    "    \n",
    "    json_data_nonrandom.extend(this_json_data)\n",
    "\n",
    "# apply data_source label for these groups\n",
    "for tweet in json_data_nonrandom:\n",
    "    tweet['data_source'] = \"verified_user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load JSON files to memory - random verified users\n",
    "json_groups_random: list[str] = ['random']\n",
    "\n",
    "json_data_random: list[dict] = []\n",
    "\n",
    "for this_group in json_groups_random:\n",
    "    if (local_or_cloud == \"local\"):\n",
    "        this_file_list: list[str] = get_json_files(data_paths[this_group])\n",
    "        this_json_data: list[dict] = merge_json_files(this_file_list)\n",
    "    elif (local_or_cloud == \"cloud\"):\n",
    "        this_obj_list: list[str] = list_gcp_objects(\n",
    "            storage_client=gcp_storage_client, \n",
    "            bucket_name=gcp_bucket_name, \n",
    "            obj_prefix=data_paths[this_group]\n",
    "            )\n",
    "        this_json_data: list[dict] = merge_gcp_json_files(gcp_bucket, this_obj_list)\n",
    "    \n",
    "    json_data_random.extend(this_json_data)\n",
    "\n",
    "# apply data_source label for these groups\n",
    "for tweet in json_data_random:\n",
    "    tweet['data_source'] = \"verified_random\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with `data_source` labels applied, merge the two lists with JSON data\n",
    "json_data = []\n",
    "json_data.extend(json_data_nonrandom)\n",
    "json_data.extend(json_data_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result to a new file\n",
    "#   resource warning: file size is ~3.67 GB\n",
    "json_filename: str = \"json_snapshot.json\"\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    json_path: str = f\"{snapshot_paths['json_snapshot']}{json_filename}\"\n",
    "\n",
    "    with open(file=json_path, mode=\"w\", encoding='utf-8') as json_fh:\n",
    "        json.dump(json_data, json_fh)\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    pass    # TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6350edbffa5f4617867057bfd50f00ea9376d607df54ae8d843647e02695eca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
