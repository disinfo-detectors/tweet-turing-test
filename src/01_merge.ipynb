{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Turing Test: Detecting Disinformation on Twitter  \n",
    "\n",
    "|          | Group #2 - Disinformation Detectors                     |\n",
    "|---------:|---------------------------------------------------------|\n",
    "| Members  | John Johnson, Katy Matulay, Justin Minnion, Jared Rubin |\n",
    "| Notebook | `01_merge.ipynb`                                        |\n",
    "| Purpose  | Merge together acquired data into a common format.      |\n",
    "\n",
    "Our acquired data is stored in the following directory structure:  \n",
    "- `/` (root project directory)\n",
    "    - `/data`\n",
    "        - `/data/raw`\n",
    "            - `/data/raw/tweets-govt_entities`\n",
    "            - `/data/raw/tweets-inidviduals`\n",
    "            - `/data/raw/tweets-news_orgs`\n",
    "            - `/data/raw/tweets-random`\n",
    "        - `/data/processed`\n",
    "            - `/data/processed/...`\n",
    "\n",
    "> (TODO - write more explaining notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from Python standard library\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# imports requiring installation\n",
    "#   connection to Google Cloud Storage\n",
    "from google.cloud import storage            # pip install google-cloud-storage\n",
    "from google.oauth2 import service_account   # pip install google-auth\n",
    "\n",
    "#  data science packages\n",
    "import demoji                               # pip install demoji\n",
    "import numpy as np                          # pip install numpy\n",
    "import pandas as pd                         # pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from tweet_turing.py\n",
    "from tweet_turing import get_json_files, merge_json_files, get_csv_files, merge_csv_files, \\\n",
    "    get_gcp_storage_client, get_gcp_bucket, list_gcp_objects, get_gcp_object_as_json, \\\n",
    "    get_gcp_object_as_text, set_gcp_object_from_json, merge_gcp_json_files, merge_gcp_csv_files, \\\n",
    "    set_gcp_object_from_df_as_parq\n",
    "\n",
    "# imports from tweet_turing_paths.py\n",
    "from tweet_turing_paths import local_data_paths, local_snapshot_paths, gcp_data_paths, \\\n",
    "    gcp_snapshot_paths, gcp_project_name, gcp_bucket_name, gcp_key_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local or Cloud?\n",
    "Decide here whether to run notebook with local data or GCP bucket data\n",
    " - if the working directory of this notebook has a \"../data/\" folder with data loaded (e.g. working on local computer or have data files loaded to a cloud VM) then use the \"local files\" option and comment out the \"gcp bucket files\" option\n",
    " - if this notebook is being run from a GCP VM (preferrably in the `us-central1` location) then use the \"gcp bucket files\" option and comment out the \"local files\" option "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option: local files\n",
    "#local_or_cloud: str = \"local\"   # comment/uncomment this line or next\n",
    "\n",
    "# option: gcp bucket files\n",
    "local_or_cloud: str = \"cloud\"   # comment/uncomment this line or previous\n",
    "\n",
    "# don't comment/uncomment for remainder of cell\n",
    "if (local_or_cloud == \"local\"):\n",
    "    data_paths = local_data_paths\n",
    "    snapshot_paths = local_snapshot_paths\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    data_paths = gcp_data_paths\n",
    "    snapshot_paths = gcp_snapshot_paths\n",
    "else:\n",
    "    raise ValueError(\"Variable 'local_or_cloud' can only take on one of two values, 'local' or 'cloud'.\")\n",
    "    # subsequent cells will not do this final \"else\" check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell only needs to run its code if local_or_cloud==\"cloud\"\n",
    "#   (though it is harmless if run when local_or_cloud==\"local\")\n",
    "gcp_storage_client: storage.Client = None\n",
    "gcp_bucket: storage.Bucket = None\n",
    "\n",
    "if (local_or_cloud == \"cloud\"):\n",
    "    gcp_storage_client = get_gcp_storage_client(project_name=gcp_project_name, key_file=gcp_key_file)\n",
    "    gcp_bucket = get_gcp_bucket(storage_client=gcp_storage_client, bucket_name=gcp_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Troll Tweet CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this cell requires package `pyarrow` to be installed in environment\n",
    "csv_filename: str = \"csv_snapshot.parquet.snappy\"\n",
    "csv_path: str = f\"{snapshot_paths['csv_snapshot']}{csv_filename}\"\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    # load list of filenames/paths\n",
    "    csv_file_list = get_csv_files(data_paths[\"troll\"])\n",
    "\n",
    "    # merge\n",
    "    csv_df: pd.DataFrame = merge_csv_files(csv_file_list)\n",
    "\n",
    "    # save result to a new file\n",
    "    csv_df.to_parquet(csv_path, engine='pyarrow', index=False)\n",
    "    \n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    # load list of filenames/paths\n",
    "    csv_file_list = list_gcp_objects(storage_client=gcp_storage_client, bucket_name=gcp_bucket_name, obj_prefix=data_paths[\"troll\"])\n",
    "    \n",
    "    # merge\n",
    "    csv_df: pd.DataFrame = merge_gcp_csv_files(gcp_bucket, csv_file_list)\n",
    "    \n",
    "    # save result to a new file\n",
    "    csv_blob: storage.Blob = gcp_bucket.blob(csv_path)\n",
    "    csv_df.to_parquet(csv_blob.open(\"wb\"), engine='pyarrow', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Twitter API JSON Files\n",
    "\n",
    "> TODO - explain difference between `verified_user` / `verified_random`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load JSON files to memory - nonrandom verified users\n",
    "json_groups_nonrandom = ['govt_entities', 'individuals', 'news_orgs']\n",
    "\n",
    "json_data_nonrandom = []\n",
    "\n",
    "for this_group in json_groups_nonrandom:\n",
    "    if (local_or_cloud == \"local\"):\n",
    "        this_file_list = get_json_files(data_paths[this_group])\n",
    "        this_json_data = merge_json_files(this_file_list)\n",
    "    elif (local_or_cloud == \"cloud\"):\n",
    "        this_obj_list = list_gcp_objects(\n",
    "            storage_client=gcp_storage_client, \n",
    "            bucket_name=gcp_bucket_name, \n",
    "            obj_prefix=data_paths[this_group]\n",
    "            )\n",
    "        this_json_data = merge_gcp_json_files(gcp_bucket, this_obj_list)\n",
    "    \n",
    "    json_data_nonrandom.extend(this_json_data)\n",
    "\n",
    "# apply data_source label for these groups\n",
    "for tweet in json_data_nonrandom:\n",
    "    tweet['data_source'] = \"verified_user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load JSON files to memory - random verified users\n",
    "json_groups_random = ['random']\n",
    "\n",
    "json_data_random = []\n",
    "\n",
    "for this_group in json_groups_random:\n",
    "    if (local_or_cloud == \"local\"):\n",
    "        this_file_list = get_json_files(data_paths[this_group])\n",
    "        this_json_data = merge_json_files(this_file_list)\n",
    "    elif (local_or_cloud == \"cloud\"):\n",
    "        this_obj_list = list_gcp_objects(\n",
    "            storage_client=gcp_storage_client, \n",
    "            bucket_name=gcp_bucket_name, \n",
    "            obj_prefix=data_paths[this_group]\n",
    "            )\n",
    "        this_json_data = merge_gcp_json_files(gcp_bucket, this_obj_list)\n",
    "    \n",
    "    json_data_random.extend(this_json_data)\n",
    "\n",
    "# apply data_source label for these groups\n",
    "for tweet in json_data_random:\n",
    "    tweet['data_source'] = \"verified_random\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with `data_source` labels applied, merge the two lists with JSON data\n",
    "json_data = []\n",
    "json_data.extend(json_data_nonrandom)\n",
    "json_data.extend(json_data_random)\n",
    "\n",
    "# normalize into a pandas dataframe (intermediate step to get to parquet easily)\n",
    "json_df = pd.json_normalize(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this cell requires package `pyarrow` to be installed in environment\n",
    "# save result to a new file\n",
    "json_filename: str = \"json_snapshot.parquet.snappy\"\n",
    "json_path: str = f\"{snapshot_paths['json_snapshot']}{json_filename}\"\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    # save a local file\n",
    "    json_df.to_parquet(json_path, engine=\"pyarrow\", index=False)\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    # save to the bucket\n",
    "    set_gcp_object_from_df_as_parq(bucket=gcp_bucket, object_name=json_path, df=json_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2709f604237a04264ecc775f88c78ac45228a864edc8e08ce0d33e6ed578a355"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
