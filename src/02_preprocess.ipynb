{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Turing Test: Detecting Disinformation on Twitter  \n",
    "\n",
    "|          | Group #2 - Disinformation Detectors                     |\n",
    "|---------:|---------------------------------------------------------|\n",
    "| Members  | John Johnson, Katy Matulay, Justin Minnion, Jared Rubin |\n",
    "| Notebook | `02_preprocess.ipynb`                                   |\n",
    "| Purpose  | Apply a pre-processing pipeline to merged data.         |\n",
    "\n",
    "> (TODO - write more explaining notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from Python standard library\n",
    "\n",
    "# imports requiring installation\n",
    "#   connection to Google Cloud Storage\n",
    "from google.cloud import storage            # pip install google-cloud-storage\n",
    "from google.oauth2 import service_account   # pip install google-auth\n",
    "\n",
    "#  data science packages\n",
    "import pandas as pd                         # pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from tweet_turing.py\n",
    "from tweet_turing import get_json_files, load_local_json, get_gcp_storage_client, get_gcp_bucket, \\\n",
    "    list_gcp_objects, get_gcp_object_as_json, get_gcp_object_as_text, set_gcp_object_from_json, \\\n",
    "    is_retweet, has_url, convert_emoji_list, emoji_count, load_local_json_parquet, \\\n",
    "    authentic_df_eda_dtype_mapping, get_gcp_object_from_parq_as_df, set_gcp_object_from_df_as_parq\n",
    "\n",
    "# imports from tweet_turing_paths.py\n",
    "from tweet_turing_paths import local_data_paths, local_snapshot_paths, gcp_data_paths, \\\n",
    "    gcp_snapshot_paths, gcp_project_name, gcp_bucket_name, gcp_key_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local or Cloud?\n",
    "\n",
    "Decide here whether to run notebook with local data or GCP bucket data\n",
    " - if the working directory of this notebook has a \"../data/\" folder with data loaded (e.g. working on local computer or have data files loaded to a cloud VM) then use the \"local files\" option and comment out the \"gcp bucket files\" option\n",
    " - if this notebook is being run from a GCP VM (preferrably in the `us-central1` location) then use the \"gcp bucket files\" option and comment out the \"local files\" option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option: local files\n",
    "local_or_cloud: str = \"local\"   # comment/uncomment this line or next\n",
    "\n",
    "# option: gcp bucket files\n",
    "#local_or_cloud: str = \"cloud\"   # comment/uncomment this line or previous\n",
    "\n",
    "# don't comment/uncomment for remainder of cell\n",
    "if (local_or_cloud == \"local\"):\n",
    "    data_paths = local_data_paths\n",
    "    snapshot_paths = local_snapshot_paths\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    data_paths = gcp_data_paths\n",
    "    snapshot_paths = gcp_snapshot_paths\n",
    "else:\n",
    "    raise ValueError(\"Variable 'local_or_cloud' can only take on one of two values, 'local' or 'cloud'.\")\n",
    "    # subsequent cells will not do this final \"else\" check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell only needs to run its code if local_or_cloud==\"cloud\"\n",
    "#   (though it is harmless if run when local_or_cloud==\"local\")\n",
    "gcp_storage_client: storage.Client = None\n",
    "gcp_bucket: storage.Bucket = None\n",
    "\n",
    "if (local_or_cloud == \"cloud\"):\n",
    "    gcp_storage_client = get_gcp_storage_client(project_name=gcp_project_name, key_file=gcp_key_file)\n",
    "    gcp_bucket = get_gcp_bucket(storage_client=gcp_storage_client, bucket_name=gcp_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Troll Tweets (CSV) Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load CSV Snapshot (from prior merge step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the merged troll tweet CSV snapshot file\n",
    "csv_filename: str = \"csv_snapshot.parquet.snappy\"\n",
    "csv_path: str = f\"{snapshot_paths['csv_snapshot']}{csv_filename}\"\n",
    "troll_df_raw: pd.DataFrame = pd.DataFrame()\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    troll_df_raw = pd.read_parquet(csv_path, engine='pyarrow')\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    troll_df_raw = get_gcp_object_from_parq_as_df(bucket=gcp_bucket, object_name=csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Filter for *\"Only English language tweets\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for english language tweets only\n",
    "#   - relevant dataframe column is `language`\n",
    "mask_lang_en: pd.Series = (troll_df_raw['language'] == 'English')\n",
    "\n",
    "troll_df = troll_df_raw[mask_lang_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['external_author_id',\n",
       " 'author',\n",
       " 'content',\n",
       " 'region',\n",
       " 'language',\n",
       " 'publish_date',\n",
       " 'harvested_date',\n",
       " 'following',\n",
       " 'followers',\n",
       " 'updates',\n",
       " 'post_type',\n",
       " 'account_type',\n",
       " 'retweet',\n",
       " 'account_category',\n",
       " 'new_june_2018',\n",
       " 'alt_external_id',\n",
       " 'tweet_id',\n",
       " 'article_url',\n",
       " 'tco1_step1',\n",
       " 'tco2_step1',\n",
       " 'tco3_step1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debug\n",
    "troll_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Extract columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Troll Dataframe Shape (rows, cols): (2116867, 14)\n"
     ]
    }
   ],
   "source": [
    "# extract only the columns we will use for later steps\n",
    "cols_to_keep = [\n",
    "    'external_author_id',\n",
    "    'author',\n",
    "    'content',\n",
    "    'region',\n",
    "    'language',\n",
    "    'publish_date',\n",
    "    'following',\n",
    "    'followers',\n",
    "    'updates',\n",
    "    'post_type',\n",
    "    'retweet',\n",
    "    'account_category',\n",
    "    'tweet_id',\n",
    "    'tco1_step1'\n",
    "    ]\n",
    "\n",
    "troll_df = troll_df[cols_to_keep]\n",
    "\n",
    "print(\"Troll Dataframe Shape (rows, cols):\", troll_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Derive new feature: `data_source`\n",
    "\n",
    "This feature is setup as a constant value __\"Troll\"__ for this subset of the dataset to indicate that the data originates from the troll tweets CSV snapshot file. The tweets obtained from Twitter API (in JSON files) have the same feature added by the `01_merge` notebook, but their values are either __\"verified_user\"__ or __\"verified_random\"__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column using a pd.Series constructor so we can specify dtype\n",
    "troll_df['data_source'] = pd.Series('Troll', index=troll_df.index, dtype='string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Align column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup rename mapping\n",
    "#   key = old column name; value = new column name\n",
    "col_name_mapping = {\n",
    "    \"retweet\": \"is_retweet\",\n",
    "}\n",
    "\n",
    "troll_df.rename(columns=col_name_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Authentic Tweets (JSON) Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load JSON Snapshot (from prior merge step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the merged authentic tweet snapshot file\n",
    "json_filename: str = \"json_snapshot.parquet.snappy\"\n",
    "json_path: str = f\"{snapshot_paths['json_snapshot']}{json_filename}\"\n",
    "\n",
    "json_df: pd.DataFrame = None\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    json_df = load_local_json_parquet(json_path)\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    pass\n",
    "    # \"this kills the cloud VM\"\n",
    "    #   - crashes when loading ~3.6GB (uncompressed) from parquet file to pandas dataframe\n",
    "    #   - commenting out until workaround determined\n",
    "    #json_df = get_gcp_object_from_parq_as_df(bucket=gcp_bucket, object_name=json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Extract columns of interest (1 of 2)\n",
    "\n",
    "We'll first extract columns into a copy of the `authentic_df_raw` dataframe. This copy will fork off into a stand-alone snapshot file for use with EDA of the authentic tweets. Before forking off, though, we'll add in our derived features.\n",
    "\n",
    "Later we'll modify the `authentic_df_raw` dataframe to keep only the columns we intend to merge with the `troll_df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fork off with a larger subset of columns for later use in authentic-tweet-specific EDA\n",
    "cols_keep_EDA = [\n",
    "    'author_id',\n",
    "    'created_at',\n",
    "    'id',\n",
    "    'text',\n",
    "    'lang',\n",
    "    'referenced_tweets',\n",
    "    'public_metrics.retweet_count', \n",
    "    'public_metrics.reply_count', \n",
    "    'public_metrics.like_count', \n",
    "    'public_metrics.quote_count',\n",
    "    'author.location', \n",
    "    'author.name', \n",
    "    'author.username', \n",
    "    'author.public_metrics.followers_count',\n",
    "    'author.public_metrics.following_count', \n",
    "    'author.entities.url.urls', \n",
    "    'author.created_at',\n",
    "    'author.verified', \n",
    "    'context_annotations', \n",
    "    'entities.annotations', \n",
    "    'entities.mentions',\n",
    "    'entities.hashtags', \n",
    "    'entities.urls',\n",
    "    'data_source'\n",
    "    ]\n",
    "\n",
    "# setup a new dataframe with subset of columns\n",
    "authentic_df_eda = json_df[cols_keep_EDA].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Set data types\n",
    "\n",
    "This helps our pandas dataframes run more smoothly and predictably, and sets up for an accurate type mapping when we save to a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "authentic_df_eda = authentic_df_eda.astype(authentic_df_eda_dtype_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Derive new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Derive new feature: `is_retweet`\n",
    "\n",
    "A tweet is flagged as a retweet if field `referenced_tweets` meets all the following criteria:\n",
    " - is not `NaN`\n",
    " - contains a list with at least one element\n",
    " - the first (index=0) element is a dict where key `type` has value \"`retweeted`\" or has value \"`quoted`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a mask of non-NaN `referenced_tweets` rows\n",
    "notna_mask = authentic_df_eda['referenced_tweets'].notna()\n",
    "\n",
    "# mask off for non-NaN and apply `is_retweet_alt`, outputting 1 or 0 to masked rows\n",
    "new_column = authentic_df_eda.loc[notna_mask].apply(is_retweet, axis='columns')\n",
    "authentic_df_eda.loc[notna_mask, 'is_retweet'] = new_column.astype('uint8')\n",
    "\n",
    "# fill in NaN values for any rows filtered out of prior step\n",
    "authentic_df_eda.loc[~notna_mask, 'is_retweet'] = int(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Derive new feature: `updates`\n",
    "\n",
    "This feature is derived by adding together four public metrics for a given tweet. This matches up with the feature definition from the troll dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_cols = [\n",
    "    'public_metrics.retweet_count',\n",
    "    'public_metrics.reply_count', \n",
    "    'public_metrics.like_count',\n",
    "    'public_metrics.quote_count'\n",
    "    ]\n",
    "\n",
    "authentic_df_eda['updates'] = authentic_df_eda[update_cols].sum(axis='columns').astype('uint64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Derive new feature: `account_category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_category_mapping = {\n",
    "    True: \"Verified_User\",\n",
    "    False: \"Unknown\",\n",
    "    }\n",
    "\n",
    "authentic_df_eda['account_category'] = authentic_df_eda['author.verified'].apply(lambda b: account_category_mapping[b]).astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4 Derive new feature: `tco1_step1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask for tweets where `entities.urls` has a non-null value\n",
    "has_url_mask = (authentic_df_eda['entities.urls'].notnull())\n",
    "\n",
    "# use this mask to feed into a lambda function pipeline:\n",
    "#   first lambda function grabs the first (index=0) element of the `entities.urls` list\n",
    "#   second lambda function grabs the `unwound_url` value (preferrably) or the `expanded_url` value as a backup (if `unwound_url` is None)\n",
    "authentic_df_eda.loc[has_url_mask, 'tco1_step1'] = authentic_df_eda.loc[has_url_mask, 'entities.urls'] \\\n",
    "    .apply(lambda entities_urls_list: entities_urls_list[0]) \\\n",
    "    .apply(lambda url_element: \n",
    "        url_element['expanded_url'] if (url_element['unwound_url'] is None) \n",
    "        else url_element['unwound_url']\n",
    "        ).astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.5 Derive new feature: `post_type`\n",
    "\n",
    "A variation of `is_retweet` but with an additional dimension: differentiates retweet tweets between plain retweets (reposts original tweet with no comment added) and quote tweets (reposts original tweet with comment added). This mimics a feature by the same name from the troll dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to be moved to `tweet_turing.py` after testing\n",
    "def get_post_type(tweet_series: pd.Series) -> str:\n",
    "    \"\"\"Examines a tweet series, returns whether it is a generic tweet, a retweet, or a quote tweet\"\"\"\n",
    "    ref_twt = tweet_series['referenced_tweets']\n",
    "\n",
    "    if (ref_twt is None):\n",
    "        return None\n",
    "    else:\n",
    "        return ref_twt[0]['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "authentic_df_eda['post_type'] = authentic_df_eda.apply(get_post_type, axis='columns').astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NA>          819915\n",
       "replied_to    334749\n",
       "retweeted     315175\n",
       "quoted         38189\n",
       "Name: post_type, dtype: Int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authentic_df_eda['post_type'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Align column names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup rename mapping\n",
    "#   key = old column name; value = new column name\n",
    "col_name_mapping = {\n",
    "    \"author_id\": \"external_author_id\", \n",
    "    \"created_at\": \"publish_date\", \n",
    "    \"text\": \"content\",\n",
    "    \"lang\": \"language\", \n",
    "    \"author.location\": \"region\", \n",
    "    \"author.username\": \"author\",\n",
    "    \"author.name\": \"full_name\",\n",
    "    \"author.public_metrics.followers_count\": \"followers\",\n",
    "    \"author.public_metrics.following_count\": \"following\",\n",
    "    \"id\": \"tweet_id\",\n",
    "    }\n",
    "\n",
    "authentic_df_eda.rename(columns=col_name_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Save EDA Snapshot\n",
    "\n",
    "As a follow-up to section 3.2 above, save a snapshot of the dataset intended for authentic-specific EDA. This dataset will have additional columns beyond the troll dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this cell requires package `pyarrow` to be installed in environment\n",
    "# save `authentic_df_eda` snapshot\n",
    "parq_filename: str = \"authentic_df_eda.parquet.snappy\"\n",
    "parq_path: str = f\"{snapshot_paths['parq_snapshot']}{parq_filename}\"\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    authentic_df_eda.to_parquet(parq_path, engine='pyarrow', index=False)\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Drop columns not needed for merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    'public_metrics.retweet_count',\n",
    "    'public_metrics.reply_count', \n",
    "    'public_metrics.like_count',\n",
    "    'public_metrics.quote_count',\n",
    "    'author.entities.url.urls', \n",
    "    'author.created_at', \n",
    "    'author.verified',\n",
    "    'context_annotations', \n",
    "    'entities.annotations', \n",
    "    'entities.mentions',\n",
    "    'entities.hashtags',\n",
    "    'full_name',\n",
    "    'referenced_tweets',\n",
    "    'entities.urls'\n",
    "    ]\n",
    "\n",
    "authentic_df = authentic_df_eda.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Merge (Partially) Pre-processed Tweets\n",
    "\n",
    "At this stage, the two separate datasets can be merged. Additional pre-processing will still be performed but can be applied to the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataframe shape: (3624895, 15)\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.concat([troll_df, authentic_df], axis='index', ignore_index=True)\n",
    "\n",
    "print(\"Merged dataframe shape:\", merged_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Save Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this cell requires package `pyarrow` to be installed in environment\n",
    "# save `merged_df` snapshot\n",
    "parq_filename: str = \"merged_df.parquet.snappy\"\n",
    "parq_path: str = f\"{snapshot_paths['json_snapshot']}{parq_filename}\"\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    merged_df.to_parquet(parq_path, engine='pyarrow', index=False)\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Merged DF Pre-Processing\n",
    "\n",
    "Below are preprocessing steps intended for the full, merged dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 (Optional) Load Snapshot of `merged_df`\n",
    "\n",
    "Optional cell to load snapshot (parquet file) saved during prior step (4.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this cell requires package `pyarrow` to be installed in environment\n",
    "parq_filename: str = \"merged_df.parquet.snappy\"\n",
    "parq_path: str = f\"{snapshot_paths['json_snapshot']}{parq_filename}\"\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    merged_df = pd.read_parquet(parq_path, engine='pyarrow')\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Remove tweets with null `content` field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.dropna(subset=['content'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Derive new feature: `has_url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column = merged_df.apply(has_url, axis='columns')\n",
    "merged_df.loc[:, 'has_url'] = new_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 Derive new feature: `emoji_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply convert_emoji_list\n",
    "new_column = merged_df.apply(convert_emoji_list, axis='columns')\n",
    "merged_df.loc[:, 'emoji_text'] = new_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 Derive new feature: `emoji_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count emoji list\n",
    "new_column = merged_df.apply(emoji_count, axis='columns')\n",
    "merged_df.loc[:, 'emoji_count'] = new_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Convert `publish_date` to `datetime` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates from Twitter API, using ISO 8601 format\n",
    "#   pandas docs report good performance using \"infer_datetime_format=True\" when format is ISO 8601 already\n",
    "verified_mask = merged_df['data_source'].isin(['verified_random', 'verified_user'])\n",
    "merged_df.loc[verified_mask, 'publish_date_parsed'] = pd.to_datetime(merged_df.loc[verified_mask, 'publish_date'], infer_datetime_format=True, utc=True)\n",
    "\n",
    "# dates from Troll dataset, using a defined format\n",
    "#   unclear if times are in UTC or their local time zone\n",
    "troll_mask = (merged_df['data_source'] == 'Troll')\n",
    "troll_format = r\"%m/%d/%Y %H:%M\"  # example: 10/1/2017 22:43\n",
    "merged_df.loc[troll_mask, 'publish_date_parsed'] = pd.to_datetime(merged_df.loc[troll_mask, 'publish_date'], format=troll_format, utc=True)\n",
    "\n",
    "# rename new column, drop old column\n",
    "merged_df.drop(columns=['publish_date'], inplace=True)\n",
    "merged_df.rename(columns={'publish_date_parsed': 'publish_date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "external_author_id          0\n",
       "author                      0\n",
       "content                     0\n",
       "region                 180192\n",
       "language                    0\n",
       "following                   0\n",
       "followers                   0\n",
       "updates                     0\n",
       "post_type             2772358\n",
       "is_retweet                  0\n",
       "account_category            0\n",
       "tweet_id                    0\n",
       "tco1_step1            1427061\n",
       "data_source                 0\n",
       "has_url                     0\n",
       "emoji_text                  0\n",
       "emoji_count                 0\n",
       "publish_date                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this cell requires package `pyarrow` to be installed in environment\n",
    "# save `merged_df` snapshot\n",
    "parq_filename: str = \"merged_df_preprocessed.parquet.snappy\"\n",
    "parq_path: str = f\"{snapshot_paths['json_snapshot']}{parq_filename}\"\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    merged_df.to_parquet(parq_path, engine='pyarrow', index=False)\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('capstone_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "2709f604237a04264ecc775f88c78ac45228a864edc8e08ce0d33e6ed578a355"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
