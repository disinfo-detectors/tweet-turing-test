{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Turing Test: Detecting Disinformation on Twitter  \n",
    "\n",
    "|          | Group #2 - Disinformation Detectors                     |\n",
    "|---------:|---------------------------------------------------------|\n",
    "| Members  | John Johnson, Katy Matulay, Justin Minnion, Jared Rubin |\n",
    "| Notebook | `02_preprocess.ipynb`                                   |\n",
    "| Purpose  | Apply a pre-processing pipeline to merged data.         |\n",
    "\n",
    "> (TODO - write more explaining notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from Python standard library\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# imports requiring installation\n",
    "#   connection to Google Cloud Storage\n",
    "from google.cloud import storage            # pip install google-cloud-storage\n",
    "from google.oauth2 import service_account   # pip install google-auth\n",
    "\n",
    "#  data science packages\n",
    "import demoji                               # pip install demoji\n",
    "import numpy as np                          # pip install numpy\n",
    "import pandas as pd                         # pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from tweet_turing.py\n",
    "from tweet_turing import get_json_files, load_local_json, get_gcp_storage_client, get_gcp_bucket, \\\n",
    "    list_gcp_objects, get_gcp_object_as_json, get_gcp_object_as_text, set_gcp_object_from_json, \\\n",
    "    is_retweet, is_retweet_alt\n",
    "\n",
    "# imports from tweet_turing_paths.py\n",
    "from tweet_turing_paths import local_data_paths, local_snapshot_paths, gcp_data_paths, \\\n",
    "    gcp_snapshot_paths, gcp_project_name, gcp_bucket_name, gcp_key_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local or Cloud?\n",
    "\n",
    "Decide here whether to run notebook with local data or GCP bucket data\n",
    " - if the working directory of this notebook has a \"../data/\" folder with data loaded (e.g. working on local computer or have data files loaded to a cloud VM) then use the \"local files\" option and comment out the \"gcp bucket files\" option\n",
    " - if this notebook is being run from a GCP VM (preferrably in the `us-central1` location) then use the \"gcp bucket files\" option and comment out the \"local files\" option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option: local files\n",
    "local_or_cloud: str = \"local\"   # comment/uncomment this line or next\n",
    "\n",
    "# option: gcp bucket files\n",
    "#local_or_cloud: str = \"cloud\"   # comment/uncomment this line or previous\n",
    "\n",
    "# don't comment/uncomment for remainder of cell\n",
    "if (local_or_cloud == \"local\"):\n",
    "    data_paths = local_data_paths\n",
    "    snapshot_paths = local_snapshot_paths\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    data_paths = gcp_data_paths\n",
    "    snapshot_paths = gcp_snapshot_paths\n",
    "else:\n",
    "    raise ValueError(\"Variable 'local_or_cloud' can only take on one of two values, 'local' or 'cloud'.\")\n",
    "    # subsequent cells will not do this final \"else\" check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell only needs to run its code if local_or_cloud==\"cloud\"\n",
    "#   (though it is harmless if run when local_or_cloud==\"local\")\n",
    "gcp_storage_client: storage.Client = None\n",
    "gcp_bucket: storage.Bucket = None\n",
    "\n",
    "if (local_or_cloud == \"cloud\"):\n",
    "    gcp_storage_client = get_gcp_storage_client(project_name=gcp_project_name, key_file=gcp_key_file)\n",
    "    gcp_bucket = get_gcp_bucket(storage_client=gcp_storage_client, bucket_name=gcp_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Troll Tweets (CSV) Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load CSV Snapshot (from prior merge step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the merged troll tweet CSV snapshot file\n",
    "csv_filename: str = \"csv_snapshot.csv\"\n",
    "csv_path: str = f\"{snapshot_paths['csv_snapshot']}{csv_filename}\"\n",
    "troll_df_raw: pd.DataFrame = pd.DataFrame()\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    troll_df_raw = pd.read_csv(csv_path, encoding='utf-8', low_memory=False)\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Filter for *\"Only English language tweets\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for english language tweets only\n",
    "#   - relevant dataframe column is `language`\n",
    "mask_lang_en: pd.Series = (troll_df_raw['language'] == 'English')\n",
    "\n",
    "troll_df = troll_df_raw[mask_lang_en]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Extract columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Troll Dataframe Shape (rows, cols): (2116867, 13)\n"
     ]
    }
   ],
   "source": [
    "# extract only the columns we will use for later steps\n",
    "cols_to_keep = [\n",
    "    'external_author_id',\n",
    "    'author',\n",
    "    'content',\n",
    "    'region',\n",
    "    'language',\n",
    "    'publish_date',\n",
    "    'following',\n",
    "    'followers',\n",
    "    'updates',\n",
    "    'retweet',\n",
    "    'account_category',\n",
    "    'tweet_id',\n",
    "    'tco1_step1'\n",
    "    ]\n",
    "\n",
    "troll_df = troll_df[cols_to_keep]\n",
    "\n",
    "print(\"Troll Dataframe Shape (rows, cols):\", troll_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Derive new feature: `data_source`\n",
    "\n",
    "This feature is setup as a constant value __\"Troll\"__ for this subset of the dataset to indicate that the data originates from the troll tweets CSV snapshot file. The tweets obtained from Twitter API (in JSON files) have the same feature added by the `01_merge` notebook, but their values are either __\"verified_user\"__ or __\"verified_random\"__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "troll_df['data_source'] = 'Troll'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Align column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup rename mapping\n",
    "#   key = old column name; value = new column name\n",
    "col_name_mapping = {\n",
    "    \"retweet\": \"is_retweet\",\n",
    "    \"tco1_step1\": \"full_url\",\n",
    "}\n",
    "\n",
    "troll_df.rename(columns=col_name_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Authentic Tweets (JSON) Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load JSON Snapshot (from prior merge step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the merged troll tweet CSV snapshot file\n",
    "json_filename: str = \"json_snapshot.json\"\n",
    "json_path: str = f\"{snapshot_paths['json_snapshot']}{json_filename}\"\n",
    "\n",
    "json_data: list = []\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    json_data = load_local_json(json_path)\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    json_data = load_gcp_json(gcp_bucket, json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Transform from JSON to tabular form\n",
    "\n",
    "Apply the pandas function `json_normalize()` to flatten JSON dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert json to pandas dataframe using normalize to flatten dict\n",
    "authentic_df_raw = pd.json_normalize(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.X Transform: Encode as 'utf-8'\n",
    "\n",
    "Pipeline step skipped, data is already utf-8. Noting here so pipeline diagram can be updated, then delete this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Extract columns of interest (1 of 2)\n",
    "\n",
    "We'll first extract columns into a copy of the `authentic_df_raw` dataframe. This copy will fork off into a stand-alone snapshot file for use with EDA of the authentic tweets. Before forking off, though, we'll add in our derived features.\n",
    "\n",
    "Later we'll modify the `authentic_df_raw` dataframe to keep only the columns we intend to merge with the `troll_df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fork off with a larger subset of columns for later use in authentic-tweet-specific EDA\n",
    "cols_keep_EDA = [\n",
    "    'author_id',\n",
    "    'created_at',\n",
    "    'id',\n",
    "    'text',\n",
    "    'lang',\n",
    "    'referenced_tweets',\n",
    "    'public_metrics.retweet_count', \n",
    "    'public_metrics.reply_count', \n",
    "    'public_metrics.like_count', \n",
    "    'public_metrics.quote_count',\n",
    "    'author.location', \n",
    "    'author.name', \n",
    "    'author.username', \n",
    "    'author.public_metrics.followers_count',\n",
    "    'author.public_metrics.following_count', \n",
    "    'author.entities.url.urls', \n",
    "    'author.created_at',\n",
    "    'author.verified', \n",
    "    'context_annotations', \n",
    "    'entities.annotations', \n",
    "    'entities.mentions',\n",
    "    'entities.hashtags', \n",
    "    'entities.urls',\n",
    "    'data_source'\n",
    "    ]\n",
    "\n",
    "# setup a new dataframe with subset of columns\n",
    "authentic_df_eda = authentic_df_raw[cols_keep_EDA].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Derive new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Derive new feature: `is_retweet`\n",
    "\n",
    "Two ways to identify a retweeted tweet:  \n",
    "1. Field `text` starts with the string \"`RT @`\", though we need to determine if this can be faked.\n",
    "2. Field `referenced_tweets` meets all the following criteria:\n",
    "    - is not `NaN`\n",
    "    - contains a list with at least one element\n",
    "    - the first (index=0) element is a dict where key \"`type`\" has value \"`retweeted`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive feature `is_retweet`\n",
    "#   method 1\n",
    "new_column = authentic_df_eda.apply(is_retweet, axis='columns')\n",
    "authentic_df_eda.loc[:, 'is_retweet'] = new_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive feature `is_retweet_alt`\n",
    "#   method 2\n",
    "#    1- make a mask of non-NaN `referenced_tweets` rows\n",
    "notna_mask = authentic_df_eda['referenced_tweets'].notna()\n",
    "\n",
    "#    2- mask off for non-NaN and apply `is_retweet_alt`, outputting 1 or 0 to masked rows\n",
    "new_column = authentic_df_eda.loc[notna_mask].apply(is_retweet_alt, axis='columns')\n",
    "authentic_df_eda.loc[notna_mask, 'is_retweet_alt'] = new_column\n",
    "\n",
    "#   3- fill in NaN values for any rows filtered out of prior step\n",
    "authentic_df_eda.loc[~notna_mask, 'is_retweet_alt'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Derive new feature: `updates`\n",
    "\n",
    "This feature is derived by adding together four public metrics for a given tweet. This matches up with the feature definition from the troll dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_cols = [\n",
    "    'public_metrics.retweet_count',\n",
    "    'public_metrics.reply_count', \n",
    "    'public_metrics.like_count',\n",
    "    'public_metrics.quote_count'\n",
    "    ]\n",
    "\n",
    "authentic_df_eda['updates'] = authentic_df_eda[update_cols].sum(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Derive new feature: `account_category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_category_mapping = {\n",
    "    True: \"Verified_User\",\n",
    "    False: \"Unknown\",\n",
    "    }\n",
    "\n",
    "authentic_df_eda['account_category'] = authentic_df_eda['author.verified'].apply(lambda b: account_category_mapping[b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Align column names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup rename mapping\n",
    "#   key = old column name; value = new column name\n",
    "col_name_mapping = {\n",
    "    \"author_id\": \"external_author_id\", \n",
    "    \"created_at\": \"publish_date\", \n",
    "    \"text\": \"content\",\n",
    "    \"lang\": \"language\", \n",
    "    \"author.location\":\"region\", \n",
    "    \"author.username\":\"author\",\n",
    "    \"author.name\":\"full_name\",\n",
    "    \"author.public_metrics.followers_count\": \"followers\",\n",
    "    \"author.public_metrics.following_count\": \"following\",\n",
    "    \"id\": \"tweet_id\",\n",
    "    \"entities.urls\":\"full_url\"\n",
    "    }\n",
    "\n",
    "authentic_df_eda.rename(columns=col_name_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Save EDA Snapshot\n",
    "\n",
    "As a follow-up to section 3.3 above, save a snapshot of the dataset intended for authentic-specific EDA. This dataset will have additional columns beyond the troll dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this cell requires package `pyarrow` to be installed in environment\n",
    "# save `authentic_df_eda` snapshot\n",
    "parq_filename: str = \"authentic_df_eda.parquet.snappy\"\n",
    "parq_path: str = f\"{snapshot_paths['json_snapshot']}{parq_filename}\"\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    authentic_df_eda.to_parquet(parq_path, engine='pyarrow')\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Drop columns not needed for merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    'public_metrics.retweet_count',\n",
    "    'public_metrics.reply_count', \n",
    "    'public_metrics.like_count',\n",
    "    'public_metrics.quote_count',\n",
    "    'author.entities.url.urls', \n",
    "    'author.created_at', \n",
    "    'author.verified',\n",
    "    'context_annotations', \n",
    "    'entities.annotations', \n",
    "    'entities.mentions',\n",
    "    'entities.hashtags',\n",
    "    'full_name',\n",
    "    'referenced_tweets',\n",
    "    ]\n",
    "\n",
    "authentic_df = authentic_df_eda.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# debug\n",
    "print(len(troll_df.columns))\n",
    "print(len(authentic_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['account_category',\n",
       " 'author',\n",
       " 'content',\n",
       " 'data_source',\n",
       " 'external_author_id',\n",
       " 'followers',\n",
       " 'following',\n",
       " 'full_url',\n",
       " 'is_retweet',\n",
       " 'language',\n",
       " 'publish_date',\n",
       " 'region',\n",
       " 'tweet_id',\n",
       " 'updates']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debug\n",
    "sorted(troll_df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['account_category',\n",
       " 'author',\n",
       " 'content',\n",
       " 'data_source',\n",
       " 'external_author_id',\n",
       " 'followers',\n",
       " 'following',\n",
       " 'full_url',\n",
       " 'is_retweet',\n",
       " 'is_retweet_alt',\n",
       " 'language',\n",
       " 'publish_date',\n",
       " 'region',\n",
       " 'tweet_id',\n",
       " 'updates']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debug\n",
    "sorted(authentic_df.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Merge (Partially) Pre-processed Tweets\n",
    "\n",
    "At this stage, the two separate datasets can be merged. Additional pre-processing will still be performed but can be applied to the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataframe shape: (3624895, 15)\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.concat([troll_df, authentic_df], axis='index')\n",
    "\n",
    "print(\"Merged dataframe shape:\", merged_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Fix dtypes\n",
    "\n",
    "Two columns need to be explicitly typed as strings in order to be saved to parquet format.\n",
    "\n",
    "Using `string[python]` but could also try `string[pyarrow]` to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['tweet_id'] = merged_df['tweet_id'].astype(\"string[python]\")\n",
    "merged_df['full_url'] = merged_df['full_url'].astype(\"string[python]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Save Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this cell requires package `pyarrow` to be installed in environment\n",
    "# save `merged_df` snapshot\n",
    "parq_filename: str = \"merged_df.parquet.snappy\"\n",
    "parq_path: str = f\"{snapshot_paths['json_snapshot']}{parq_filename}\"\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    merged_df.to_parquet(parq_path, engine='pyarrow')\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Merged DF Pre-Processing\n",
    "\n",
    "Below are preprocessing steps intended for the full, merged dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 (Optional) Load Snapshot of `merged_df`\n",
    "\n",
    "Optional cell to load snapshot (parquet file) saved during prior step (4.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this cell requires package `pyarrow` to be installed in environment\n",
    "parq_filename: str = \"merged_df.parquet.snappy\"\n",
    "parq_path: str = f\"{snapshot_paths['json_snapshot']}{parq_filename}\"\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    merged_df = pd.read_parquet(parq_path, engine='pyarrow')\n",
    "    # TODO -> confirm index is maintained from before snapshot\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Remove tweets with null `content` field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.dropna(subset=['content'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Derive new feature: `has_url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define here for testing, then move to tweet_turing.py\n",
    "def has_url(tweet_series: pd.Series, search_str: str = 'http') -> int:\n",
    "    if (tweet_series['content'] is not None):\n",
    "        return int(search_str in tweet_series['content'])\n",
    "    else:\n",
    "        return 0\n",
    "    #\n",
    "    # \n",
    "    # try:\n",
    "    #     return int(search_str in tweet_series['content'])\n",
    "    # except:\n",
    "    #     print(tweet_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column = merged_df.apply(has_url, axis='columns')\n",
    "merged_df.loc[:, 'has_url'] = new_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 Derive new feature: `emoji_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define here for testing, then move to tweet_turing.py\n",
    "''' The following converts a text string with emojis into a list of descriptive text strings.\n",
    "    Duplicate emojis are captured as each emoji converts to 1 text string.'''\n",
    "def convert_emoji_list(x):\n",
    "    lst=[]\n",
    "    estring = ''\n",
    "    # import demoji\n",
    "    # import numpy as np\n",
    "    if x is not np.nan:\n",
    "        #extract list of text from demoji func\n",
    "        lst = demoji.findall_list(x)\n",
    "        if len(lst)<0:\n",
    "            return np.nan\n",
    "     \n",
    "        else:\n",
    "            return(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply convert_emoji_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 Derive new feature: `emoji_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count emoji list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Convert `publish_date` to `datetime` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this cell requires package `pyarrow` to be installed in environment\n",
    "# save `merged_df` snapshot\n",
    "parq_filename: str = \"merged_df_preprocessed.parquet.snappy\"\n",
    "parq_path: str = f\"{snapshot_paths['json_snapshot']}{parq_filename}\"\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    merged_df.to_parquet(parq_path, engine='pyarrow')\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('capstone_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c415e9a8202b4ead7a18a2980a657f34ccc0b50512c3bb4fbee09dd6dc0ae907"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
