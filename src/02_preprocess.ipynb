{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Turing Test: Detecting Disinformation on Twitter  \n",
    "\n",
    "|          | Group #2 - Disinformation Detectors                     |\n",
    "|---------:|---------------------------------------------------------|\n",
    "| Members  | John Johnson, Katy Matulay, Justin Minnion, Jared Rubin |\n",
    "| Notebook | `02_preprocess.ipynb`                                   |\n",
    "| Purpose  | Apply a pre-processing pipeline to merged data.         |\n",
    "\n",
    "> (TODO - write more explaining notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from Python standard library\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# imports requiring installation\n",
    "#   connection to Google Cloud Storage\n",
    "from google.cloud import storage            # pip install google-cloud-storage\n",
    "from google.oauth2 import service_account   # pip install google-auth\n",
    "\n",
    "#  data science packages\n",
    "import demoji                               # pip install demoji\n",
    "import numpy as np                          # pip install numpy\n",
    "import pandas as pd                         # pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from tweet_turing.py\n",
    "from tweet_turing import get_json_files, load_local_json, get_gcp_storage_client, get_gcp_bucket, \\\n",
    "    list_gcp_objects, get_gcp_object_as_json, get_gcp_object_as_text, \\\n",
    "    set_gcp_object_from_json\n",
    "\n",
    "# imports from tweet_turing_paths.py\n",
    "from tweet_turing_paths import local_data_paths, local_snapshot_paths, gcp_data_paths, \\\n",
    "    gcp_snapshot_paths, gcp_project_name, gcp_bucket_name, gcp_key_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local or Cloud?\n",
    "\n",
    "Decide here whether to run notebook with local data or GCP bucket data\n",
    " - if the working directory of this notebook has a \"../data/\" folder with data loaded (e.g. working on local computer or have data files loaded to a cloud VM) then use the \"local files\" option and comment out the \"gcp bucket files\" option\n",
    " - if this notebook is being run from a GCP VM (preferrably in the `us-central1` location) then use the \"gcp bucket files\" option and comment out the \"local files\" option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option: local files\n",
    "local_or_cloud: str = \"local\"   # comment/uncomment this line or next\n",
    "\n",
    "# option: gcp bucket files\n",
    "#local_or_cloud: str = \"cloud\"   # comment/uncomment this line or previous\n",
    "\n",
    "# don't comment/uncomment for remainder of cell\n",
    "if (local_or_cloud == \"local\"):\n",
    "    data_paths = local_data_paths\n",
    "    snapshot_paths = local_snapshot_paths\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    data_paths = gcp_data_paths\n",
    "    snapshot_paths = gcp_snapshot_paths\n",
    "else:\n",
    "    raise ValueError(\"Variable 'local_or_cloud' can only take on one of two values, 'local' or 'cloud'.\")\n",
    "    # subsequent cells will not do this final \"else\" check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell only needs to run its code if local_or_cloud==\"cloud\"\n",
    "#   (though it is harmless if run when local_or_cloud==\"local\")\n",
    "gcp_storage_client: storage.Client = None\n",
    "gcp_bucket: storage.Bucket = None\n",
    "\n",
    "if (local_or_cloud == \"cloud\"):\n",
    "    gcp_storage_client = get_gcp_storage_client(project_name=gcp_project_name, key_file=gcp_key_file)\n",
    "    gcp_bucket = get_gcp_bucket(storage_client=gcp_storage_client, bucket_name=gcp_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Troll Tweets (CSV) Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load CSV Snapshot (from prior merge step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the merged troll tweet CSV snapshot file\n",
    "csv_filename: str = \"csv_snapshot.csv\"\n",
    "csv_path: str = f\"{snapshot_paths['csv_snapshot']}{csv_filename}\"\n",
    "troll_df_raw: pd.DataFrame = pd.DataFrame()\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    troll_df_raw = pd.read_csv(csv_path, encoding='utf-8', low_memory=False)\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Filter for *\"Only English language tweets\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for english language tweets only\n",
    "#   - relevant dataframe column is `language`\n",
    "mask_lang_en: pd.Series = (troll_df_raw['language'] == 'English')\n",
    "\n",
    "troll_df = troll_df_raw[mask_lang_en]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Extract columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Troll Dataframe Shape (rows, cols): (2116867, 13)\n"
     ]
    }
   ],
   "source": [
    "# extract only the columns we will use for later steps\n",
    "cols_to_keep = [\n",
    "    'external_author_id',\n",
    "    'author',\n",
    "    'content',\n",
    "    'region',\n",
    "    'language',\n",
    "    'publish_date',\n",
    "    'following',\n",
    "    'followers',\n",
    "    'updates',\n",
    "    'retweet',\n",
    "    'account_category',\n",
    "    'tweet_id',\n",
    "    'tco1_step1'\n",
    "    ]\n",
    "\n",
    "troll_df = troll_df[cols_to_keep]\n",
    "\n",
    "print(\"Troll Dataframe Shape (rows, cols):\", troll_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Derive new feature: `data_source`\n",
    "\n",
    "This feature is setup as a constant value __\"Troll\"__ for this subset of the dataset to indicate that the data originates from the troll tweets CSV snapshot file. The tweets obtained from Twitter API (in JSON files) have the same feature added by the `01_merge` notebook, but their values are either __\"verified_user\"__ or __\"verified_random\"__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "troll_df['data_source'] = 'Troll'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Align column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup rename mapping\n",
    "#   key = old column name; value = new column name\n",
    "col_name_mapping = {\n",
    "    \"retweet\": \"is_retweet\",\n",
    "    \"tcol1_step1\": \"full_url\",\n",
    "}\n",
    "\n",
    "troll_df.rename(columns=col_name_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Authentic Tweets (JSON) Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load JSON Snapshot (from prior merge step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the merged troll tweet CSV snapshot file\n",
    "json_filename: str = \"json_snapshot.json\"\n",
    "json_path: str = f\"{snapshot_paths['json_snapshot']}{json_filename}\"\n",
    "\n",
    "json_data: list = []\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    json_data = load_local_json(json_path)\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    json_data = load_gcp_json(gcp_bucket, json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Transform from JSON to tabular form\n",
    "\n",
    "Apply the pandas function `json_normalize()` to flatten JSON dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert json to pandas dataframe using normalize to flatten dict\n",
    "authentic_df_raw = pd.json_normalize(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.X Transform: Encode as 'utf-8'\n",
    "\n",
    "Pipeline step skipped, data is already utf-8. Noting here so pipeline diagram can be updated, then delete this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Extract columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Derive new feature: `is_retweet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Derive new feature: `updates`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Derive new feature: `account_category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Align column names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Merge (Partially) Pre-processed Tweets\n",
    "\n",
    "At this stage, the two separate datasets can be merged. Additional pre-processing will still be performed but can be applied to the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('capstone_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2709f604237a04264ecc775f88c78ac45228a864edc8e08ce0d33e6ed578a355"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
