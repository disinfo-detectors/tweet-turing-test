{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Turing Test: Detecting Disinformation on Twitter  \n",
    "\n",
    "|          | Group #2 - Disinformation Detectors                     |\n",
    "|---------:|---------------------------------------------------------|\n",
    "| Members  | John Johnson, Katy Matulay, Justin Minnion, Jared Rubin |\n",
    "| Notebook | `xx_modelA_nlp_preprocess.ipynb`                        |\n",
    "| Purpose  | NLP-specific preprocessing for Model A                  |\n",
    "\n",
    "(todo: description)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from Python standard library\n",
    "\n",
    "# imports requiring installation\n",
    "#   connection to Google Cloud Storage\n",
    "from google.cloud import storage            # pip install google-cloud-storage\n",
    "from google.oauth2 import service_account   # pip install google-auth\n",
    "\n",
    "#  data science packages\n",
    "import numpy as np                          # pip install numpy\n",
    "import pandas as pd                         # pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from tweet_turing.py\n",
    "import tweet_turing as tur      # note - different import approach from prior notebooks\n",
    "\n",
    "# imports from tweet_turing_paths.py\n",
    "from tweet_turing_paths import local_data_paths, local_snapshot_paths, gcp_data_paths, \\\n",
    "    gcp_snapshot_paths, gcp_project_name, gcp_bucket_name, gcp_key_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas options\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local or Cloud?\n",
    "\n",
    "Decide here whether to run notebook with local data or GCP bucket data\n",
    " - if the working directory of this notebook has a \"../data/\" folder with data loaded (e.g. working on local computer or have data files loaded to a cloud VM) then use the \"local files\" option and comment out the \"gcp bucket files\" option\n",
    " - if this notebook is being run from a GCP VM (preferrably in the `us-central1` location) then use the \"gcp bucket files\" option and comment out the \"local files\" option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option: local files\n",
    "local_or_cloud: str = \"local\"   # comment/uncomment this line or next\n",
    "\n",
    "# option: gcp bucket files\n",
    "#local_or_cloud: str = \"cloud\"   # comment/uncomment this line or previous\n",
    "\n",
    "# don't comment/uncomment for remainder of cell\n",
    "if (local_or_cloud == \"local\"):\n",
    "    data_paths = local_data_paths\n",
    "    snapshot_paths = local_snapshot_paths\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    data_paths = gcp_data_paths\n",
    "    snapshot_paths = gcp_snapshot_paths\n",
    "else:\n",
    "    raise ValueError(\"Variable 'local_or_cloud' can only take on one of two values, 'local' or 'cloud'.\")\n",
    "    # subsequent cells will not do this final \"else\" check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell only needs to run its code if local_or_cloud==\"cloud\"\n",
    "#   (though it is harmless if run when local_or_cloud==\"local\")\n",
    "gcp_storage_client: storage.Client = None\n",
    "gcp_bucket: storage.Bucket = None\n",
    "\n",
    "if (local_or_cloud == \"cloud\"):\n",
    "    gcp_storage_client = tur.get_gcp_storage_client(project_name=gcp_project_name, key_file=gcp_key_file)\n",
    "    gcp_bucket = tur.get_gcp_bucket(storage_client=gcp_storage_client, bucket_name=gcp_bucket_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Load Dataset\n",
    "\n",
    "Core dataset, as prepared by prior notebook `03_eda.ipynb`, will be loaded as \"`df_full`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this cell requires package `pyarrow` to be installed in environment\n",
    "parq_filename: str = \"data_after_03_eda.parquet.gz\"\n",
    "parq_path: str = f\"{snapshot_paths['parq_snapshot']}{parq_filename}\"\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    df_full = pd.read_parquet(parq_path, engine='pyarrow')\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    pass    # TODO: implement loading of cloud file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Subset Data\n",
    "\n",
    "Data subset will be created as simply \"`df`\" for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset parameters\n",
    "sample_fraction = 0.10  # within range (0.0, 1.0)\n",
    "random_seed = 3         # for reproducability, and \"the number of the counting shall be three\"\n",
    "\n",
    "# generate sample\n",
    "df = df_full.sample(frac=sample_fraction, random_state=random_seed).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataframe size:\t    2.74 GB\n",
      "Sampled dataframe size:\t    0.28 GB\n",
      "\n",
      "Full dataframe rows:\t  3,624,894\n",
      "Sampled dataframe rows:\t    362,489\n",
      "\n",
      "Full df class split:\t['58.4%', '41.6%']\n",
      "Sampled df class split:\t['58.4%', '41.6%']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BYTES_PER_GIGABYTE = 10**9  # using IEC-recommended conversion; https://en.wikipedia.org/wiki/Gigabyte#Base_10_(decimal)\n",
    "\n",
    "df_full_size_gb = df_full.memory_usage(deep=True).sum() / BYTES_PER_GIGABYTE\n",
    "df_size_gb = df.memory_usage(deep=True).sum() / BYTES_PER_GIGABYTE\n",
    "\n",
    "print(f\"Full dataframe size:\\t{df_full_size_gb:8.2f} GB\")\n",
    "print(f\"Sampled dataframe size:\\t{df_size_gb:8.2f} GB\\n\")\n",
    "\n",
    "print(f\"Full dataframe rows:\\t{len(df_full.index):>11,}\")\n",
    "print(f\"Sampled dataframe rows:\\t{len(df.index):>11,}\\n\")\n",
    "\n",
    "class_split_full = [f\"{x*100:0.1f}%\" for x in df_full['class'].value_counts().div(len(df_full.index)).tolist()]\n",
    "class_split_samp = [f\"{x*100:0.1f}%\" for x in df['class'].value_counts().div(len(df.index)).tolist()]\n",
    "\n",
    "print(f\"Full df class split:\\t{class_split_full}\")\n",
    "print(f\"Sampled df class split:\\t{class_split_samp}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy of sampled df so above steps don't need to be repeated everytime\n",
    "# note this cell requires package `pyarrow` to be installed in environment\n",
    "parq_filename: str = \"data_sample_ten_percent.parquet.gz\"\n",
    "parq_path: str = f\"{snapshot_paths['parq_snapshot']}{parq_filename}\"\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    df.to_parquet(parq_path, engine='pyarrow', index=False, compression='gzip')\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf290_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65a6dd32d71ed4a2e5ac9ab3f52d3aeee49f01a00467a63b19dc274a1d27154b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
