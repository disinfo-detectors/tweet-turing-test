{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "add0cef4-a4de-47c5-a9a0-e6b818d6fa2d",
   "metadata": {},
   "source": [
    "# Tweet Turing Test: Detecting Disinformation on Twitter  \n",
    "\n",
    "|          | Group #2 - Disinformation Detectors                     |\n",
    "|---------:|---------------------------------------------------------|\n",
    "| Members  | John Johnson, Katy Matulay, Justin Minnion, Jared Rubin |\n",
    "| Notebook | `05_multimodal.ipynb`                                   |\n",
    "| Purpose  | Combining tabular data with a BERT transformer.         |\n",
    "\n",
    "(todo: description)\n",
    "\n",
    "*Assumptions*  \n",
    " - The dataset being used has binary class labels following convention: 0 = authentic tweet; 1 = troll tweet\n",
    " - The execution environment has internet access (to download models from huggingface.co)\n",
    " - The execution environment has a CUDA-capable GPU available\n",
    "\n",
    "*General Notes*\n",
    " - Notebook kernel must be completely restarted between runs to release reserved VRAM from the GPU.\n",
    " - Notebook contains our usual code to load dataset file from GCP bucket, but model files are always saved locally regardless of `local_or_cloud` setting.\n",
    " - Notebook is based on a [notebook by georgian-io (github.com)](https://github.com/georgian-io/Multimodal-Toolkit/blob/master/notebooks/text_w_tabular_classification.ipynb) from their [Multimodal-Toolkit repository (github.com)](https://github.com/georgian-io/Multimodal-Toolkit) and [accompanying blog post (medium.com)](https://medium.com/georgian-impact-blog/how-to-incorporate-tabular-data-with-huggingface-transformers-b70ac45fcfb4).\n",
    " \n",
    " __*Note on `multimodal-transformers` Package Version*__\n",
    " - As of this writing (07-Mar-2023), the master branch of georgian-io's multimodal-transformers package [only supports ðŸ¤— transformers version 3.1](https://github.com/georgian-io/Multimodal-Toolkit/blob/7ac1e713570ab81fbbbdc32caa01c6d35607f840/setup.py#L8) (the current released transformers version is 4.26.1). \n",
    " - A georgian-io contributor, however, has been actively working on a separate branch to refactor the code to support ðŸ¤— transformers version 4.26.1. \n",
    " - The contributor has proposed [a pull request (PR #31)](https://github.com/georgian-io/Multimodal-Toolkit/pull/31) signalling they're ready to merge in their updated code, so we have installed the package using the GitHub branch corresponding to that pull request (branch \"`akash/update`\" and PR#31). To install this **very specific** version of multimodal-transformers from this branch, use the below `pip` command. This clones the repository for the latest commit associated with the subject pull request:  \n",
    "\n",
    "> `pip install 'multimodal-transformers @ https://github.com/georgian-io/Multimodal-Toolkit/commit/242180a31a0793e686197b5ccbd90368cdcfbb15`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a7349-cf9c-461b-aebd-de5542e2ded8",
   "metadata": {},
   "source": [
    "# 1 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm installed version of `transformers` matches the version required by `multimodal_transformers`\n",
    "supported_transformers_versions = {'4.25.1', '4.26.1'}\n",
    "\n",
    "import transformers\n",
    "assert (transformers.__version__ in supported_transformers_versions), \\\n",
    "    \"Unsupported version of 'transformers' installed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c0673b-7027-407e-8435-e29090105df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports from Python standard library\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import shutil\n",
    "import zipfile\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Optional\n",
    "\n",
    "# imports requiring installation\n",
    "#   connection to Google Cloud Storage\n",
    "from google.cloud import storage            # pip install google-cloud-storage\n",
    "from google.oauth2 import service_account   # pip install google-auth\n",
    "\n",
    "#  data science packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pynvml import *    # for debugging\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, auc, brier_score_loss, confusion_matrix, f1_score, precision_recall_curve, \n",
    "    precision_score, recall_score, roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "\n",
    "# ðŸ¤— (huggingface) packages\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig, AutoModel, \n",
    "    BertTokenizerFast, DistilBertTokenizerFast, RobertaTokenizerFast, BertweetTokenizer, XLMRobertaTokenizerFast,\n",
    "    TrainingArguments, Trainer, EvalPrediction, \n",
    "    set_seed\n",
    ")\n",
    "\n",
    "# Georgian packages\n",
    "#   Note: see note in notebook header about installing a highly specific version of `multimodal_transformers`\n",
    "from multimodal_transformers.data.load_data import load_train_val_test_helper   # shhh, didn't have a leading underscore\n",
    "from multimodal_transformers.model import (\n",
    "    TabularConfig, AutoModelWithTabular, BertWithTabular, DistilBertWithTabular, RobertaWithTabular\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7494318-b0ee-4452-80fa-c959050a6c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports from tweet_turing.py\n",
    "import tweet_turing as tur      # note - different import approach from prior notebooks\n",
    "\n",
    "# imports from tweet_turing_paths.py\n",
    "from tweet_turing_paths import local_data_paths, local_snapshot_paths, gcp_data_paths, \\\n",
    "    gcp_snapshot_paths, gcp_project_name, gcp_bucket_name, gcp_key_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee18cdf-9d6c-4315-90ad-3e470718b1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pandas options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# sns.set_theme(context='paper', style='ticks', palette='bright', font='georgia', rc={'figure.dpi': 200})\n",
    "# twitter_colors = {\n",
    "#     'blue': r\"#1DA1F2\",     # source: https://usbrandcolors.com/twitter-colors/\n",
    "#     'black': r\"#14171A\",\n",
    "#     'dark gray': r\"#657786\",\n",
    "#     'dark blue': r\"#0f5580\" # \"blue\" with brightness changed 88% to 50%, non-standard color\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7124b0f-b27f-450a-ba49-f0560c5b38b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Local or Cloud?\n",
    "\n",
    "Decide here whether to run notebook with local data or GCP bucket data\n",
    " - if the working directory of this notebook has a \"../data/\" folder with data loaded (e.g. working on local computer or have data files loaded to a cloud VM) then use the \"local files\" option and comment out the \"gcp bucket files\" option\n",
    " - if this notebook is being run from a GCP VM (preferrably in the `us-central1` location) then use the \"gcp bucket files\" option and comment out the \"local files\" option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d579d5d-f289-4b49-a1aa-f72f04d503b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# option: local files\n",
    "local_or_cloud: str = \"local\"   # comment/uncomment this line or next\n",
    "\n",
    "# option: gcp bucket files\n",
    "#local_or_cloud: str = \"cloud\"   # comment/uncomment this line or previous\n",
    "\n",
    "# don't comment/uncomment for remainder of cell\n",
    "if (local_or_cloud == \"local\"):\n",
    "    data_paths = local_data_paths\n",
    "    snapshot_paths = local_snapshot_paths\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    data_paths = gcp_data_paths\n",
    "    snapshot_paths = gcp_snapshot_paths\n",
    "else:\n",
    "    raise ValueError(\"Variable 'local_or_cloud' can only take on one of two values, 'local' or 'cloud'.\")\n",
    "    # subsequent cells will not do this final \"else\" check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b206dfc-7e39-49a1-a48d-71a4cb46e863",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell only needs to run its code if local_or_cloud==\"cloud\"\n",
    "#   (though it is harmless if run when local_or_cloud==\"local\")\n",
    "gcp_storage_client: storage.Client = None\n",
    "gcp_bucket: storage.Bucket = None\n",
    "\n",
    "if (local_or_cloud == \"cloud\"):\n",
    "    gcp_storage_client = tur.get_gcp_storage_client(project_name=gcp_project_name, key_file=gcp_key_file)\n",
    "    gcp_bucket = tur.get_gcp_bucket(storage_client=gcp_storage_client, bucket_name=gcp_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46318833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# from huggingface tutorial: https://huggingface.co/docs/transformers/perf_train_gpu_one#efficient-training-on-a-single-gpu\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fa7ee9f-857e-4c69-bd3f-803a970c3199",
   "metadata": {},
   "source": [
    "# 2 - Load Dataset\n",
    "\n",
    "Starting with the full dataset (with engineered features) from notebook **`06_feature_engineering.ipynb`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e19c3b-cab5-4e4c-a6b8-950f4c0dbc58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note this cell requires package `pyarrow` to be installed in environment\n",
    "parq_filename: str = \"06data_full_final_en2.parquet.gz\"\n",
    "parq_path = Path(snapshot_paths['parq_snapshot'], parq_filename)\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    df_full = pd.read_parquet(parq_path, engine='pyarrow')\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    df_full = tur.get_gcp_object_from_parq_as_df(bucket=gcp_bucket, object_name=parq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96506d5c-cd41-4382-9518-55132bfad5d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_full.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf47eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make a smaller slice of full dataset for testing\n",
    "# n_tweets = 100000\n",
    "\n",
    "# df_small = df_full.groupby(by='class_numeric').sample(n=(n_tweets//2), random_state=42).reset_index()\n",
    "\n",
    "# # confirm stratefied\n",
    "# df_small['class_numeric'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "808911e9",
   "metadata": {},
   "source": [
    "# 3 - Setup Experimental Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf46bdf6",
   "metadata": {},
   "source": [
    "## 3.1 - Define dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0458b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataclasses to store the experiment parameters\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: str = field()\n",
    "    config_name:  Optional[str] = field(default=None)\n",
    "    tokenizer_name: Optional[str] = field(default=None)\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    num_labels: int = field(default=2)\n",
    "\n",
    "@dataclass\n",
    "class MultimodalDataTrainingArguments:\n",
    "    #data_path: str = field()\n",
    "    data_df: pd.DataFrame = field()     # modified to use dataframe\n",
    "\n",
    "    column_info_path: str = field(default=None)\n",
    "    column_info: dict = field(default=None)\n",
    "\n",
    "    categorical_encode_type: str = field(default='ohe')\n",
    "    numerical_transform_method: str = field(default='yeo_johnson')\n",
    "    \n",
    "    task: str = field(default='classification')\n",
    "    mlp_division: int = field(default=4)\n",
    "    combine_feat_method: str = field(default='individual_mlps_on_cat_and_numerical_feats_then_concat')\n",
    "    mlp_dropout: float = field(default=0.1)\n",
    "    numerical_bn: bool = field(default=True)\n",
    "    use_simple_classifier: str = field(default=True)\n",
    "    mlp_act: str = field(default='relu')\n",
    "    gating_beta: float = field(default=0.2)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.column_info != self.column_info_path\n",
    "        if (self.column_info is None and self.column_info_path):\n",
    "            with open(self.column_info_path, mode='r') as f:\n",
    "                self.column_info = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87412485",
   "metadata": {},
   "source": [
    "## 3.2 - Select Data / Model / Training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccebbc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model choices available\n",
    "multimodal_model_classes = {\n",
    "    # key = (huggingface) model name\n",
    "    # val = multimodal-transformers ModelWithTabular class (each a subclass of their respective ModelForSequenceClassification)\n",
    "    'bert-base-uncased': BertWithTabular,\n",
    "    'distilbert-base-uncased': DistilBertWithTabular,\n",
    "    'roberta-base': RobertaWithTabular,\n",
    "    'vinai/bertweet-base': RobertaWithTabular,\n",
    "    'Twitter/twhin-bert-base': BertWithTabular, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Select the fine_tuned model\n",
    "fine_tuned_models_folder = '../data/models/'\n",
    "selected_model_folder_name = 'vinai__bertweet-base-200k'  # no error checking implemented so type carefully\n",
    "base_hf_model_name = 'vinai/bertweet-base'  # select from keys of `multimodal_model_classes` above\n",
    "\n",
    "model_path = Path(fine_tuned_models_folder, selected_model_folder_name)\n",
    "model_class = multimodal_model_classes[base_hf_model_name]\n",
    "\n",
    "# Step 2 - (mostly automatic) Choose a descriptive run name for this training/model.\n",
    "#   Suggested format: multimodal__[fine-tuned-model-name]__[YYYY-MM-DD]\n",
    "stub = selected_model_folder_name # 'distilbert-base-uncased-50k'\n",
    "date = pd.Timestamp.now().strftime(format=r\"%Y-%m-%d\")\n",
    "run_name = f\"multi-modal__{stub}__{date}\"\n",
    "\n",
    "# Step 3 - (mostly automatic) Choose a folder name for where to store the output of the model\n",
    "#   Will be created as a subfolder of `../data/models/multimodal`\n",
    "output_dir_name = f\"multi-modal__{stub}\"\n",
    "\n",
    "# Step 4 - Choose which columns from dataframe will be used.\n",
    "label_col = 'class_numeric'\n",
    "text_cols = ['cleaned_tweet']\n",
    "categorical_cols = [\n",
    "    'is_retweet',\n",
    "    'has_url',\n",
    "    'emoji_flagUS',\n",
    "    'emoji_police',\n",
    "    'emoji_check',\n",
    "    'emoji_exclamation',\n",
    "    'emoji_fist',\n",
    "    'emoji_collision',\n",
    "    'emoji_prohibited',\n",
    "    'emoji_loudcryface',\n",
    "    'emoji_smilinghearteye',\n",
    "    'emoji_fire',\n",
    "    'emoji_redheart',\n",
    "    'emoji_tearsjoy',\n",
    "    'emoji_thumbsup',\n",
    "    'emoji_claphands',\n",
    "    'emoji_blowingkiss',\n",
    "    'emoji_partypop',\n",
    "    'emoji_raisehands',\n",
    "    'region_United_States',\n",
    "    'region_Unknown',\n",
    "    'region_New_York_NY',\n",
    "    'region_United_Kingdom',\n",
    "    'region_Los_Angeles_CA',\n",
    "    'region_Boston_MA',\n",
    "    'region_London',\n",
    "    'region_New_York_and_the_World',\n",
    "    'region_New_York_City',\n",
    "    'region_Pale_Blue_Dot',\n",
    "    'region_Atlanta_GA',\n",
    "    'region_Australia',\n",
    "    'region_Global',\n",
    "    'region_Washington_DC',\n",
    "    'region_All_Other',\n",
    "    'multi_authors',\n",
    "]\n",
    "numericical_cols = [\n",
    "    'following', \n",
    "    'followers',\n",
    "    'updates',\n",
    "    'emoji_count',\n",
    "    'following_ratio',\n",
    "    'RUS_lett_count',\n",
    "    'num_dashes',\n",
    "    'num_commas',\n",
    "    'num_hashs',\n",
    "    'num_URLs',\n",
    "    'median_word_length',\n",
    "    'sentiment',\n",
    "    'emoji_sentiment',\n",
    "    'tweet_length',\n",
    "]\n",
    "\n",
    "# don't edit this\n",
    "column_info_dict = {\n",
    "    'text_cols': text_cols,\n",
    "    'num_cols': numericical_cols,\n",
    "    'cat_cols': categorical_cols,\n",
    "    'label_col': label_col,\n",
    "    'label_list': ['Authentic', 'Troll']\n",
    "}\n",
    "\n",
    "# Step 5 - specify dataset's dataframe and how many rows from dataset to feed\n",
    "#   also stratify by `label_col`\n",
    "#n_rows = 100000\n",
    "#dataset_dataframe = df_full.groupby(by=label_col).sample(n=(n_rows//2), random_state=42).reset_index()\n",
    "dataset_dataframe = df_full\n",
    "\n",
    "# Step 6 - Set model arguments\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=model_path,\n",
    "    tokenizer_name=base_hf_model_name,\n",
    "    config_name=base_hf_model_name,\n",
    "    num_labels=2,\n",
    ")\n",
    "\n",
    "# Step 7 - Set data arguments\n",
    "data_args = MultimodalDataTrainingArguments(\n",
    "    data_df=dataset_dataframe,\n",
    "    combine_feat_method='weighted_feature_sum_on_transformer_cat_and_numerical_feats',\n",
    "    column_info=column_info_dict,\n",
    "    task='classification',\n",
    "    categorical_encode_type=None,\n",
    "    numerical_transform_method=\"yeo_johnson\",\n",
    "    #use_simple_classifier=False,\n",
    ")\n",
    "\n",
    "# Step 8 - Set training arguments\n",
    "multimodal_parent_output_dir = '../data/models/multimodal/'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    ## file args\n",
    "    run_name=run_name,\n",
    "    output_dir=Path(multimodal_parent_output_dir, output_dir_name),\n",
    "    logging_dir=Path(multimodal_parent_output_dir, output_dir_name, \"runs\"),\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    "    ## training hyperparams\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=160,\n",
    "    per_device_eval_batch_size=160,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,   # comment out this line for DistilBERT\n",
    "    weight_decay=0.01,\n",
    "    seed=42,\n",
    "    ## eval/log strategies\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    log_level='warning',\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86cedf60",
   "metadata": {},
   "source": [
    "## 3.2 - Setup Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9715405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tokenizer associated with our chosen model type\n",
    "#   - let AutoTokenizer pick the class for now but confirm the expected one gets picked\n",
    "#       - BERTbase      -> BertTokenizerFast\n",
    "#       - DistilBERT    -> DistilBertTokenizerFast\n",
    "#       - RoBERTA       -> RobertaTokenizerFast\n",
    "#       - BERTweet      -> BertweetTokenizer\n",
    "#       - TwHIN-BERT    -> XLMRobertaTokenizerFast\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name)\n",
    "print(f\"Model class specified:\\t'{model_args.tokenizer_name}'\")\n",
    "print(f\"Tokenizer class chosen:\\t'{tokenizer.__class__.__name__}'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f3ad92a",
   "metadata": {},
   "source": [
    "## 3.3 - Training / Validation / Test Split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4898afa8",
   "metadata": {},
   "source": [
    "### 3.3.1 - Filter out any tweets that were used for fine tuning\n",
    "\n",
    "Especially important for testing and validation datasets to filter these out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660fbc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for a file `fine_tune_tweet_ids.json` within fine-tuned model directory\n",
    "fine_tuned_tweets_filename = 'fine_tune_tweet_ids.json'\n",
    "fine_tuned_tweets_file = Path(model_args.model_name_or_path, fine_tuned_tweets_filename)\n",
    "\n",
    "if (not fine_tuned_tweets_file.exists()):\n",
    "    print(\"Heads up: no fine-tuned tweet file found at\", fine_tuned_tweets_file)\n",
    "else:\n",
    "    # load the list of tweets already used for fine-tuning\n",
    "    tweet_id_list = json.load(fine_tuned_tweets_file.open(encoding='utf-8'))\n",
    "\n",
    "    # filter the dataframe\n",
    "    filtered_df = data_args.data_df.loc[~data_args.data_df['tweet_id'].isin(tweet_id_list)]\n",
    "\n",
    "    # calculate delta\n",
    "    delta_tweets = data_args.data_df.shape[0] - filtered_df.shape[0]\n",
    "    print(f\"Number of tweets filtered out:{delta_tweets:>9,}\")\n",
    "\n",
    "    # re-assign the filtered datafram\n",
    "    data_args.data_df = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf08e4a3",
   "metadata": {},
   "source": [
    "### 3.3.2 - Split (and Tokenize)\n",
    "\n",
    "The tokenizer defined above is invoked within `load_train_val_test_helper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c364a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aiming for a 70% / 15% / 15% split\n",
    "train_df, test_and_val_df = train_test_split(data_args.data_df, train_size=0.7, random_state=42, shuffle=True)\n",
    "test_df, val_df = train_test_split(test_and_val_df, train_size=0.5, shuffle=False)\n",
    "\n",
    "# using this helper function (potentially not intended for public interface) because it allows\n",
    "#   us to bring in our dataframe directly (rather than needing to import from CSV)\n",
    "train_dataset, val_dataset, test_dataset = load_train_val_test_helper(\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    test_df=test_df,\n",
    "    text_cols=data_args.column_info['text_cols'],\n",
    "    tokenizer=tokenizer,\n",
    "    label_col=data_args.column_info['label_col'],\n",
    "    label_list=data_args.column_info['label_list'],\n",
    "    categorical_cols=data_args.column_info['cat_cols'],\n",
    "    numerical_cols=data_args.column_info['num_cols'],\n",
    "    sep_text_token_str=tokenizer.sep_token,\n",
    "    categorical_encode_type=data_args.categorical_encode_type,\n",
    "    numerical_transformer_method=data_args.numerical_transform_method\n",
    ")\n",
    "\n",
    "print(f\"Training dataset:  {len(train_dataset):>12,} samples\")\n",
    "print(f\"Validation dataset:{len(val_dataset):>12,} samples\")\n",
    "print(f\"Testing dataset:   {len(test_dataset):>12,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f960cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output\n",
    "print(\">> Dataset:\\n\", train_dataset[0], end=\"\\n\\n\")\n",
    "print(\">> Origin DF:\\n\", train_df.iloc[0][text_cols + categorical_cols + numericical_cols + [\"class_numeric\"]], end=\"\\n\\n\")\n",
    "\n",
    "print(f\">> Unique labels: {np.unique(train_dataset.labels)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d941b71a",
   "metadata": {},
   "source": [
    "## 3.4 - Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0869d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help out with setup\n",
    "pprint(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a80751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a ðŸ¤— transformer config\n",
    "config = AutoConfig.from_pretrained(model_args.config_name)\n",
    "\n",
    "# setup the multimodal-transformers-specific TabularConfig\n",
    "tabular_config = TabularConfig(\n",
    "    num_labels=model_args.num_labels,                           # default is 2\n",
    "    cat_feat_dim=train_dataset.cat_feats.shape[1],              # number of cat feature columns\n",
    "    numerical_feat_dim=train_dataset.numerical_feats.shape[1],  # number of num feature columns\n",
    "    **vars(data_args)                                           # dump in everything else as kwarg\n",
    ")\n",
    "\n",
    "# add TabularConfig to ðŸ¤— transformer config\n",
    "config.tabular_config = tabular_config\n",
    "\n",
    "# define which class we're going to use for model\n",
    "#model = DistilBertWithTabular.from_pretrained(\n",
    "model = model_class.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_args.model_name_or_path,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd246fad",
   "metadata": {},
   "source": [
    "## 3.5 - Setup Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeff92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binary_classification_metrics(eval_pred: EvalPrediction) -> dict[str, float]:\n",
    "    logits_obj, labels = eval_pred\n",
    "\n",
    "    if (isinstance(logits_obj, tuple)):\n",
    "        # not quite sure what is causing a tuple to be returned here but this line of code cost 3+ hours of sadness\n",
    "        logits=logits_obj[0]\n",
    "    else:\n",
    "        logits=logits_obj\n",
    "\n",
    "    # for each row of eval_pred, pick the larger column and return its column index (0 or 1)\n",
    "    #   - result looks like: [0, 1, 0, 0, 1, ...]\n",
    "    predictions_as_labels = np.argmax(logits, axis=1)   # axis=1 gives max column value in each row\n",
    "\n",
    "    # for each row of eval_pred, apply softmax to ~normalize as scores summing to 1, \n",
    "    #   and grab the probability of class=1 (i.e. second column of each row)\n",
    "    #   - result looks like: [0.993, 0.003, 0.234, ...]\n",
    "    predictions_as_probs = softmax(logits, axis=1)[:,1]\n",
    "\n",
    "    # binary classification metrics\n",
    "    #   label-based\n",
    "    metric_accuracy = accuracy_score(y_true=labels, y_pred=predictions_as_labels)\n",
    "    metric_f1score = f1_score(y_true=labels, y_pred=predictions_as_labels, zero_division=0)\n",
    "    metric_precision = precision_score(y_true=labels, y_pred=predictions_as_labels, zero_division=0)\n",
    "    metric_recall = recall_score(y_true=labels, y_pred=predictions_as_labels, zero_division=0)\n",
    "\n",
    "    #   probability-based\n",
    "    metric_roc_auc = roc_auc_score(y_true=labels, y_score=predictions_as_probs)\n",
    "    metric_brier = brier_score_loss(y_true=labels, y_prob=predictions_as_probs)\n",
    "\n",
    "    #   confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, predictions_as_labels, labels=[0, 1]).ravel()\n",
    "\n",
    "    # package up the results\n",
    "    result = {\n",
    "        'accuracy': metric_accuracy,\n",
    "        'f1score': metric_f1score,\n",
    "        'precision': metric_precision,\n",
    "        'recall': metric_recall,\n",
    "        'roc_auc': metric_roc_auc,\n",
    "        'brier_score': metric_recall,\n",
    "        'tn': tn.item(),\n",
    "        'fp': fp.item(),\n",
    "        'fn': fn.item(),\n",
    "        'tp': tp.item()\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65a56d80",
   "metadata": {},
   "source": [
    "## 3.6 - Setup / Run Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_training_start = pd.Timestamp.now()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_binary_classification_metrics\n",
    ")\n",
    "\n",
    "result = trainer.train()\n",
    "\n",
    "time_training_stop = pd.Timestamp.now()\n",
    "time_training = time_training_stop - time_training_start\n",
    "print(\"\\nTraining duration:\", str(time_training), end=\"\\n\\n\")\n",
    "\n",
    "# debug\n",
    "print_summary(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aabf3ed",
   "metadata": {},
   "source": [
    "## 3.7 - Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fba9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()    # defaults to self.args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63964e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional - delete checkpoint directories\n",
    "checkpoint_dirs = [\n",
    "    f\"{trainer.args.output_dir}/{directory}\"\n",
    "    for directory in os.listdir(trainer.args.output_dir)\n",
    "        if (\n",
    "            os.path.isdir(os.path.join(trainer.args.output_dir, directory))\n",
    "            and\n",
    "            directory.startswith('checkpoint')\n",
    "        )\n",
    "]\n",
    "\n",
    "for checkpoint_dir in checkpoint_dirs:\n",
    "    print(f\"Attempting to delete '{checkpoint_dir}' ...\", end='')\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    print(f\" success\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0371be7",
   "metadata": {},
   "source": [
    "## 3.8 - Generate Model Predictions and Evaluate\n",
    "\n",
    "The `Trainer.predict()` method also calculates performance metrics so we'll cover both tasks with one loop through our test dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "737b266f",
   "metadata": {},
   "source": [
    "### 3.8.1 - Calculate Predictions with Test Dataset\n",
    "\n",
    "And save the predictions as JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "predictions_output = trainer.predict(test_dataset, metric_key_prefix='test')\n",
    "\n",
    "# predictions output is a subclass of NamedTuple, so to save to JSON we convert to a dict first\n",
    "#   Note: according to python docs, the leading underscore is to avoid name conflicts, not as the usual \"discouraged from use\" meaning\n",
    "#   Source: https://docs.python.org/3.10/library/collections.html#collections.somenamedtuple._asdict\n",
    "predictions_dict = predictions_output._asdict()\n",
    "\n",
    "# process the incoming object depending on its type\n",
    "if (isinstance(predictions_dict['predictions'], tuple)):\n",
    "    predictions = predictions_dict['predictions'][0]\n",
    "else:\n",
    "    predictions = predictions_dict['predictions']\n",
    "\n",
    "# overwrite the dict values with vanilla Python types\n",
    "predictions_dict['run_name'] = trainer.args.run_name\n",
    "predictions_dict['predictions'] = predictions.tolist()\n",
    "predictions_dict['label_ids'] = predictions_dict['label_ids'].tolist()\n",
    "\n",
    "# sort the keys\n",
    "dict_order = ['run_name', 'metrics', 'predictions', 'label_ids']\n",
    "predictions_dict = {key: predictions_dict[key] for key in dict_order}\n",
    "\n",
    "# save predictions to `output_dir`\n",
    "predictions_file = Path(trainer.args.output_dir, 'predictions.json')\n",
    "with predictions_file.open(mode='w', encoding='utf-8') as fp:\n",
    "    json.dump(predictions_dict, fp, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e162fe72",
   "metadata": {},
   "source": [
    "### 3.8.2 - Display and Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274c7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "pprint(predictions_dict['metrics'], sort_dicts=False)\n",
    "\n",
    "# consolidate\n",
    "metrics_dict = {\n",
    "    'run_name': trainer.args.run_name,\n",
    "    'final_test_metrics': predictions_dict['metrics'],\n",
    "    'trainer_log': trainer.state.log_history\n",
    "}\n",
    "\n",
    "# save\n",
    "metrics_file = Path(trainer.args.output_dir, 'model_metrics.json')\n",
    "with metrics_file.open(mode='w', encoding='utf-8') as fp:\n",
    "    json.dump(metrics_dict, fp, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d338443a",
   "metadata": {},
   "source": [
    "### 3.8.3 - All The Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b129c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidate\n",
    "all_args = {\n",
    "    'run_name': trainer.args.run_name,\n",
    "    'column_info_dict': column_info_dict,\n",
    "    'data_args': {k: v for (k, v) in data_args.__dict__.items() if (k != 'data_df')},\n",
    "    'model_args': model_args.__dict__,\n",
    "    'training_args': training_args.__dict__\n",
    "}\n",
    "\n",
    "# fix args that doesn't json serialize\n",
    "all_args['model_args']['model_name_or_path'] = all_args['model_args']['model_name_or_path'].as_posix()\n",
    "all_args['training_args'][\"__cached__setup_devices\"] = str(all_args['training_args'][\"__cached__setup_devices\"])\n",
    "\n",
    "# save\n",
    "all_args_file = Path(trainer.args.output_dir, 'all_args.json')\n",
    "with all_args_file.open(mode='w', encoding='utf-8') as fp:\n",
    "    json.dump(all_args, fp, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb284867",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m103"
  },
  "kernelspec": {
   "display_name": "tf290_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "65a6dd32d71ed4a2e5ac9ab3f52d3aeee49f01a00467a63b19dc274a1d27154b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
