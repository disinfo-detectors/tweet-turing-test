{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "add0cef4-a4de-47c5-a9a0-e6b818d6fa2d",
   "metadata": {},
   "source": [
    "# Tweet Turing Test: Detecting Disinformation on Twitter  \n",
    "\n",
    "|          | Group #2 - Disinformation Detectors                     |\n",
    "|---------:|---------------------------------------------------------|\n",
    "| Members  | John Johnson, Katy Matulay, Justin Minnion, Jared Rubin |\n",
    "| Notebook | `05_multimodal.ipynb`                                   |\n",
    "| Purpose  | Combining tabular data with a BERT transformer.         |\n",
    "\n",
    "(todo: description)\n",
    "\n",
    "*Assumptions*  \n",
    " - The dataset being used has binary class labels following convention: 0 = authentic tweet; 1 = troll tweet\n",
    " - The execution environment has internet access (to download models from huggingface.co)\n",
    " - The execution environment has a CUDA-capable GPU available\n",
    "\n",
    "*General Notes*\n",
    " - Notebook kernel must be completely restarted between runs to release reserved VRAM from the GPU.\n",
    " - Notebook contains our usual code to load dataset file from GCP bucket, but model files are always saved locally regardless of `local_or_cloud` setting.\n",
    " - Notebook is based on a [notebook by georgian-io (github.com)](https://github.com/georgian-io/Multimodal-Toolkit/blob/master/notebooks/text_w_tabular_classification.ipynb) from their [Multimodal-Toolkit repository (github.com)](https://github.com/georgian-io/Multimodal-Toolkit) and [accompanying blog post (medium.com)](https://medium.com/georgian-impact-blog/how-to-incorporate-tabular-data-with-huggingface-transformers-b70ac45fcfb4).\n",
    " \n",
    " __*Note on `multimodal-transformers` Package Version*__\n",
    " - As of this writing (07-Mar-2023), the master branch of georgian-io's multimodal-transformers package [only supports ðŸ¤— transformers version 3.1](https://github.com/georgian-io/Multimodal-Toolkit/blob/7ac1e713570ab81fbbbdc32caa01c6d35607f840/setup.py#L8) (the current released transformers version is 4.26.1). \n",
    " - A georgian-io contributor, however, has been actively working on a separate branch to refactor the code to support ðŸ¤— transformers version 4.26.1. \n",
    " - The contributor has proposed [a pull request (PR #31)](https://github.com/georgian-io/Multimodal-Toolkit/pull/31) signalling they're ready to merge in their updated code, so we have installed the package using the GitHub branch corresponding to that pull request (branch \"`akash/update`\" and PR#31). To install this **very specific** version of multimodal-transformers from this branch, use the below `pip` command. This clones the repository for the latest commit associated with the subject pull request:  \n",
    "\n",
    "> `pip install 'multimodal-transformers @ https://github.com/georgian-io/Multimodal-Toolkit/commit/242180a31a0793e686197b5ccbd90368cdcfbb15`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a7349-cf9c-461b-aebd-de5542e2ded8",
   "metadata": {},
   "source": [
    "# 1 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d88ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm installed version of `transformers` matches the version required by `multimodal_transformers`\n",
    "supported_transformers_versions = {'4.25.1', '4.26.1'}\n",
    "\n",
    "import transformers\n",
    "assert (transformers.__version__ in supported_transformers_versions), \\\n",
    "    \"Unsupported version of 'transformers' installed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c0673b-7027-407e-8435-e29090105df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports from Python standard library\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "import shutil\n",
    "import zipfile\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "# imports requiring installation\n",
    "#   connection to Google Cloud Storage\n",
    "from google.cloud import storage            # pip install google-cloud-storage\n",
    "from google.oauth2 import service_account   # pip install google-auth\n",
    "\n",
    "#  data science packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pynvml import *    # for debugging\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, auc, brier_score_loss, confusion_matrix, f1_score, precision_recall_curve, \n",
    "    precision_score, recall_score, roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "\n",
    "# ðŸ¤— (huggingface) packages\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig, AutoModel, \n",
    "    BertTokenizerFast, DistilBertTokenizerFast, RobertaTokenizerFast, BertweetTokenizer, XLMRobertaTokenizerFast,\n",
    "    TrainingArguments, Trainer, EvalPrediction, \n",
    "    set_seed\n",
    ")\n",
    "\n",
    "# Georgian packages\n",
    "#   Note: see note in notebook header about installing a highly specific version of `multimodal_transformers`\n",
    "from multimodal_transformers.data.load_data import load_train_val_test_helper   # shhh, didn't have a leading underscore\n",
    "from multimodal_transformers.model import (\n",
    "    TabularConfig, AutoModelWithTabular, BertWithTabular, DistilBertWithTabular, RobertaWithTabular\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7494318-b0ee-4452-80fa-c959050a6c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports from tweet_turing.py\n",
    "import tweet_turing as tur      # note - different import approach from prior notebooks\n",
    "\n",
    "# imports from tweet_turing_paths.py\n",
    "from tweet_turing_paths import local_data_paths, local_snapshot_paths, gcp_data_paths, \\\n",
    "    gcp_snapshot_paths, gcp_project_name, gcp_bucket_name, gcp_key_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eee18cdf-9d6c-4315-90ad-3e470718b1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pandas options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# sns.set_theme(context='paper', style='ticks', palette='bright', font='georgia', rc={'figure.dpi': 200})\n",
    "# twitter_colors = {\n",
    "#     'blue': r\"#1DA1F2\",     # source: https://usbrandcolors.com/twitter-colors/\n",
    "#     'black': r\"#14171A\",\n",
    "#     'dark gray': r\"#657786\",\n",
    "#     'dark blue': r\"#0f5580\" # \"blue\" with brightness changed 88% to 50%, non-standard color\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7124b0f-b27f-450a-ba49-f0560c5b38b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Local or Cloud?\n",
    "\n",
    "Decide here whether to run notebook with local data or GCP bucket data\n",
    " - if the working directory of this notebook has a \"../data/\" folder with data loaded (e.g. working on local computer or have data files loaded to a cloud VM) then use the \"local files\" option and comment out the \"gcp bucket files\" option\n",
    " - if this notebook is being run from a GCP VM (preferrably in the `us-central1` location) then use the \"gcp bucket files\" option and comment out the \"local files\" option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d579d5d-f289-4b49-a1aa-f72f04d503b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# option: local files\n",
    "local_or_cloud: str = \"local\"   # comment/uncomment this line or next\n",
    "\n",
    "# option: gcp bucket files\n",
    "#local_or_cloud: str = \"cloud\"   # comment/uncomment this line or previous\n",
    "\n",
    "# don't comment/uncomment for remainder of cell\n",
    "if (local_or_cloud == \"local\"):\n",
    "    data_paths = local_data_paths\n",
    "    snapshot_paths = local_snapshot_paths\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    data_paths = gcp_data_paths\n",
    "    snapshot_paths = gcp_snapshot_paths\n",
    "else:\n",
    "    raise ValueError(\"Variable 'local_or_cloud' can only take on one of two values, 'local' or 'cloud'.\")\n",
    "    # subsequent cells will not do this final \"else\" check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b206dfc-7e39-49a1-a48d-71a4cb46e863",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell only needs to run its code if local_or_cloud==\"cloud\"\n",
    "#   (though it is harmless if run when local_or_cloud==\"local\")\n",
    "gcp_storage_client: storage.Client = None\n",
    "gcp_bucket: storage.Bucket = None\n",
    "\n",
    "if (local_or_cloud == \"cloud\"):\n",
    "    gcp_storage_client = tur.get_gcp_storage_client(project_name=gcp_project_name, key_file=gcp_key_file)\n",
    "    gcp_bucket = tur.get_gcp_bucket(storage_client=gcp_storage_client, bucket_name=gcp_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46318833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# from huggingface tutorial: https://huggingface.co/docs/transformers/perf_train_gpu_one#efficient-training-on-a-single-gpu\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fa7ee9f-857e-4c69-bd3f-803a970c3199",
   "metadata": {},
   "source": [
    "# 2 - Load Dataset\n",
    "\n",
    "Starting with the full dataset (with engineered features) from notebook **`06_feature_engineering.ipynb`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e19c3b-cab5-4e4c-a6b8-950f4c0dbc58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note this cell requires package `pyarrow` to be installed in environment\n",
    "parq_filename: str = \"06data_full_final_en2.parquet.gz\"\n",
    "parq_path: str = f\"{snapshot_paths['parq_snapshot']}{parq_filename}\"\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    df_full = pd.read_parquet(parq_path, engine='pyarrow')\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    df_full = tur.get_gcp_object_from_parq_as_df(bucket=gcp_bucket, object_name=parq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96506d5c-cd41-4382-9518-55132bfad5d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3596578 entries, 0 to 3596577\n",
      "Data columns (total 54 columns):\n",
      " #   Column                         Dtype  \n",
      "---  ------                         -----  \n",
      " 0   external_author_id             string \n",
      " 1   author                         string \n",
      " 2   following                      int64  \n",
      " 3   followers                      int64  \n",
      " 4   updates                        int64  \n",
      " 5   is_retweet                     int64  \n",
      " 6   tweet_id                       string \n",
      " 7   has_url                        int64  \n",
      " 8   emoji_count                    int64  \n",
      " 9   following_ratio                float64\n",
      " 10  class_numeric                  int8   \n",
      " 11  RUS_lett_count                 int64  \n",
      " 12  emoji_flagUS                   int64  \n",
      " 13  emoji_police                   int64  \n",
      " 14  emoji_check                    int64  \n",
      " 15  emoji_exclamation              int64  \n",
      " 16  emoji_fist                     int64  \n",
      " 17  emoji_collision                int64  \n",
      " 18  emoji_prohibited               int64  \n",
      " 19  emoji_loudcryface              int64  \n",
      " 20  emoji_smilinghearteye          int64  \n",
      " 21  emoji_fire                     int64  \n",
      " 22  emoji_redheart                 int64  \n",
      " 23  emoji_tearsjoy                 int64  \n",
      " 24  emoji_thumbsup                 int64  \n",
      " 25  emoji_claphands                int64  \n",
      " 26  emoji_blowingkiss              int64  \n",
      " 27  emoji_partypop                 int64  \n",
      " 28  emoji_raisehands               int64  \n",
      " 29  region_United_States           int64  \n",
      " 30  region_Unknown                 int64  \n",
      " 31  region_New_York_NY             int64  \n",
      " 32  region_United_Kingdom          int64  \n",
      " 33  region_Los_Angeles_CA          int64  \n",
      " 34  region_Boston_MA               int64  \n",
      " 35  region_London                  int64  \n",
      " 36  region_New_York_and_the_World  int64  \n",
      " 37  region_New_York_City           int64  \n",
      " 38  region_Pale_Blue_Dot           int64  \n",
      " 39  region_Atlanta_GA              int64  \n",
      " 40  region_Australia               int64  \n",
      " 41  region_Global                  int64  \n",
      " 42  region_Washington_DC           int64  \n",
      " 43  region_All_Other               int64  \n",
      " 44  multi_authors                  int64  \n",
      " 45  num_dashes                     int64  \n",
      " 46  num_commas                     int64  \n",
      " 47  num_hashs                      int64  \n",
      " 48  num_URLs                       int64  \n",
      " 49  median_word_length             float64\n",
      " 50  cleaned_tweet                  object \n",
      " 51  sentiment                      float64\n",
      " 52  emoji_sentiment                float64\n",
      " 53  tweet_length                   int64  \n",
      "dtypes: float64(4), int64(45), int8(1), object(1), string(3)\n",
      "memory usage: 2.5 GB\n"
     ]
    }
   ],
   "source": [
    "df_full.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddf47eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    50000\n",
       "1    50000\n",
       "Name: class_numeric, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a smaller slice of full dataset for testing\n",
    "n_tweets = 100000\n",
    "\n",
    "df_small = df_full.groupby(by='class_numeric').sample(n=(n_tweets//2), random_state=42).reset_index()\n",
    "\n",
    "# confirm stratefied\n",
    "df_small['class_numeric'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "808911e9",
   "metadata": {},
   "source": [
    "# 3 - Setup Experimental Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf46bdf6",
   "metadata": {},
   "source": [
    "## 3.1 - Define dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0458b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataclasses to store the experiment parameters\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: str = field()\n",
    "    config_name:  Optional[str] = field(default=None)\n",
    "    tokenizer_name: Optional[str] = field(default=None)\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    num_labels: int = field(default=2)\n",
    "\n",
    "@dataclass\n",
    "class MultimodalDataTrainingArguments:\n",
    "    #data_path: str = field()\n",
    "    data_df: pd.DataFrame = field()     # modified to use dataframe\n",
    "\n",
    "    column_info_path: str = field(default=None)\n",
    "    column_info: dict = field(default=None)\n",
    "\n",
    "    categorical_encode_type: str = field(default='ohe')\n",
    "    numerical_transform_method: str = field(default='yeo_johnson')\n",
    "    \n",
    "    task: str = field(default='classification')\n",
    "    mlp_division: int = field(default=4)\n",
    "    combine_feat_method: str = field(default='individual_mlps_on_cat_and_numerical_feats_then_concat')\n",
    "    mlp_dropout: float = field(default=0.1)\n",
    "    numerical_bn: bool = field(default=True)\n",
    "    use_simple_classifier: str = field(default=True)\n",
    "    mlp_act: str = field(default='relu')\n",
    "    gating_beta: float = field(default=0.2)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.column_info != self.column_info_path\n",
    "        if (self.column_info is None and self.column_info_path):\n",
    "            with open(self.column_info_path, mode='r') as f:\n",
    "                self.column_info = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87412485",
   "metadata": {},
   "source": [
    "## 3.2 - Select Data / Model / Training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fe2e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Select the fine_tuned model\n",
    "fine_tuned_models_folder = '../data/models/'\n",
    "selected_model_folder_name = 'distilbert-base-uncased-50k'\n",
    "model_path = fine_tuned_models_folder + selected_model_folder_name\n",
    "\n",
    "base_hf_model_name = 'distilbert-base-uncased'  # used to make the config and tokenizer objects\n",
    "\n",
    "# Step 2 - (mostly automatic) Choose a descriptive run name for this training/model.\n",
    "#   Suggested format: multimodal__[fine-tuned-model-name]__[YYYY-MM-DD]\n",
    "stub = selected_model_folder_name # 'distilbert-base-uncased-50k'\n",
    "date = pd.Timestamp.now().strftime(format=r\"%Y-%m-%d\")\n",
    "run_name = f\"multi-modal__{stub}__{date}\"\n",
    "\n",
    "# Step 3 - (mostly automatic) Choose a folder name for where to store the output of the model\n",
    "#   Will be created as a subfolder of `../data/models/multimodal`\n",
    "output_dir_name = f\"multi-modal__{stub}\"\n",
    "\n",
    "# Step 4 - Choose which columns from dataframe will be used.\n",
    "label_col = 'class_numeric'\n",
    "text_cols = ['cleaned_tweet']\n",
    "categorical_cols = [\n",
    "    'is_retweet',\n",
    "    'has_url',\n",
    "    'emoji_flagUS',\n",
    "    'emoji_police',\n",
    "    'emoji_check',\n",
    "    'emoji_exclamation',\n",
    "    'emoji_fist',\n",
    "    'emoji_collision',\n",
    "    'emoji_prohibited',\n",
    "    'emoji_loudcryface',\n",
    "    'emoji_smilinghearteye',\n",
    "    'emoji_fire',\n",
    "    'emoji_redheart',\n",
    "    'emoji_tearsjoy',\n",
    "    'emoji_thumbsup',\n",
    "    'emoji_claphands',\n",
    "    'emoji_blowingkiss',\n",
    "    'emoji_partypop',\n",
    "    'emoji_raisehands',\n",
    "    'region_United_States',\n",
    "    'region_Unknown',\n",
    "    'region_New_York_NY',\n",
    "    'region_United_Kingdom',\n",
    "    'region_Los_Angeles_CA',\n",
    "    'region_Boston_MA',\n",
    "    'region_London',\n",
    "    'region_New_York_and_the_World',\n",
    "    'region_New_York_City',\n",
    "    'region_Pale_Blue_Dot',\n",
    "    'region_Atlanta_GA',\n",
    "    'region_Australia',\n",
    "    'region_Global',\n",
    "    'region_Washington_DC',\n",
    "    'region_All_Other',\n",
    "    'multi_authors',\n",
    "]\n",
    "numericical_cols = [\n",
    "    'following', \n",
    "    'followers',\n",
    "    'updates',\n",
    "    'emoji_count',\n",
    "    'following_ratio',\n",
    "    'RUS_lett_count',\n",
    "    'num_dashes',\n",
    "    'num_commas',\n",
    "    'num_hashs',\n",
    "    'num_URLs',\n",
    "    'median_word_length',\n",
    "    'sentiment',\n",
    "    'emoji_sentiment',\n",
    "    'tweet_length',\n",
    "]\n",
    "\n",
    "# don't edit this\n",
    "column_info_dict = {\n",
    "    'text_cols': text_cols,\n",
    "    'num_cols': numericical_cols,\n",
    "    'cat_cols': categorical_cols,\n",
    "    'label_col': label_col,\n",
    "    'label_list': ['Authentic', 'Troll']    # todo: determine if order matters (e.g. 0,1 or 1,0)\n",
    "}\n",
    "\n",
    "# Step 5 - specify dataset's dataframe and how many rows from dataset to feed\n",
    "n_rows = 100000\n",
    "dataset_dataframe = df_full.groupby(by=label_col).sample(n=(n_rows//2), random_state=42).reset_index()\n",
    "\n",
    "# Step 6 - Set model arguments\n",
    "model_args = ModelArguments(\n",
    "    # model_name_or_path='distilbert-base-uncased',   # todo: load local, fine-tuned model\n",
    "    # tokenizer_name='distilbert-base-uncased',\n",
    "    # config_name='distilbert-base-uncased',\n",
    "    model_name_or_path=model_path,\n",
    "    tokenizer_name=base_hf_model_name,\n",
    "    config_name=base_hf_model_name,\n",
    "    num_labels=2,\n",
    ")\n",
    "\n",
    "# Step 7 - Set data arguments\n",
    "data_args = MultimodalDataTrainingArguments(\n",
    "    data_df=dataset_dataframe,\n",
    "    combine_feat_method='weighted_feature_sum_on_transformer_cat_and_numerical_feats',\n",
    "    column_info=column_info_dict,\n",
    "    task='classification',\n",
    "    categorical_encode_type=None,\n",
    "    numerical_transform_method=\"yeo_johnson\",\n",
    "\n",
    ")\n",
    "\n",
    "# Step 8 - Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../data/models/multimodal/{output_dir_name}\",\n",
    "    logging_dir=f\"../data/models/multimodal/{output_dir_name}/runs\",\n",
    "    #overwrite_output_dir=True,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    "    # do_train=True,\n",
    "    # do_eval=True,\n",
    "    \n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    gradient_accumulation_steps=4,\n",
    "    #gradient_checkpointing=True,   # comment out this line for DistilBERT\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    log_level='warning',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86cedf60",
   "metadata": {},
   "source": [
    "## 3.2 - Setup Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9715405f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model class specified:\t'distilbert-base-uncased'\n",
      "Tokenizer class chosen:\t'DistilBertTokenizerFast'\n"
     ]
    }
   ],
   "source": [
    "# use tokenizer associated with our chosen model type\n",
    "#   - let AutoTokenizer pick the class for now but confirm the expected one gets picked\n",
    "#       - BERTbase      -> BertTokenizerFast\n",
    "#       - DistilBERT    -> DistilBertTokenizerFast\n",
    "#       - RoBERTA       -> RobertaTokenizerFast\n",
    "#       - BERTweet      -> BertweetTokenizer\n",
    "#       - TwHIN-BERT    -> XLMRobertaTokenizerFast\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name)\n",
    "print(f\"Model class specified:\\t'{model_args.tokenizer_name}'\")\n",
    "print(f\"Tokenizer class chosen:\\t'{tokenizer.__class__.__name__}'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f3ad92a",
   "metadata": {},
   "source": [
    "## 3.3 - Training / Validation / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660fbc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo - filter out any tweets (by `tweet_id``) that were used in fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3c364a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aiming for a 70% / 15% / 15% split\n",
    "train_df, test_and_val_df = train_test_split(data_args.data_df, train_size=0.7, random_state=42, shuffle=True)\n",
    "test_df, val_df = train_test_split(test_and_val_df, train_size=0.5, shuffle=False)\n",
    "\n",
    "# using this helper function (potentially not intended for public interface) because it allows\n",
    "#   us to bring in our dataframe directly (rather than needing to import from CSV)\n",
    "train_dataset, val_dataset, test_dataset = load_train_val_test_helper(\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    test_df=test_df,\n",
    "    text_cols=data_args.column_info['text_cols'],\n",
    "    tokenizer=tokenizer,\n",
    "    label_col=data_args.column_info['label_col'],\n",
    "    label_list=data_args.column_info['label_list'],\n",
    "    categorical_cols=data_args.column_info['cat_cols'],\n",
    "    numerical_cols=data_args.column_info['num_cols'],\n",
    "    sep_text_token_str=tokenizer.sep_token,\n",
    "    categorical_encode_type=data_args.categorical_encode_type,\n",
    "    numerical_transformer_method=data_args.numerical_transform_method\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f960cf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Dataset:\n",
      " {'input_ids': tensor([  101, 19387,  3566,  2015,  5157,  1001,  4409, 16700,  2031,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(1, dtype=torch.int8), 'cat_feats': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'numerical_feats': tensor([ 0.3881, -0.6338,  0.4959, -0.1926,  1.3314, -0.0120, -0.4168, -0.5575,\n",
      "         1.1617, -1.1816,  0.6796, -0.4298, -0.0527, -1.5379])}\n",
      "\n",
      ">> Origin DF:\n",
      " cleaned_tweet                    RT Moms Demand #gunsense have\n",
      "is_retweet                                                   0\n",
      "has_url                                                      0\n",
      "emoji_flagUS                                                 0\n",
      "emoji_police                                                 0\n",
      "emoji_check                                                  0\n",
      "emoji_exclamation                                            0\n",
      "emoji_fist                                                   0\n",
      "emoji_collision                                              0\n",
      "emoji_prohibited                                             0\n",
      "emoji_loudcryface                                            0\n",
      "emoji_smilinghearteye                                        0\n",
      "emoji_fire                                                   0\n",
      "emoji_redheart                                               0\n",
      "emoji_tearsjoy                                               0\n",
      "emoji_thumbsup                                               0\n",
      "emoji_claphands                                              0\n",
      "emoji_blowingkiss                                            0\n",
      "emoji_partypop                                               0\n",
      "emoji_raisehands                                             0\n",
      "region_United_States                                         1\n",
      "region_Unknown                                               0\n",
      "region_New_York_NY                                           0\n",
      "region_United_Kingdom                                        0\n",
      "region_Los_Angeles_CA                                        0\n",
      "region_Boston_MA                                             0\n",
      "region_London                                                0\n",
      "region_New_York_and_the_World                                0\n",
      "region_New_York_City                                         0\n",
      "region_Pale_Blue_Dot                                         0\n",
      "region_Atlanta_GA                                            0\n",
      "region_Australia                                             0\n",
      "region_Global                                                0\n",
      "region_Washington_DC                                         0\n",
      "region_All_Other                                             0\n",
      "multi_authors                                                0\n",
      "following                                                 2702\n",
      "followers                                                 1861\n",
      "updates                                                   1376\n",
      "emoji_count                                                  0\n",
      "following_ratio                                       1.451128\n",
      "RUS_lett_count                                               0\n",
      "num_dashes                                                   0\n",
      "num_commas                                                   0\n",
      "num_hashs                                                    1\n",
      "num_URLs                                                     0\n",
      "median_word_length                                         5.0\n",
      "sentiment                                               -0.128\n",
      "emoji_sentiment                                            0.0\n",
      "tweet_length                                                29\n",
      "class_numeric                                                1\n",
      "Name: 76513, dtype: object\n",
      "\n",
      ">> Unique labels: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# check output\n",
    "print(\">> Dataset:\\n\", train_dataset[0], end=\"\\n\\n\")\n",
    "print(\">> Origin DF:\\n\", train_df.iloc[0][text_cols + categorical_cols + numericical_cols + [\"class_numeric\"]], end=\"\\n\\n\")\n",
    "\n",
    "print(f\">> Unique labels: {np.unique(train_dataset.labels)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d941b71a",
   "metadata": {},
   "source": [
    "## 3.4 - Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85a80751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertWithTabular were not initialized from the model checkpoint at ../data/models/distilbert-base-uncased-50k and are newly initialized: ['tabular_combiner.cat_layer.weight', 'tabular_combiner.weight_num', 'tabular_combiner.num_bn.running_mean', 'tabular_combiner.layer_norm.weight', 'tabular_combiner.num_bn.num_batches_tracked', 'tabular_combiner.num_bn.running_var', 'tabular_combiner.cat_layer.bias', 'tabular_combiner.num_layer.bias', 'tabular_combiner.num_bn.bias', 'tabular_classifier.bias', 'tabular_combiner.num_bn.weight', 'tabular_combiner.layer_norm.bias', 'tabular_classifier.weight', 'tabular_combiner.num_layer.weight', 'tabular_combiner.weight_cat']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# make a ðŸ¤— transformer config\n",
    "config = AutoConfig.from_pretrained(model_args.config_name)\n",
    "\n",
    "# setup the multimodal-transformers-specific TabularConfig\n",
    "tabular_config = TabularConfig(\n",
    "    num_labels=model_args.num_labels,                           # default is 2\n",
    "    cat_feat_dim=train_dataset.cat_feats.shape[1],              # number of cat feature columns\n",
    "    numerical_feat_dim=train_dataset.numerical_feats.shape[1],  # number of num feature columns\n",
    "    **vars(data_args)                                           # dump in everything else as kwarg\n",
    ")\n",
    "\n",
    "# add TabularConfig to ðŸ¤— transformer config\n",
    "config.tabular_config = tabular_config\n",
    "\n",
    "# define which class we're going to use for model\n",
    "# debug -> get working with distilbert first\n",
    "model = DistilBertWithTabular.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_args.model_name_or_path,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd246fad",
   "metadata": {},
   "source": [
    "## 3.5 - Setup Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daeff92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binary_classification_metrics(eval_pred: EvalPrediction) -> dict[str, float]:\n",
    "    logits_obj, labels = eval_pred\n",
    "\n",
    "    if (isinstance(logits_obj, tuple)):\n",
    "        # not quite sure what is causing a tuple to be returned here but this line of code cost 3+ hours of sadness\n",
    "        logits=logits_obj[0]\n",
    "    else:\n",
    "        logits=logits_obj\n",
    "\n",
    "    # for each row of eval_pred, pick the larger column and return its column index (0 or 1)\n",
    "    #   - result looks like: [0, 1, 0, 0, 1, ...]\n",
    "    predictions_as_labels = np.argmax(logits, axis=1)   # axis=1 gives max column value in each row\n",
    "\n",
    "    # for each row of eval_pred, apply softmax to ~normalize as scores summing to 1, \n",
    "    #   and grab the probability of class=1 (i.e. second column of each row)\n",
    "    #   - result looks like: [0.993, 0.003, 0.234, ...]\n",
    "    predictions_as_probs = softmax(logits, axis=1)[:,1]\n",
    "\n",
    "    # binary classification metrics\n",
    "    #   label-based\n",
    "    metric_accuracy = accuracy_score(y_true=labels, y_pred=predictions_as_labels)\n",
    "    metric_f1score = f1_score(y_true=labels, y_pred=predictions_as_labels, zero_division=0)\n",
    "    metric_precision = precision_score(y_true=labels, y_pred=predictions_as_labels, zero_division=0)\n",
    "    metric_recall = recall_score(y_true=labels, y_pred=predictions_as_labels, zero_division=0)\n",
    "\n",
    "    #   probability-based\n",
    "    metric_roc_auc = roc_auc_score(y_true=labels, y_score=predictions_as_probs)\n",
    "    metric_brier = brier_score_loss(y_true=labels, y_prob=predictions_as_probs)\n",
    "\n",
    "    #   confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, predictions_as_labels, labels=[0, 1]).ravel()\n",
    "\n",
    "    # package up the results\n",
    "    result = {\n",
    "        'accuracy': metric_accuracy,\n",
    "        'f1score': metric_f1score,\n",
    "        'precision': metric_precision,\n",
    "        'recall': metric_recall,\n",
    "        'roc_auc': metric_roc_auc,\n",
    "        'brier_score': metric_recall,\n",
    "        'tn': tn.item(),\n",
    "        'fp': fp.item(),\n",
    "        'fn': fn.item(),\n",
    "        'tp': tp.item()\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa0ed6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values in 'predictions':\n",
      " [[-2.89058518  3.39713144]\n",
      " [ 2.25393295 -2.2034111 ]\n",
      " [ 2.25393295 -2.2034111 ]] \n",
      "\n",
      "shape:\n",
      " (3, 2) \n",
      "\n",
      "softmax on rows:\n",
      " [[0.00185555 0.99814445]\n",
      " [0.98853975 0.01146025]\n",
      " [0.98853975 0.01146025]] \n",
      "\n",
      "softmax[:,1]:\n",
      " [0.99814445 0.01146025 0.01146025] \n",
      "\n",
      "argmax on rows:\n",
      " [1 0 0] \n",
      "\n",
      "-------------------- \n",
      "\n",
      "softmax on [[1, 2]]\n",
      " [[0.26894142 0.73105858]] \n",
      "\n",
      "softmax[:,1] on [[1, 2]]\n",
      " [0.73105858] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# some tests to understand argmax/softmax better\n",
    "x = np.array([\n",
    "    [\n",
    "        -2.89058518409729,\n",
    "        3.3971314430236816\n",
    "    ],\n",
    "    [\n",
    "        2.2539329528808594,\n",
    "        -2.203411102294922\n",
    "    ],\n",
    "    [\n",
    "        2.2539329528808594,\n",
    "        -2.203411102294922\n",
    "    ]\n",
    "])\n",
    "\n",
    "print(\"values in 'predictions':\\n\", x, \"\\n\")\n",
    "\n",
    "print(\"shape:\\n\", x.shape, \"\\n\")\n",
    "\n",
    "print(\"softmax on rows:\\n\", softmax(x, axis=1), \"\\n\")\n",
    "\n",
    "print(\"softmax[:,1]:\\n\", softmax(x, axis=1)[:,1], \"\\n\")\n",
    "\n",
    "print(\"argmax on rows:\\n\", np.argmax(x, axis=1), \"\\n\")\n",
    "\n",
    "print(\"-\"*20, \"\\n\")\n",
    "\n",
    "print(\"softmax on [[1, 2]]\\n\", softmax([[1, 2]], axis=1), \"\\n\")\n",
    "print(\"softmax[:,1] on [[1, 2]]\\n\", softmax([[1, 2]], axis=1)[:,1], \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65a56d80",
   "metadata": {},
   "source": [
    "## 3.6 - Setup / Run Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49fc9922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Justin\\.envs\\tf290_env\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 70000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 408\n",
      "  Number of trainable parameters = 66998816\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112b9f20764444dd9e904b383f8d9123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3811, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60413f4a72c4c81a0fa77ef5b9847a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../data/models/multimodal/multi-modal__distilbert-base-uncased-50k\\checkpoint-136\n",
      "Configuration saved in ../data/models/multimodal/multi-modal__distilbert-base-uncased-50k\\checkpoint-136\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3145900368690491, 'eval_accuracy': 0.8581333333333333, 'eval_f1score': 0.8541866520487871, 'eval_precision': 0.877887323943662, 'eval_recall': 0.8317320523085134, 'eval_roc_auc': 0.9399648415774987, 'eval_brier_score': 0.8317320523085134, 'eval_tn': 6639, 'eval_fp': 867, 'eval_fn': 1261, 'eval_tp': 6233, 'eval_runtime': 17.2807, 'eval_samples_per_second': 868.02, 'eval_steps_per_second': 6.828, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../data/models/multimodal/multi-modal__distilbert-base-uncased-50k\\checkpoint-136\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2671, 'learning_rate': 1.6666666666666667e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc9d421aef54376848f75de6ba025d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../data/models/multimodal/multi-modal__distilbert-base-uncased-50k\\checkpoint-272\n",
      "Configuration saved in ../data/models/multimodal/multi-modal__distilbert-base-uncased-50k\\checkpoint-272\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2661682665348053, 'eval_accuracy': 0.8982666666666667, 'eval_f1score': 0.896696452748443, 'eval_precision': 0.9100027480076944, 'eval_recall': 0.8837736856151588, 'eval_roc_auc': 0.9593952451240679, 'eval_brier_score': 0.8837736856151588, 'eval_tn': 6851, 'eval_fp': 655, 'eval_fn': 871, 'eval_tp': 6623, 'eval_runtime': 15.5777, 'eval_samples_per_second': 962.917, 'eval_steps_per_second': 7.575, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../data/models/multimodal/multi-modal__distilbert-base-uncased-50k\\checkpoint-272\\pytorch_model.bin\n",
      "Deleting older checkpoint [..\\data\\models\\multimodal\\multi-modal__distilbert-base-uncased-50k\\checkpoint-136] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2242, 'learning_rate': 0.0, 'epoch': 2.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9bbb0a69324bab9c48b298dbc2d2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../data/models/multimodal/multi-modal__distilbert-base-uncased-50k\\checkpoint-408\n",
      "Configuration saved in ../data/models/multimodal/multi-modal__distilbert-base-uncased-50k\\checkpoint-408\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2607055604457855, 'eval_accuracy': 0.893, 'eval_f1score': 0.8916199608346277, 'eval_precision': 0.902529049897471, 'eval_recall': 0.8809714438217241, 'eval_roc_auc': 0.9596708630782413, 'eval_brier_score': 0.8809714438217241, 'eval_tn': 6793, 'eval_fp': 713, 'eval_fn': 892, 'eval_tp': 6602, 'eval_runtime': 16.482, 'eval_samples_per_second': 910.085, 'eval_steps_per_second': 7.159, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../data/models/multimodal/multi-modal__distilbert-base-uncased-50k\\checkpoint-408\\pytorch_model.bin\n",
      "Deleting older checkpoint [..\\data\\models\\multimodal\\multi-modal__distilbert-base-uncased-50k\\checkpoint-272] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 805.3671, 'train_samples_per_second': 260.751, 'train_steps_per_second': 0.507, 'train_loss': 0.2908016653621898, 'epoch': 2.99}\n",
      "\n",
      "Training duration: 0 days 00:13:25.692407\n",
      "\n",
      "Time: 805.37\n",
      "Samples/second: 260.75\n",
      "GPU memory occupied: 7728 MB.\n"
     ]
    }
   ],
   "source": [
    "time_training_start = pd.Timestamp.now()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_binary_classification_metrics\n",
    ")\n",
    "\n",
    "result = trainer.train()\n",
    "\n",
    "time_training_stop = pd.Timestamp.now()\n",
    "time_training = time_training_stop - time_training_start\n",
    "print(\"\\nTraining duration:\", str(time_training), end=\"\\n\\n\")\n",
    "\n",
    "# debug\n",
    "print_summary(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aabf3ed",
   "metadata": {},
   "source": [
    "## 3.7 - Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71fba9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../data/models/multimodal/multi-modal__distilbert-base-uncased-50k\n",
      "Configuration saved in ../data/models/multimodal/multi-modal__distilbert-base-uncased-50k\\config.json\n",
      "Model weights saved in ../data/models/multimodal/multi-modal__distilbert-base-uncased-50k\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()    # defaults to self.args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63964e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional - delete checkpoint directories\n",
    "checkpoint_dirs = [\n",
    "    f\"{trainer.args.output_dir}/{directory}\"\n",
    "    for directory in os.listdir(trainer.args.output_dir)\n",
    "        if (\n",
    "            os.path.isdir(os.path.join(trainer.args.output_dir, directory))\n",
    "            and\n",
    "            directory.startswith('checkpoint')\n",
    "        )\n",
    "]\n",
    "\n",
    "for checkpoint_dir in checkpoint_dirs:\n",
    "    print(f\"Attempting to delete '{checkpoint_dir}' ...\", end='')\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    print(f\" success\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0371be7",
   "metadata": {},
   "source": [
    "## 3.8 - Evaluate trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e709c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if evaluating immediately after training\n",
    "test_metrics = trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix='final_test')\n",
    "\n",
    "print(\"\\nTest Metrics:\\n\", test_metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b087177",
   "metadata": {},
   "source": [
    "## 3.9 - Output key data as JSON"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "873d2280",
   "metadata": {},
   "source": [
    "### 3.9.1 Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "predictions_output = trainer.predict(test_dataset)\n",
    "\n",
    "# predictions output is a subclass of NamedTuple, so to save to JSON we convert to a dict first\n",
    "#   Note: according to python docs, the leading underscore is to avoid name conflicts, not as the usual \"discouraged from use\" meaning\n",
    "#   Source: https://docs.python.org/3.10/library/collections.html#collections.somenamedtuple._asdict\n",
    "predictions_dict = predictions_out._asdict()\n",
    "\n",
    "predictions_dict['run_name'] = run_name\n",
    "predictions_dict['predictions'] = predictions_dict['predictions'].tolist()\n",
    "predictions_dict['label_ids'] = predictions_dict['label_ids'].tolist()\n",
    "\n",
    "# sort the keys\n",
    "dict_order = ['run_name', 'metrics', 'predictions', 'label_ids']\n",
    "predictions_dict = {key: predictions_dict[key] for key in dict_order}\n",
    "\n",
    "# save predictions to `output_dir`\n",
    "predictions_filename = trainer.args.output_dir + '/predictions.json'\n",
    "with open(predictions_filename, mode='w', encoding='utf-8') as fp:\n",
    "    json.dump(predictions_dict, fp, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b06bda9",
   "metadata": {},
   "source": [
    "### 3.9.1 - Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e760a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params_dict = {\n",
    "    'run_name': run_name,\n",
    "    'output_dir_name': output_dir_name,\n",
    "    'text_columns': text_cols,\n",
    "    'label_column': label_col,\n",
    "    'categorical_cols': categorical_cols,\n",
    "    'numerical_cols': numericical_cols,\n",
    "    # stopped here\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb284867",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m103"
  },
  "kernelspec": {
   "display_name": "tf290_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "65a6dd32d71ed4a2e5ac9ab3f52d3aeee49f01a00467a63b19dc274a1d27154b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
