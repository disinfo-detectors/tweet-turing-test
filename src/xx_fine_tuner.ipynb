{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add0cef4-a4de-47c5-a9a0-e6b818d6fa2d",
   "metadata": {},
   "source": [
    "# Tweet Turing Test: Detecting Disinformation on Twitter  \n",
    "\n",
    "|          | Group #2 - Disinformation Detectors                     |\n",
    "|---------:|---------------------------------------------------------|\n",
    "| Members  | John Johnson, Katy Matulay, Justin Minnion, Jared Rubin |\n",
    "| Notebook | `xx_fine_tuner.ipynb`                                   |\n",
    "| Purpose  | A notebook to fine-tune BERT models.                    |\n",
    "\n",
    "(todo: description)\n",
    "\n",
    "Based on tutorial from: https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a7349-cf9c-461b-aebd-de5542e2ded8",
   "metadata": {},
   "source": [
    "# 1 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54c0673b-7027-407e-8435-e29090105df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports from Python standard library\n",
    "\n",
    "# imports requiring installation\n",
    "#   connection to Google Cloud Storage\n",
    "from google.cloud import storage            # pip install google-cloud-storage\n",
    "from google.oauth2 import service_account   # pip install google-auth\n",
    "\n",
    "#  data science packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import evaluate\n",
    "from datasets import Dataset, ClassLabel\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7494318-b0ee-4452-80fa-c959050a6c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports from tweet_turing.py\n",
    "import tweet_turing as tur      # note - different import approach from prior notebooks\n",
    "\n",
    "# imports from tweet_turing_paths.py\n",
    "from tweet_turing_paths import local_data_paths, local_snapshot_paths, gcp_data_paths, \\\n",
    "    gcp_snapshot_paths, gcp_project_name, gcp_bucket_name, gcp_key_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eee18cdf-9d6c-4315-90ad-3e470718b1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pandas options\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7124b0f-b27f-450a-ba49-f0560c5b38b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Local or Cloud?\n",
    "\n",
    "Decide here whether to run notebook with local data or GCP bucket data\n",
    " - if the working directory of this notebook has a \"../data/\" folder with data loaded (e.g. working on local computer or have data files loaded to a cloud VM) then use the \"local files\" option and comment out the \"gcp bucket files\" option\n",
    " - if this notebook is being run from a GCP VM (preferrably in the `us-central1` location) then use the \"gcp bucket files\" option and comment out the \"local files\" option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d579d5d-f289-4b49-a1aa-f72f04d503b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# option: local files\n",
    "#local_or_cloud: str = \"local\"   # comment/uncomment this line or next\n",
    "\n",
    "# option: gcp bucket files\n",
    "local_or_cloud: str = \"cloud\"   # comment/uncomment this line or previous\n",
    "\n",
    "# don't comment/uncomment for remainder of cell\n",
    "if (local_or_cloud == \"local\"):\n",
    "    data_paths = local_data_paths\n",
    "    snapshot_paths = local_snapshot_paths\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    data_paths = gcp_data_paths\n",
    "    snapshot_paths = gcp_snapshot_paths\n",
    "else:\n",
    "    raise ValueError(\"Variable 'local_or_cloud' can only take on one of two values, 'local' or 'cloud'.\")\n",
    "    # subsequent cells will not do this final \"else\" check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b206dfc-7e39-49a1-a48d-71a4cb46e863",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell only needs to run its code if local_or_cloud==\"cloud\"\n",
    "#   (though it is harmless if run when local_or_cloud==\"local\")\n",
    "gcp_storage_client: storage.Client = None\n",
    "gcp_bucket: storage.Bucket = None\n",
    "\n",
    "if (local_or_cloud == \"cloud\"):\n",
    "    #gcp_storage_client = tur.get_gcp_storage_client(project_name=gcp_project_name, key_file=gcp_key_file)\n",
    "    gcp_storage_client = tur.get_gcp_storage_client(project_name=gcp_project_name)\n",
    "    gcp_bucket = tur.get_gcp_bucket(storage_client=gcp_storage_client, bucket_name=gcp_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7ee9f-857e-4c69-bd3f-803a970c3199",
   "metadata": {},
   "source": [
    "# 2 - Load Dataset\n",
    "\n",
    "Starting with the ten-percent sample with NLP-preprocessing completed from notebook **`04_nlp_preprocess.ipynb`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47e19c3b-cab5-4e4c-a6b8-950f4c0dbc58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note this cell requires package `pyarrow` to be installed in environment\n",
    "parq_filename: str = \"data_sample_ten_percent_NLP_preprocessed.parquet.gz\"\n",
    "parq_path: str = f\"{snapshot_paths['parq_snapshot']}{parq_filename}\"\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    df = pd.read_parquet(parq_path, engine='pyarrow')\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    df = tur.get_gcp_object_from_parq_as_df(bucket=gcp_bucket, object_name=parq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "448511ca-787d-4dc8-90d7-8a67332bc1fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external_author_id</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>region</th>\n",
       "      <th>language</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>updates</th>\n",
       "      <th>post_type</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>...</th>\n",
       "      <th>has_url</th>\n",
       "      <th>emoji_text</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>class</th>\n",
       "      <th>following_ratio</th>\n",
       "      <th>class_numeric</th>\n",
       "      <th>RUS_lett_count</th>\n",
       "      <th>content_demoji</th>\n",
       "      <th>content_no_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23785050</td>\n",
       "      <td>radiowoody</td>\n",
       "      <td>To live dangerously on Friday the 13th, we're doing the radio show from the UNLUCKIEST place on earth! The @TennesseeTitans Locker Room!</td>\n",
       "      <td>Nashville Tennessee</td>\n",
       "      <td>en</td>\n",
       "      <td>2585</td>\n",
       "      <td>5710</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-12-13 10:03:43+00:00</td>\n",
       "      <td>Verified</td>\n",
       "      <td>0.452635</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>To live dangerously on Friday the 13th, we're doing the radio show from the UNLUCKIEST place on earth! The @TennesseeTitans Locker Room!</td>\n",
       "      <td>To live dangerously on Friday the 13th, we're doing the radio show from the UNLUCKIEST place on earth! The @TennesseeTitans Locker Room!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59020162</td>\n",
       "      <td>matthewpouliot</td>\n",
       "      <td>@legsanity I like it. Almost like a free Gio. Pujols is still about as good of a bet as Gonzalez the rest of the way.</td>\n",
       "      <td>Florida</td>\n",
       "      <td>en</td>\n",
       "      <td>999</td>\n",
       "      <td>12637</td>\n",
       "      <td>0</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-04-26 20:13:58+00:00</td>\n",
       "      <td>Verified</td>\n",
       "      <td>0.079047</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@legsanity I like it. Almost like a free Gio. Pujols is still about as good of a bet as Gonzalez the rest of the way.</td>\n",
       "      <td>@legsanity I like it. Almost like a free Gio. Pujols is still about as good of a bet as Gonzalez the rest of the way.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1656024374</td>\n",
       "      <td>IMISSOBAMA</td>\n",
       "      <td>Man servants can have a good purpose as long as they come with cash and don't touch me ever.</td>\n",
       "      <td>United States</td>\n",
       "      <td>en</td>\n",
       "      <td>473</td>\n",
       "      <td>760</td>\n",
       "      <td>4122</td>\n",
       "      <td>RETWEET</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-12-24 13:12:00+00:00</td>\n",
       "      <td>Troll</td>\n",
       "      <td>0.621551</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Man servants can have a good purpose as long as they come with cash and don't touch me ever.</td>\n",
       "      <td>Man servants can have a good purpose as long as they come with cash and don't touch me ever.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  external_author_id          author  \\\n",
       "0           23785050      radiowoody   \n",
       "1           59020162  matthewpouliot   \n",
       "2         1656024374      IMISSOBAMA   \n",
       "\n",
       "                                                                                                                                    content  \\\n",
       "0  To live dangerously on Friday the 13th, we're doing the radio show from the UNLUCKIEST place on earth! The @TennesseeTitans Locker Room!   \n",
       "1                     @legsanity I like it. Almost like a free Gio. Pujols is still about as good of a bet as Gonzalez the rest of the way.   \n",
       "2                                              Man servants can have a good purpose as long as they come with cash and don't touch me ever.   \n",
       "\n",
       "                region language  following  followers  updates   post_type  \\\n",
       "0  Nashville Tennessee       en       2585       5710        2         NaN   \n",
       "1              Florida       en        999      12637        0  replied_to   \n",
       "2        United States       en        473        760     4122     RETWEET   \n",
       "\n",
       "   is_retweet  ... has_url emoji_text emoji_count              publish_date  \\\n",
       "0         0.0  ...       0         []           0 2013-12-13 10:03:43+00:00   \n",
       "1         0.0  ...       0         []           0 2015-04-26 20:13:58+00:00   \n",
       "2         1.0  ...       0         []           0 2016-12-24 13:12:00+00:00   \n",
       "\n",
       "      class following_ratio  class_numeric RUS_lett_count  \\\n",
       "0  Verified        0.452635              0              0   \n",
       "1  Verified        0.079047              0              0   \n",
       "2     Troll        0.621551              1              0   \n",
       "\n",
       "                                                                                                                             content_demoji  \\\n",
       "0  To live dangerously on Friday the 13th, we're doing the radio show from the UNLUCKIEST place on earth! The @TennesseeTitans Locker Room!   \n",
       "1                     @legsanity I like it. Almost like a free Gio. Pujols is still about as good of a bet as Gonzalez the rest of the way.   \n",
       "2                                              Man servants can have a good purpose as long as they come with cash and don't touch me ever.   \n",
       "\n",
       "                                                                                                                           content_no_emoji  \n",
       "0  To live dangerously on Friday the 13th, we're doing the radio show from the UNLUCKIEST place on earth! The @TennesseeTitans Locker Room!  \n",
       "1                     @legsanity I like it. Almost like a free Gio. Pujols is still about as good of a bet as Gonzalez the rest of the way.  \n",
       "2                                              Man servants can have a good purpose as long as they come with cash and don't touch me ever.  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96506d5c-cd41-4382-9518-55132bfad5d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 362314 entries, 0 to 362313\n",
      "Data columns (total 24 columns):\n",
      " #   Column              Non-Null Count   Dtype              \n",
      "---  ------              --------------   -----              \n",
      " 0   external_author_id  362314 non-null  string             \n",
      " 1   author              362314 non-null  string             \n",
      " 2   content             362314 non-null  string             \n",
      " 3   region              344249 non-null  string             \n",
      " 4   language            362314 non-null  category           \n",
      " 5   following           362314 non-null  uint64             \n",
      " 6   followers           362314 non-null  uint64             \n",
      " 7   updates             362314 non-null  uint64             \n",
      " 8   post_type           154729 non-null  category           \n",
      " 9   is_retweet          362314 non-null  float64            \n",
      " 10  account_category    362314 non-null  category           \n",
      " 11  tweet_id            362314 non-null  string             \n",
      " 12  tco1_step1          219778 non-null  string             \n",
      " 13  data_source         362314 non-null  category           \n",
      " 14  has_url             362314 non-null  int64              \n",
      " 15  emoji_text          362314 non-null  object             \n",
      " 16  emoji_count         362314 non-null  int64              \n",
      " 17  publish_date        362314 non-null  datetime64[ns, UTC]\n",
      " 18  class               362314 non-null  category           \n",
      " 19  following_ratio     362314 non-null  float64            \n",
      " 20  class_numeric       362314 non-null  int8               \n",
      " 21  RUS_lett_count      362314 non-null  int64              \n",
      " 22  content_demoji      362314 non-null  object             \n",
      " 23  content_no_emoji    362314 non-null  object             \n",
      "dtypes: category(5), datetime64[ns, UTC](1), float64(2), int64(3), int8(1), object(3), string(6), uint64(3)\n",
      "memory usage: 387.6 MB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9b1aca6-46d1-472d-8b33-4c830be6aa65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Verified', 'Troll']\n",
       "Categories (2, object): ['Troll', 'Verified']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "679eb1f2-e964-4e2c-8476-2522a5b77649",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_demoji</th>\n",
       "      <th>content_no_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>218744</th>\n",
       "      <td>The future is female &amp;amp; we make history everyday. Happy #WomensHistoryMonth  ÔøΩ‚úäÔøΩ https://t.co/MbKatuKSpW</td>\n",
       "      <td>The future is female &amp;amp; we make history everyday. Happy #WomensHistoryMonth  ÔøΩ:raised fist:ÔøΩ https://t.co/MbKatuKSpW</td>\n",
       "      <td>The future is female &amp;amp; we make history everyday. Happy #WomensHistoryMonth  ÔøΩÔøΩ https://t.co/MbKatuKSpW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273179</th>\n",
       "      <td>@melindagates Thx for highlighting - we üíõthis! Maybe the next Millie is here! https://t.co/evyuBRJgUW üí°</td>\n",
       "      <td>@melindagates Thx for highlighting - we :yellow heart:this! Maybe the next Millie is here! https://t.co/evyuBRJgUW :light bulb:</td>\n",
       "      <td>@melindagates Thx for highlighting - we this! Maybe the next Millie is here! https://t.co/evyuBRJgUW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232730</th>\n",
       "      <td>Shhh, you! I thought we were friends üòú https://t.co/eOu0IOVmD5</td>\n",
       "      <td>Shhh, you! I thought we were friends :winking face with tongue: https://t.co/eOu0IOVmD5</td>\n",
       "      <td>Shhh, you! I thought we were friends  https://t.co/eOu0IOVmD5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304524</th>\n",
       "      <td>It smells like garlic bread at the @ymca right now word to @DMX üëÉüçû</td>\n",
       "      <td>It smells like garlic bread at the @ymca right now word to @DMX :nose::bread:</td>\n",
       "      <td>It smells like garlic bread at the @ymca right now word to @DMX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35606</th>\n",
       "      <td>RT ericbolling: üí•BOOMüí• .nytimes influential Best Seller list: \"The Swamp\" earned its 5th straight week!  Thanks to‚Ä¶ https://t.co/eBdETSSjHM</td>\n",
       "      <td>RT ericbolling: :collision:BOOM:collision: .nytimes influential Best Seller list: \"The Swamp\" earned its 5th straight week!  Thanks to‚Ä¶ https://t.co/eBdETSSjHM</td>\n",
       "      <td>RT ericbolling: BOOM .nytimes influential Best Seller list: \"The Swamp\" earned its 5th straight week!  Thanks to‚Ä¶ https://t.co/eBdETSSjHM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                            content  \\\n",
       "218744                                  The future is female &amp; we make history everyday. Happy #WomensHistoryMonth  ÔøΩ‚úäÔøΩ https://t.co/MbKatuKSpW   \n",
       "273179                                      @melindagates Thx for highlighting - we üíõthis! Maybe the next Millie is here! https://t.co/evyuBRJgUW üí°   \n",
       "232730                                                                               Shhh, you! I thought we were friends üòú https://t.co/eOu0IOVmD5   \n",
       "304524                                                                           It smells like garlic bread at the @ymca right now word to @DMX üëÉüçû   \n",
       "35606   RT ericbolling: üí•BOOMüí• .nytimes influential Best Seller list: \"The Swamp\" earned its 5th straight week!  Thanks to‚Ä¶ https://t.co/eBdETSSjHM   \n",
       "\n",
       "                                                                                                                                                         content_demoji  \\\n",
       "218744                                          The future is female &amp; we make history everyday. Happy #WomensHistoryMonth  ÔøΩ:raised fist:ÔøΩ https://t.co/MbKatuKSpW   \n",
       "273179                                  @melindagates Thx for highlighting - we :yellow heart:this! Maybe the next Millie is here! https://t.co/evyuBRJgUW :light bulb:   \n",
       "232730                                                                          Shhh, you! I thought we were friends :winking face with tongue: https://t.co/eOu0IOVmD5   \n",
       "304524                                                                                    It smells like garlic bread at the @ymca right now word to @DMX :nose::bread:   \n",
       "35606   RT ericbolling: :collision:BOOM:collision: .nytimes influential Best Seller list: \"The Swamp\" earned its 5th straight week!  Thanks to‚Ä¶ https://t.co/eBdETSSjHM   \n",
       "\n",
       "                                                                                                                                 content_no_emoji  \n",
       "218744                                 The future is female &amp; we make history everyday. Happy #WomensHistoryMonth  ÔøΩÔøΩ https://t.co/MbKatuKSpW  \n",
       "273179                                      @melindagates Thx for highlighting - we this! Maybe the next Millie is here! https://t.co/evyuBRJgUW   \n",
       "232730                                                                              Shhh, you! I thought we were friends  https://t.co/eOu0IOVmD5  \n",
       "304524                                                                           It smells like garlic bread at the @ymca right now word to @DMX   \n",
       "35606   RT ericbolling: BOOM .nytimes influential Best Seller list: \"The Swamp\" earned its 5th straight week!  Thanks to‚Ä¶ https://t.co/eBdETSSjHM  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['emoji_count'] > 0, ['content', 'content_demoji', 'content_no_emoji']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2534d6-e6f7-4698-ada0-3781ef116bc4",
   "metadata": {},
   "source": [
    "# 3 - Choose Dataset Fields and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f8aea-48f4-4e0a-8dac-0b326522b38a",
   "metadata": {},
   "source": [
    "## 3.1 - Set Args\n",
    "\n",
    "To make the subsequent encoding/training code more modular, set as many args as we can within this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b44ca1a8-709d-4990-a2e8-273ed17ef576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# where to store the output of the model (as a subfolder of ../data/models/)\n",
    "output_dir_name = 'dist-test1'\n",
    "\n",
    "# which columns from dataframe will be used\n",
    "content_column = 'content_no_emoji'\n",
    "class_column = 'class_numeric'        # Assumes: 0=authentic, 1=troll\n",
    "\n",
    "# select pre-trained model\n",
    "#pretrained_model_name = 'Twitter/twhin-bert-base'    # https://huggingface.co/Twitter/twhin-bert-base\n",
    "#pretrained_model_name = 'bert-base-cased'            # https://huggingface.co/bert-base-uncased\n",
    "pretrained_model_name = 'distilbert-base-uncased'    # https://huggingface.co/distilbert-base-uncased\n",
    "\n",
    "# these are passed on to model object as keyword args\n",
    "extra_model_args = {\n",
    "    'num_labels': 2\n",
    "}\n",
    "\n",
    "# these are passed on to trainer object as keyword args\n",
    "extra_train_args = {\n",
    "    'output_dir': f'../data/models/{output_dir_name}',\n",
    "    'num_train_epochs': 2,\n",
    "}\n",
    "\n",
    "# maximum tweets (per class) used for fine tuning (set to None for no limit)\n",
    "#  e.g. if this value is 5000, a maximum of 5000 troll and 5000 authentic tweets will be used\n",
    "#       for a total of 10,000 tweets used for fine tuning\n",
    "max_tweets_per_class = 5000\n",
    "sampling_random_seed = 42\n",
    "\n",
    "# for train/test split\n",
    "train_test_random_seed = 3    # for reproducability, and \"the number of the counting shall be three\"\n",
    "test_fraction = 0.20          # within range (0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "54226667-f567-4e50-8c02-bbfedc32ac3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for model summary we can track how long it took to encode and train\n",
    "time_encoding = None\n",
    "time_training = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ed522-c079-43f1-8354-b0e9cee13eb2",
   "metadata": {},
   "source": [
    "## 3.2 - Convert Pandas Dataframe to ü§ó Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "80f06a07-4e98-4484-b57d-a6ca104ace46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c692585bf26453cbddf7dbba0dd3c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'On #MuslimWomensDay, these women are empowering themselves and fighting back against Islamophobia. https://t.co/Y5NXaTHjZi',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a view (not a copy) of dataframe\n",
    "if (max_tweets_per_class is None):\n",
    "    df_view = df[[content_column, class_column]]\n",
    "else:\n",
    "    df_view = pd.concat(\n",
    "        [\n",
    "            df.loc[df[class_column] == 1, [content_column, class_column]].sample(n=max_tweets_per_class, random_state=sampling_random_seed),\n",
    "            df.loc[df[class_column] == 0, [content_column, class_column]].sample(n=max_tweets_per_class, random_state=sampling_random_seed)\n",
    "        ], \n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "# convert to ü§ó Dataset object\n",
    "dataset = Dataset.from_pandas(df_view) \\\n",
    "            .rename_columns({content_column: \"text\", class_column: \"label\"}) \\\n",
    "            .cast_column(\"label\", ClassLabel(names=['authentic', 'troll']))\n",
    "\n",
    "# check results\n",
    "assert (dataset.features['label'].str2int('authentic') == 0) and (dataset.features['label'].str2int('troll') == 1), 'class labels mismatched'\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e06bc37-5ce9-4bf0-90d8-c13f5fc5ec01",
   "metadata": {},
   "source": [
    "## 3.3 - Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b2cac37a-a997-4564-85ea-a6677b5b6b47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_split = dataset.train_test_split(\n",
    "    test_size=test_fraction,\n",
    "    seed=train_test_random_seed,\n",
    ")\n",
    "\n",
    "# check output\n",
    "dataset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef6dcbe-52de-481b-9793-c3ff35c7f7bc",
   "metadata": {},
   "source": [
    "## 3.4 - Tokenize / Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "186a18b1-af51-41be-aeba-0cdcb2d3924b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34c188042cd4aa2acd735a41333e2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98222c8cf1c4bd9b82baf1165d62c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f6736f9bfb4ce3abba0a0e8e482ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1cf49620fb4e56a254919961c75e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # create the tokenizer to prepare text for model\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a2e8536-27a8-4c74-8127-dcc5b02ad3e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a tokenizer function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ebc8519-f873-47dc-9e5f-7f44c9e13aa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51ec35d9b1e4edf877693add709e818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e09d6014e346e298d526eb1a03c10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_encoding_start = pd.Timestamp.now()\n",
    "\n",
    "# encode the training and test sets\n",
    "#tokenized_datasets = dataset_split.map(tokenize_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "tokenized_datasets = dataset_split.map(tokenize_function, batched=True)\n",
    "\n",
    "time_encoding_stop = pd.Timestamp.now()\n",
    "time_encoding = time_encoding_stop - time_encoding_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e70390c7-fcb5-4854-87f2-1a8c3f505e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639126b-38f2-4716-8582-7cb074dd28a8",
   "metadata": {},
   "source": [
    "## 3.5 - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f73d471-1726-4c7c-b6f7-3a7c19f63781",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956dee4411df44b5bd5018010f89a029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    **extra_model_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ea71de9-cc92-4ecf-9332-3b8ec0f70b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    **extra_train_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6c2910b7-3b19-4def-8385-ac89a9ecbe89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup training metric\n",
    "metric = evaluate.load('accuracy')   # TODO -> study this more\n",
    "\n",
    "def compute_metrics(eval_pred):    # TODO -> convert to pure function\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)    # TODO -> study this more\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3320b855-e785-425b-8c4a-bef53d0b462c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 8000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "  Number of trainable parameters = 66955010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 12:22, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.435500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.365300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.229500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../data/models/dist-test1/checkpoint-500\n",
      "Configuration saved in ../data/models/dist-test1/checkpoint-500/config.json\n",
      "Model weights saved in ../data/models/dist-test1/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ../data/models/dist-test1/checkpoint-1000\n",
      "Configuration saved in ../data/models/dist-test1/checkpoint-1000/config.json\n",
      "Model weights saved in ../data/models/dist-test1/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ../data/models/dist-test1/checkpoint-1500\n",
      "Configuration saved in ../data/models/dist-test1/checkpoint-1500/config.json\n",
      "Model weights saved in ../data/models/dist-test1/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ../data/models/dist-test1/checkpoint-2000\n",
      "Configuration saved in ../data/models/dist-test1/checkpoint-2000/config.json\n",
      "Model weights saved in ../data/models/dist-test1/checkpoint-2000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 days 00:12:25.324531\n"
     ]
    }
   ],
   "source": [
    "time_training_start = pd.Timestamp.now()\n",
    "\n",
    "# setup the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# execute the training\n",
    "trainer.train()\n",
    "\n",
    "time_training_stop = pd.Timestamp.now()\n",
    "time_training = time_training_stop - time_training_start\n",
    "\n",
    "print(time_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a3fc9-b859-4657-b052-c111441e78ab",
   "metadata": {},
   "source": [
    "## 3.6 - Save fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "96d4890f-d58f-4041-941f-45450005b4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../data/models/dist-test1\n",
      "Configuration saved in ../data/models/dist-test1/config.json\n",
      "Model weights saved in ../data/models/dist-test1/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()    # defaults to self.args.output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a9895-0c0b-4d04-b200-729fbd7ba87d",
   "metadata": {},
   "source": [
    "## 3.7 - Evaluate fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aad91a54-6809-4920-b63d-0fd396247d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4639328718185425,\n",
       " 'eval_accuracy': 0.8565,\n",
       " 'eval_runtime': 30.43,\n",
       " 'eval_samples_per_second': 65.725,\n",
       " 'eval_steps_per_second': 8.216,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if evaluating immediately after fine-tuning\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d0d9958b-4320-403c-a6d4-1e5c3c5b6030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "536e511d-0687-4387-b40d-4103a9d9a7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.4639328718185425,\n",
       " 'test_accuracy': 0.8565,\n",
       " 'test_runtime': 30.4455,\n",
       " 'test_samples_per_second': 65.691,\n",
       " 'test_steps_per_second': 8.211}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "93dc7b2e-be12-44a7-bf28-0d0bf18765f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../data/models/dist-test1/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"../data/models/dist-test1\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ../data/models/dist-test1/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ../data/models/dist-test1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# reload model, if evaluation is being performed separately from training\n",
    "model_dir = \"../data/models/dist-test1\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1df5ba-fffc-4f10-993a-9c2ad795a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - setup eval for re-loaded model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8cd4eb-89fe-47df-9940-e2db3034cb49",
   "metadata": {},
   "source": [
    "TODO: setup way to archive the saved model files.\n",
    "\n",
    "For now:\n",
    "`tar -czvf dist-test1.tar.gz --exclude='*checkpoint*' dist-test1`"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
