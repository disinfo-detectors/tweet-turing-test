{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "add0cef4-a4de-47c5-a9a0-e6b818d6fa2d",
   "metadata": {},
   "source": [
    "# Tweet Turing Test: Detecting Disinformation on Twitter  \n",
    "\n",
    "|          | Group #2 - Disinformation Detectors                     |\n",
    "|---------:|---------------------------------------------------------|\n",
    "| Members  | John Johnson, Katy Matulay, Justin Minnion, Jared Rubin |\n",
    "| Notebook | `xx_fine_tuner.ipynb`                                   |\n",
    "| Purpose  | A notebook to fine-tune BERT models.                    |\n",
    "\n",
    "*Assumptions*  \n",
    " - The dataset being used has binary class labels following convention: 0 = authentic tweet; 1 = troll tweet\n",
    " - The execution environment has internet access (to download models from huggingface.co)\n",
    "\n",
    "*Notes*\n",
    " - Notebook is based on tutorial from: https://huggingface.co/docs/transformers/training\n",
    " - Additional help from: https://www.youtube.com/watch?v=V1-Hm2rNkik&t=351s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a7349-cf9c-461b-aebd-de5542e2ded8",
   "metadata": {},
   "source": [
    "# 1 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54c0673b-7027-407e-8435-e29090105df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports from Python standard library\n",
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# imports requiring installation\n",
    "#   connection to Google Cloud Storage\n",
    "from google.cloud import storage            # pip install google-cloud-storage\n",
    "from google.oauth2 import service_account   # pip install google-auth\n",
    "\n",
    "#  data science packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ü§ó (huggingface) packages\n",
    "import evaluate\n",
    "from datasets import Dataset, ClassLabel\n",
    "from transformers import BertTokenizerFast, BertweetTokenizer, DistilBertTokenizerFast, RobertaTokenizerFast, XLMRobertaTokenizerFast\n",
    "from transformers import BertForSequenceClassification, RobertaForSequenceClassification, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7494318-b0ee-4452-80fa-c959050a6c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports from tweet_turing.py\n",
    "import tweet_turing as tur      # note - different import approach from prior notebooks\n",
    "\n",
    "# imports from tweet_turing_paths.py\n",
    "from tweet_turing_paths import local_data_paths, local_snapshot_paths, gcp_data_paths, \\\n",
    "    gcp_snapshot_paths, gcp_project_name, gcp_bucket_name, gcp_key_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eee18cdf-9d6c-4315-90ad-3e470718b1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pandas options\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7124b0f-b27f-450a-ba49-f0560c5b38b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Local or Cloud?\n",
    "\n",
    "Decide here whether to run notebook with local data or GCP bucket data\n",
    " - if the working directory of this notebook has a \"../data/\" folder with data loaded (e.g. working on local computer or have data files loaded to a cloud VM) then use the \"local files\" option and comment out the \"gcp bucket files\" option\n",
    " - if this notebook is being run from a GCP VM (preferrably in the `us-central1` location) then use the \"gcp bucket files\" option and comment out the \"local files\" option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d579d5d-f289-4b49-a1aa-f72f04d503b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# option: local files\n",
    "local_or_cloud: str = \"local\"   # comment/uncomment this line or next\n",
    "\n",
    "# option: gcp bucket files\n",
    "#local_or_cloud: str = \"cloud\"   # comment/uncomment this line or previous\n",
    "\n",
    "# don't comment/uncomment for remainder of cell\n",
    "if (local_or_cloud == \"local\"):\n",
    "    data_paths = local_data_paths\n",
    "    snapshot_paths = local_snapshot_paths\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    data_paths = gcp_data_paths\n",
    "    snapshot_paths = gcp_snapshot_paths\n",
    "else:\n",
    "    raise ValueError(\"Variable 'local_or_cloud' can only take on one of two values, 'local' or 'cloud'.\")\n",
    "    # subsequent cells will not do this final \"else\" check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b206dfc-7e39-49a1-a48d-71a4cb46e863",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell only needs to run its code if local_or_cloud==\"cloud\"\n",
    "#   (though it is harmless if run when local_or_cloud==\"local\")\n",
    "gcp_storage_client: storage.Client = None\n",
    "gcp_bucket: storage.Bucket = None\n",
    "\n",
    "if (local_or_cloud == \"cloud\"):\n",
    "    gcp_storage_client = tur.get_gcp_storage_client(project_name=gcp_project_name, key_file=gcp_key_file)\n",
    "    gcp_bucket = tur.get_gcp_bucket(storage_client=gcp_storage_client, bucket_name=gcp_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7ee9f-857e-4c69-bd3f-803a970c3199",
   "metadata": {},
   "source": [
    "# 2 - Load Dataset\n",
    "\n",
    "Starting with the ten-percent sample with NLP-preprocessing completed from notebook **`04_nlp_preprocess.ipynb`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47e19c3b-cab5-4e4c-a6b8-950f4c0dbc58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note this cell requires package `pyarrow` to be installed in environment\n",
    "parq_filename: str = \"data_sample_ten_percent_NLP_preprocessed.parquet.gz\"\n",
    "parq_path: str = f\"{snapshot_paths['parq_snapshot']}{parq_filename}\"\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    df = pd.read_parquet(parq_path, engine='pyarrow')\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    df = tur.get_gcp_object_from_parq_as_df(bucket=gcp_bucket, object_name=parq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "448511ca-787d-4dc8-90d7-8a67332bc1fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external_author_id</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>region</th>\n",
       "      <th>language</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>updates</th>\n",
       "      <th>post_type</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>...</th>\n",
       "      <th>has_url</th>\n",
       "      <th>emoji_text</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>class</th>\n",
       "      <th>following_ratio</th>\n",
       "      <th>class_numeric</th>\n",
       "      <th>RUS_lett_count</th>\n",
       "      <th>content_demoji</th>\n",
       "      <th>content_no_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23785050</td>\n",
       "      <td>radiowoody</td>\n",
       "      <td>To live dangerously on Friday the 13th, we're doing the radio show from the UNLUCKIEST place on earth! The @TennesseeTitans Locker Room!</td>\n",
       "      <td>Nashville Tennessee</td>\n",
       "      <td>en</td>\n",
       "      <td>2585</td>\n",
       "      <td>5710</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-12-13 10:03:43+00:00</td>\n",
       "      <td>Verified</td>\n",
       "      <td>0.452635</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>To live dangerously on Friday the 13th, we're doing the radio show from the UNLUCKIEST place on earth! The @TennesseeTitans Locker Room!</td>\n",
       "      <td>To live dangerously on Friday the 13th, we're doing the radio show from the UNLUCKIEST place on earth! The @TennesseeTitans Locker Room!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59020162</td>\n",
       "      <td>matthewpouliot</td>\n",
       "      <td>@legsanity I like it. Almost like a free Gio. Pujols is still about as good of a bet as Gonzalez the rest of the way.</td>\n",
       "      <td>Florida</td>\n",
       "      <td>en</td>\n",
       "      <td>999</td>\n",
       "      <td>12637</td>\n",
       "      <td>0</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-04-26 20:13:58+00:00</td>\n",
       "      <td>Verified</td>\n",
       "      <td>0.079047</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@legsanity I like it. Almost like a free Gio. Pujols is still about as good of a bet as Gonzalez the rest of the way.</td>\n",
       "      <td>@legsanity I like it. Almost like a free Gio. Pujols is still about as good of a bet as Gonzalez the rest of the way.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1656024374</td>\n",
       "      <td>IMISSOBAMA</td>\n",
       "      <td>Man servants can have a good purpose as long as they come with cash and don't touch me ever.</td>\n",
       "      <td>United States</td>\n",
       "      <td>en</td>\n",
       "      <td>473</td>\n",
       "      <td>760</td>\n",
       "      <td>4122</td>\n",
       "      <td>RETWEET</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-12-24 13:12:00+00:00</td>\n",
       "      <td>Troll</td>\n",
       "      <td>0.621551</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Man servants can have a good purpose as long as they come with cash and don't touch me ever.</td>\n",
       "      <td>Man servants can have a good purpose as long as they come with cash and don't touch me ever.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  external_author_id          author  \\\n",
       "0           23785050      radiowoody   \n",
       "1           59020162  matthewpouliot   \n",
       "2         1656024374      IMISSOBAMA   \n",
       "\n",
       "                                                                                                                                    content  \\\n",
       "0  To live dangerously on Friday the 13th, we're doing the radio show from the UNLUCKIEST place on earth! The @TennesseeTitans Locker Room!   \n",
       "1                     @legsanity I like it. Almost like a free Gio. Pujols is still about as good of a bet as Gonzalez the rest of the way.   \n",
       "2                                              Man servants can have a good purpose as long as they come with cash and don't touch me ever.   \n",
       "\n",
       "                region language  following  followers  updates   post_type  \\\n",
       "0  Nashville Tennessee       en       2585       5710        2         NaN   \n",
       "1              Florida       en        999      12637        0  replied_to   \n",
       "2        United States       en        473        760     4122     RETWEET   \n",
       "\n",
       "   is_retweet  ... has_url emoji_text emoji_count              publish_date  \\\n",
       "0         0.0  ...       0         []           0 2013-12-13 10:03:43+00:00   \n",
       "1         0.0  ...       0         []           0 2015-04-26 20:13:58+00:00   \n",
       "2         1.0  ...       0         []           0 2016-12-24 13:12:00+00:00   \n",
       "\n",
       "      class following_ratio  class_numeric RUS_lett_count  \\\n",
       "0  Verified        0.452635              0              0   \n",
       "1  Verified        0.079047              0              0   \n",
       "2     Troll        0.621551              1              0   \n",
       "\n",
       "                                                                                                                             content_demoji  \\\n",
       "0  To live dangerously on Friday the 13th, we're doing the radio show from the UNLUCKIEST place on earth! The @TennesseeTitans Locker Room!   \n",
       "1                     @legsanity I like it. Almost like a free Gio. Pujols is still about as good of a bet as Gonzalez the rest of the way.   \n",
       "2                                              Man servants can have a good purpose as long as they come with cash and don't touch me ever.   \n",
       "\n",
       "                                                                                                                           content_no_emoji  \n",
       "0  To live dangerously on Friday the 13th, we're doing the radio show from the UNLUCKIEST place on earth! The @TennesseeTitans Locker Room!  \n",
       "1                     @legsanity I like it. Almost like a free Gio. Pujols is still about as good of a bet as Gonzalez the rest of the way.  \n",
       "2                                              Man servants can have a good purpose as long as they come with cash and don't touch me ever.  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96506d5c-cd41-4382-9518-55132bfad5d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 362314 entries, 0 to 362313\n",
      "Data columns (total 24 columns):\n",
      " #   Column              Non-Null Count   Dtype              \n",
      "---  ------              --------------   -----              \n",
      " 0   external_author_id  362314 non-null  string             \n",
      " 1   author              362314 non-null  string             \n",
      " 2   content             362314 non-null  string             \n",
      " 3   region              344249 non-null  string             \n",
      " 4   language            362314 non-null  category           \n",
      " 5   following           362314 non-null  uint64             \n",
      " 6   followers           362314 non-null  uint64             \n",
      " 7   updates             362314 non-null  uint64             \n",
      " 8   post_type           154729 non-null  category           \n",
      " 9   is_retweet          362314 non-null  float64            \n",
      " 10  account_category    362314 non-null  category           \n",
      " 11  tweet_id            362314 non-null  string             \n",
      " 12  tco1_step1          219778 non-null  string             \n",
      " 13  data_source         362314 non-null  category           \n",
      " 14  has_url             362314 non-null  int64              \n",
      " 15  emoji_text          362314 non-null  object             \n",
      " 16  emoji_count         362314 non-null  int64              \n",
      " 17  publish_date        362314 non-null  datetime64[ns, UTC]\n",
      " 18  class               362314 non-null  category           \n",
      " 19  following_ratio     362314 non-null  float64            \n",
      " 20  class_numeric       362314 non-null  int8               \n",
      " 21  RUS_lett_count      362314 non-null  int64              \n",
      " 22  content_demoji      362314 non-null  object             \n",
      " 23  content_no_emoji    362314 non-null  object             \n",
      "dtypes: category(5), datetime64[ns, UTC](1), float64(2), int64(3), int8(1), object(3), string(6), uint64(3)\n",
      "memory usage: 390.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b1aca6-46d1-472d-8b33-4c830be6aa65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Verified', 'Troll']\n",
       "Categories (2, object): ['Troll', 'Verified']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "679eb1f2-e964-4e2c-8476-2522a5b77649",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_demoji</th>\n",
       "      <th>content_no_emoji</th>\n",
       "      <th>emoji_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>232301</th>\n",
       "      <td>RT DebAlwaystrump: Congress looking CORRUPT‚ô® 2 Americaüò†thinks we are STUPID‚§µ MUELLER is a DIRTY COPüö® Russia was 4 ‚Ä¶ https://t.co/sVXXXBIjSh</td>\n",
       "      <td>RT DebAlwaystrump: Congress looking CORRUPT:hot springs: 2 America:angry face:thinks we are STUPID:right arrow curving down: MUELLER is a DIRTY COP:police car light: Russia was 4 ‚Ä¶ https://t.co/sVXXXBIjSh</td>\n",
       "      <td>RT DebAlwaystrump: Congress looking CORRUPT 2 Americathinks we are STUPID MUELLER is a DIRTY COP Russia was 4 ‚Ä¶ https://t.co/sVXXXBIjSh</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203651</th>\n",
       "      <td>RT Laura_K69: GOOD LUCK!! ü§£ü§£ü§£ü§£ https://t.co/pc4FYSEscE</td>\n",
       "      <td>RT Laura_K69: GOOD LUCK!! :rolling on the floor laughing::rolling on the floor laughing::rolling on the floor laughing::rolling on the floor laughing: https://t.co/pc4FYSEscE</td>\n",
       "      <td>RT Laura_K69: GOOD LUCK!!  https://t.co/pc4FYSEscE</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32502</th>\n",
       "      <td>TRIGGER WARNING: Beware before reading my new @Townhallcom column, \"Dating Tips For Prominent Democrats.\" It yields maximum kurtness.  üí•üî•üí•üî•üí•üî•üí•ü§∑üèº‚Äç‚ôÇÔ∏èüî•üí•üî•üí•üî•üí•  https://t.co/9EGBRNlw1j</td>\n",
       "      <td>TRIGGER WARNING: Beware before reading my new @Townhallcom column, \"Dating Tips For Prominent Democrats.\" It yields maximum kurtness.  :collision::fire::collision::fire::collision::fire::collision::man shrugging: medium-light skin tone::fire::collision::fire::collision::fire::collision:  https://t.co/9EGBRNlw1j</td>\n",
       "      <td>TRIGGER WARNING: Beware before reading my new @Townhallcom column, \"Dating Tips For Prominent Democrats.\" It yields maximum kurtness.    https://t.co/9EGBRNlw1j</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186365</th>\n",
       "      <td>RT @BNPPARIBASOPEN: How does @EVesnina001 lose her #BNPPO17 üèÜ midway through her champion's press conference?\n",
       "\n",
       "üé•üòÇ‚û°Ô∏è https://t.co/RbDT7nlRt7</td>\n",
       "      <td>RT @BNPPARIBASOPEN: How does @EVesnina001 lose her #BNPPO17 :trophy: midway through her champion's press conference?\\n\\n:movie camera::face with tears of joy::right arrow: https://t.co/RbDT7nlRt7</td>\n",
       "      <td>RT @BNPPARIBASOPEN: How does @EVesnina001 lose her #BNPPO17  midway through her champion's press conference?\\n\\n https://t.co/RbDT7nlRt7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303189</th>\n",
       "      <td>RT @funsized411: üíØüíØüíØüíØüíØ RT @TheMisterMarcus: I'm okay with Duke losing even if it ruins my bracket.</td>\n",
       "      <td>RT @funsized411: :hundred points::hundred points::hundred points::hundred points::hundred points: RT @TheMisterMarcus: I'm okay with Duke losing even if it ruins my bracket.</td>\n",
       "      <td>RT @funsized411:  RT @TheMisterMarcus: I'm okay with Duke losing even if it ruins my bracket.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                   content  \\\n",
       "232301                                         RT DebAlwaystrump: Congress looking CORRUPT‚ô® 2 Americaüò†thinks we are STUPID‚§µ MUELLER is a DIRTY COPüö® Russia was 4 ‚Ä¶ https://t.co/sVXXXBIjSh   \n",
       "203651                                                                                                                              RT Laura_K69: GOOD LUCK!! ü§£ü§£ü§£ü§£ https://t.co/pc4FYSEscE   \n",
       "32502   TRIGGER WARNING: Beware before reading my new @Townhallcom column, \"Dating Tips For Prominent Democrats.\" It yields maximum kurtness.  üí•üî•üí•üî•üí•üî•üí•ü§∑üèº‚Äç‚ôÇÔ∏èüî•üí•üî•üí•üî•üí•  https://t.co/9EGBRNlw1j   \n",
       "186365                                         RT @BNPPARIBASOPEN: How does @EVesnina001 lose her #BNPPO17 üèÜ midway through her champion's press conference?\n",
       "\n",
       "üé•üòÇ‚û°Ô∏è https://t.co/RbDT7nlRt7   \n",
       "303189                                                                                  RT @funsized411: üíØüíØüíØüíØüíØ RT @TheMisterMarcus: I'm okay with Duke losing even if it ruins my bracket.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                  content_demoji  \\\n",
       "232301                                                                                                              RT DebAlwaystrump: Congress looking CORRUPT:hot springs: 2 America:angry face:thinks we are STUPID:right arrow curving down: MUELLER is a DIRTY COP:police car light: Russia was 4 ‚Ä¶ https://t.co/sVXXXBIjSh   \n",
       "203651                                                                                                                                            RT Laura_K69: GOOD LUCK!! :rolling on the floor laughing::rolling on the floor laughing::rolling on the floor laughing::rolling on the floor laughing: https://t.co/pc4FYSEscE   \n",
       "32502   TRIGGER WARNING: Beware before reading my new @Townhallcom column, \"Dating Tips For Prominent Democrats.\" It yields maximum kurtness.  :collision::fire::collision::fire::collision::fire::collision::man shrugging: medium-light skin tone::fire::collision::fire::collision::fire::collision:  https://t.co/9EGBRNlw1j   \n",
       "186365                                                                                                                       RT @BNPPARIBASOPEN: How does @EVesnina001 lose her #BNPPO17 :trophy: midway through her champion's press conference?\\n\\n:movie camera::face with tears of joy::right arrow: https://t.co/RbDT7nlRt7   \n",
       "303189                                                                                                                                             RT @funsized411: :hundred points::hundred points::hundred points::hundred points::hundred points: RT @TheMisterMarcus: I'm okay with Duke losing even if it ruins my bracket.   \n",
       "\n",
       "                                                                                                                                                        content_no_emoji  \\\n",
       "232301                           RT DebAlwaystrump: Congress looking CORRUPT 2 Americathinks we are STUPID MUELLER is a DIRTY COP Russia was 4 ‚Ä¶ https://t.co/sVXXXBIjSh   \n",
       "203651                                                                                                                RT Laura_K69: GOOD LUCK!!  https://t.co/pc4FYSEscE   \n",
       "32502   TRIGGER WARNING: Beware before reading my new @Townhallcom column, \"Dating Tips For Prominent Democrats.\" It yields maximum kurtness.    https://t.co/9EGBRNlw1j   \n",
       "186365                          RT @BNPPARIBASOPEN: How does @EVesnina001 lose her #BNPPO17  midway through her champion's press conference?\\n\\n https://t.co/RbDT7nlRt7   \n",
       "303189                                                                     RT @funsized411:  RT @TheMisterMarcus: I'm okay with Duke losing even if it ruins my bracket.   \n",
       "\n",
       "        emoji_count  \n",
       "232301            4  \n",
       "203651            4  \n",
       "32502            14  \n",
       "186365            4  \n",
       "303189            5  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['emoji_count'] > 3, ['content', 'content_demoji', 'content_no_emoji', 'emoji_count']].sample(5, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2534d6-e6f7-4698-ada0-3781ef116bc4",
   "metadata": {},
   "source": [
    "# 3 - Choose Dataset Fields and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f8aea-48f4-4e0a-8dac-0b326522b38a",
   "metadata": {},
   "source": [
    "## 3.1 - Set Args\n",
    "\n",
    "To make the subsequent encoding/training code more modular, set as many args as we can within this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5310825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 - Define model choices and their respective classes.\n",
    "pretrained_models = {\n",
    "    'bert-base-uncased': {\n",
    "        'name': 'bert-base-uncased',          # https://huggingface.co/bert-base-uncased\n",
    "        'tokenizer': BertTokenizerFast,\n",
    "        'model': BertForSequenceClassification,\n",
    "    },\n",
    "    'distilbert-base-uncased': {                    # https://huggingface.co/distilbert-base-uncased\n",
    "        'name': 'distilbert-base-uncased',\n",
    "        'tokenizer': DistilBertTokenizerFast,\n",
    "        'model': BertForSequenceClassification,\n",
    "    },\n",
    "    'roberta-base': {                               # https://huggingface.co/roberta-base\n",
    "        'name': 'roberta-base',\n",
    "        'tokenizer': RobertaTokenizerFast,          # note: roberta-base is case sensitive\n",
    "        'model': BertForSequenceClassification,\n",
    "    },\n",
    "    'vinai/bertweet-base': {                        # https://huggingface.co/vinai/bertweet-base\n",
    "        'name': 'vinai/bertweet-base',\n",
    "        'tokenizer': BertweetTokenizer,             # note: bertweet-base is case sensitive\n",
    "        'model': RobertaForSequenceClassification,  # note: this class is chosen by AutoModel\n",
    "    },\n",
    "    'Twitter/twhin-bert-base': {                    # https://huggingface.co/Twitter/twhin-bert-base\n",
    "        'name': 'Twitter/twhin-bert-base',\n",
    "        #'tokenizer': XLMRobertaTokenizerFast,       # twhin-bert's pre-training tokenizer is 'xlm-roberta-base' according to https://arxiv.org/pdf/2209.07562v1.pdf\n",
    "        #'model': BertForSequenceClassification,\n",
    "        'tokenizer': AutoTokenizer,\n",
    "        'model': AutoModelForSequenceClassification,\n",
    "    },\n",
    "    # 'auto_vinai/bertweet-base': {                        # https://huggingface.co/vinai/bertweet-base\n",
    "    #     'name': 'vinai/bertweet-base',\n",
    "    #     'tokenizer': AutoTokenizer,\n",
    "    #     'model': AutoModelForSequenceClassification,\n",
    "    # },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b44ca1a8-709d-4990-a2e8-273ed17ef576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1 - Select the pre-trained model (must be a key in `pretrained_models` above)\n",
    "pretrained_model_choice = 'Twitter/twhin-bert-base'\n",
    "pretrained_model = pretrained_models[pretrained_model_choice]\n",
    "\n",
    "# Step 2 - Choose a descriptive name for this fine-tuning run.\n",
    "#   Suggested format: [model name]-[2*max_tweets_per_class in thousands]k__[YYYY-MM-DD]\n",
    "run_name = \"Twitter__twhin-bert-base-10k__2023-02-22\"\n",
    "\n",
    "# Step 3 - Choose a folder name for where to store the output of the model \n",
    "#   Will be created as a subfolder of `../data/models/`\n",
    "output_dir_name = 'Twitter__twhin-bert-base-10k'\n",
    "\n",
    "# Step 4 - Choose which columns from dataframe will be used.\n",
    "content_column = 'content_no_emoji'\n",
    "class_column = 'class_numeric'          # Assumes: 0=authentic, 1=troll\n",
    "pk_column = 'tweet_id'                  # Used to identify which tweets were used for fine tuning and exclude them from later testing\n",
    "\n",
    "# Step 5A - Choose tokenizer args\n",
    "#   Note: these are passed on to tokenizer object as keyword args\n",
    "common_tokenizer_args = {\n",
    "    'padding': 'max_length', \n",
    "    'truncation': True, \n",
    "    'return_tensors': 'pt', \n",
    "    'max_length': 256,\n",
    "}\n",
    "\n",
    "# Step 5B - Choose model args\n",
    "#   Note: these are passed on to model object as keyword args\n",
    "common_model_args = {\n",
    "    'num_labels': 2,\n",
    "    #'output_hidden_states': True,\n",
    "}\n",
    "\n",
    "# Step 5C - Choose trainer args\n",
    "#   Note: these are passed on to trainer object as keyword args\n",
    "#   Docs: https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments\n",
    "common_train_args = {\n",
    "    #### model output\n",
    "    'run_name': run_name,\n",
    "    'output_dir': f'../data/models/{output_dir_name}',\n",
    "    'save_strategy': 'epoch',\n",
    "    'save_total_limit': 1,\n",
    "    #### training hyperparams\n",
    "    'num_train_epochs': 10,\n",
    "    # 'per_device_train_batch_size': 16,\n",
    "    # 'per_device_eval_batch_size': 16,\n",
    "    # 'warmup_steps': 500,\n",
    "    # 'weight_decay': 0.01,\n",
    "    #### evaluation during training\n",
    "    # 'evaluation_strategy': 'steps',\n",
    "    # 'eval_steps': 100,\n",
    "    'evaluation_strategy': 'epoch',\n",
    "    'logging_strategy': 'epoch',\n",
    "    'log_level': 'warning',\n",
    "}\n",
    "\n",
    "# Step 6 - Choose how many tweets to fine-tune with, and test split fraction size\n",
    "# Notes:\n",
    "#   - Sets maximum tweets (per class) used for fine tuning but not including evaluation (set to None for no limit)\n",
    "#       e.g. If this value is 5000, a maximum of 5000 troll and 5000 authentic tweets will be used\n",
    "#       for a total of 10,000 tweets used for fine tuning. If `test_fraction` is 0.2, then 2,000 additional\n",
    "#       tweets will be used for testing the fine tuned model, so 12,000 total tweets will be ingested.\n",
    "#   - `max_tweets_per_class` is assumed to be less than total number of tweets per class in `df` (or else pandas yells)\n",
    "max_tweets_per_class = 5000\n",
    "test_fraction = 0.20            # within range (0.0, 1.0)\n",
    "\n",
    "# Step 7 - (Optional) Set random seed values for reproducability\n",
    "sampling_random_seed = 42       # for reproducability, and \"the answer to life, the universe, and everything\"\n",
    "train_test_random_seed = 3      # for reproducability, and \"the number of the counting shall be three\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ed522-c079-43f1-8354-b0e9cee13eb2",
   "metadata": {},
   "source": [
    "## 3.2 - Convert Pandas Dataframe to ü§ó Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80f06a07-4e98-4484-b57d-a6ca104ace46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad4e2af189048c5819d9ed364d6a93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'On #MuslimWomensDay, these women are empowering themselves and fighting back against Islamophobia. https://t.co/Y5NXaTHjZi',\n",
       " 'label': 1,\n",
       " 'tweet_id': '846549212082909184'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for model summary we can track how long it took to encode and train\n",
    "time_encoding = None\n",
    "time_training = None\n",
    "\n",
    "# create a view (not a copy) of dataframe\n",
    "if (max_tweets_per_class is None):\n",
    "    df_view = df[[content_column, class_column, pk_column]]\n",
    "else:\n",
    "    n_tweets = int(max_tweets_per_class * (1.0 + test_fraction))     # \"gross up\" the number of tweets ingested (see section 3.1 above)\n",
    "\n",
    "    df_view = pd.concat(\n",
    "        [\n",
    "            df.loc[df[class_column] == 1, [content_column, class_column, pk_column]].sample(n=n_tweets, random_state=sampling_random_seed),\n",
    "            df.loc[df[class_column] == 0, [content_column, class_column, pk_column]].sample(n=n_tweets, random_state=sampling_random_seed)\n",
    "        ], \n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "# convert to ü§ó Dataset object\n",
    "dataset = Dataset.from_pandas(df_view) \\\n",
    "            .rename_columns({content_column: \"text\", class_column: \"label\"}) \\\n",
    "            .cast_column(\"label\", ClassLabel(names=['authentic', 'troll']))\n",
    "\n",
    "# check results\n",
    "assert (dataset.features['label'].str2int('authentic') == 0) and (dataset.features['label'].str2int('troll') == 1), 'class labels mismatched'\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e06bc37-5ce9-4bf0-90d8-c13f5fc5ec01",
   "metadata": {},
   "source": [
    "## 3.3 - Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2cac37a-a997-4564-85ea-a6677b5b6b47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'tweet_id'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'tweet_id'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if (max_tweets_per_class is None):\n",
    "    test_size = test_fraction\n",
    "else:\n",
    "    test_size = int(max_tweets_per_class * test_fraction * 2)   # \"2\" for our two classes\n",
    "\n",
    "dataset_split = dataset.train_test_split(\n",
    "    test_size=test_size,\n",
    "    shuffle=True,\n",
    "    seed=train_test_random_seed,\n",
    "    stratify_by_column='label'\n",
    ")\n",
    "\n",
    "# check output\n",
    "dataset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef6dcbe-52de-481b-9793-c3ff35c7f7bc",
   "metadata": {},
   "source": [
    "## 3.4 - Tokenize / Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "186a18b1-af51-41be-aeba-0cdcb2d3924b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the tokenizer to prepare text for model\n",
    "#tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "tokenizer = pretrained_model['tokenizer'].from_pretrained(pretrained_model['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a2e8536-27a8-4c74-8127-dcc5b02ad3e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a tokenizer function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], **common_tokenizer_args)\n",
    "\n",
    "# todo -> convert to pure function\n",
    "#   Use (next cell): tokenized_datasets = dataset_split.map(tokenize_function, batched=True, fn_kwargs={'tokenizer': tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ebc8519-f873-47dc-9e5f-7f44c9e13aa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb9d3b99d94446fa03ed071688239dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b9f59e258642259be49ac26dcbfad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_encoding_start = pd.Timestamp.now()\n",
    "\n",
    "# encode the training and test sets\n",
    "#tokenized_datasets = dataset_split.map(tokenize_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "tokenized_datasets = dataset_split.map(tokenize_function, batched=True)\n",
    "\n",
    "time_encoding_stop = pd.Timestamp.now()\n",
    "time_encoding = time_encoding_stop - time_encoding_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e70390c7-fcb5-4854-87f2-1a8c3f505e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding duration: 0 days 00:00:03.593261\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'tweet_id', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'tweet_id', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoding duration:\", str(time_encoding), end=\"\\n\\n\")\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639126b-38f2-4716-8582-7cb074dd28a8",
   "metadata": {},
   "source": [
    "## 3.5 - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f73d471-1726-4c7c-b6f7-3a7c19f63781",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Twitter/twhin-bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Twitter/twhin-bert-base and are newly initialized: ['classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = pretrained_model['model'].from_pretrained(\n",
    "    pretrained_model['name'],\n",
    "    **common_model_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ea71de9-cc92-4ecf-9332-3b8ec0f70b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    **common_train_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "261b4f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup training / evaluation metric\n",
    "#   Docs: https://huggingface.co/docs/evaluate/package_reference/main_classes#evaluate.combine\n",
    "#   Each of these metrics corresponds to a script from huggingface, below are the links for each script.\n",
    "#       accuracy:       https://huggingface.co/spaces/evaluate-metric/accuracy\n",
    "#       f1:             https://huggingface.co/spaces/evaluate-metric/f1\n",
    "#       precision:      https://huggingface.co/spaces/evaluate-metric/precision\n",
    "#       recall:         https://huggingface.co/spaces/evaluate-metric/recall\n",
    "#       roc_auc:        https://huggingface.co/spaces/evaluate-metric/roc_auc\n",
    "#       brier_score:    https://huggingface.co/spaces/evaluate-metric/brier_score\n",
    "metric_list = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc', 'brier_score']\n",
    "\n",
    "metric = evaluate.combine(evaluations=metric_list)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, prediction_scores=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3320b855-e785-425b-8c4a-bef53d0b462c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Justin\\.envs\\tf290_env\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5733e19cda148b9bb8e7bea72354fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4222, 'learning_rate': 4.5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d28726866c47a98ef683277842a185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35998088121414185, 'eval_accuracy': 0.826, 'eval_f1': 0.8042744656917885, 'eval_precision': 0.9190231362467867, 'eval_recall': 0.715, 'eval_roc_auc': 0.826, 'eval_brier_score': 0.174, 'eval_runtime': 18.8461, 'eval_samples_per_second': 106.123, 'eval_steps_per_second': 13.265, 'epoch': 1.0}\n",
      "{'loss': 0.3368, 'learning_rate': 4e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2015773aeae04893a6432a357d2a246c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3480275273323059, 'eval_accuracy': 0.8635, 'eval_f1': 0.8666340986809966, 'eval_precision': 0.8471824259789876, 'eval_recall': 0.887, 'eval_roc_auc': 0.8635, 'eval_brier_score': 0.1365, 'eval_runtime': 18.76, 'eval_samples_per_second': 106.61, 'eval_steps_per_second': 13.326, 'epoch': 2.0}\n",
      "{'loss': 0.2612, 'learning_rate': 3.5e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dba71fcf26b4c8bb8b7f0829dba0c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5388237237930298, 'eval_accuracy': 0.8645, 'eval_f1': 0.8724705882352941, 'eval_precision': 0.824, 'eval_recall': 0.927, 'eval_roc_auc': 0.8645, 'eval_brier_score': 0.1355, 'eval_runtime': 19.0788, 'eval_samples_per_second': 104.829, 'eval_steps_per_second': 13.104, 'epoch': 3.0}\n",
      "{'loss': 0.1799, 'learning_rate': 3e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf50d01ec2c4820983e9e918a718d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5766094326972961, 'eval_accuracy': 0.862, 'eval_f1': 0.8688212927756653, 'eval_precision': 0.8278985507246377, 'eval_recall': 0.914, 'eval_roc_auc': 0.8620000000000001, 'eval_brier_score': 0.138, 'eval_runtime': 18.9722, 'eval_samples_per_second': 105.417, 'eval_steps_per_second': 13.177, 'epoch': 4.0}\n",
      "{'loss': 0.1647, 'learning_rate': 2.5e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a2fd5634dc46d5af77f2074d2f93e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5544945001602173, 'eval_accuracy': 0.856, 'eval_f1': 0.8498435870698644, 'eval_precision': 0.8877995642701525, 'eval_recall': 0.815, 'eval_roc_auc': 0.856, 'eval_brier_score': 0.144, 'eval_runtime': 18.8471, 'eval_samples_per_second': 106.117, 'eval_steps_per_second': 13.265, 'epoch': 5.0}\n",
      "{'loss': 0.1314, 'learning_rate': 2e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7831ce37642841028318fa2ddd287e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6270579099655151, 'eval_accuracy': 0.8545, 'eval_f1': 0.8459502382212811, 'eval_precision': 0.8987626546681665, 'eval_recall': 0.799, 'eval_roc_auc': 0.8544999999999999, 'eval_brier_score': 0.1455, 'eval_runtime': 19.3326, 'eval_samples_per_second': 103.452, 'eval_steps_per_second': 12.932, 'epoch': 6.0}\n",
      "{'loss': 0.0875, 'learning_rate': 1.5e-05, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50b350899644cea9d30936c5ca520f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7378624081611633, 'eval_accuracy': 0.8595, 'eval_f1': 0.8559712967708867, 'eval_precision': 0.8780231335436383, 'eval_recall': 0.835, 'eval_roc_auc': 0.8594999999999999, 'eval_brier_score': 0.1405, 'eval_runtime': 19.5079, 'eval_samples_per_second': 102.523, 'eval_steps_per_second': 12.815, 'epoch': 7.0}\n",
      "{'loss': 0.0524, 'learning_rate': 1e-05, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da24cbde814f4e6dac0b0fc8d1e76221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7302846908569336, 'eval_accuracy': 0.862, 'eval_f1': 0.8575851393188852, 'eval_precision': 0.8859275053304904, 'eval_recall': 0.831, 'eval_roc_auc': 0.862, 'eval_brier_score': 0.138, 'eval_runtime': 19.5306, 'eval_samples_per_second': 102.403, 'eval_steps_per_second': 12.8, 'epoch': 8.0}\n",
      "{'loss': 0.0298, 'learning_rate': 5e-06, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0eead720fe408c909cc2bb6c49dd0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8355393409729004, 'eval_accuracy': 0.866, 'eval_f1': 0.863265306122449, 'eval_precision': 0.88125, 'eval_recall': 0.846, 'eval_roc_auc': 0.866, 'eval_brier_score': 0.134, 'eval_runtime': 19.5477, 'eval_samples_per_second': 102.314, 'eval_steps_per_second': 12.789, 'epoch': 9.0}\n",
      "{'loss': 0.0199, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fda07e338c402d87d22442622b403a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8467128872871399, 'eval_accuracy': 0.865, 'eval_f1': 0.86404833836858, 'eval_precision': 0.8701825557809331, 'eval_recall': 0.858, 'eval_roc_auc': 0.865, 'eval_brier_score': 0.135, 'eval_runtime': 19.4782, 'eval_samples_per_second': 102.679, 'eval_steps_per_second': 12.835, 'epoch': 10.0}\n",
      "{'train_runtime': 3900.2754, 'train_samples_per_second': 25.639, 'train_steps_per_second': 3.205, 'train_loss': 0.1685830812072754, 'epoch': 10.0}\n",
      "\n",
      "Training duration: 0 days 01:05:00.996086\n"
     ]
    }
   ],
   "source": [
    "time_training_start = pd.Timestamp.now()\n",
    "\n",
    "# setup the trainer\n",
    "#   Docs: https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# execute the training\n",
    "trainer.train()\n",
    "\n",
    "time_training_stop = pd.Timestamp.now()\n",
    "time_training = time_training_stop - time_training_start\n",
    "\n",
    "print(\"\\nTraining duration:\", str(time_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5676214e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>\n",
      "<class 'transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a3fc9-b859-4657-b052-c111441e78ab",
   "metadata": {},
   "source": [
    "## 3.6 - Save fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96d4890f-d58f-4041-941f-45450005b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()    # defaults to self.args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d84feef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to delete '../data/models/Twitter__twhin-bert-base-10k/checkpoint-12500' ... success\n"
     ]
    }
   ],
   "source": [
    "# optional - delete checkpoint directories\n",
    "checkpoint_dirs = [\n",
    "    f\"{trainer.args.output_dir}/{directory}\"\n",
    "    for directory in os.listdir(trainer.args.output_dir)\n",
    "        if (\n",
    "            os.path.isdir(os.path.join(trainer.args.output_dir, directory))\n",
    "            and\n",
    "            directory.startswith('checkpoint')\n",
    "        )\n",
    "]\n",
    "\n",
    "for checkpoint_dir in checkpoint_dirs:\n",
    "    print(f\"Attempting to delete '{checkpoint_dir}' ...\", end='')\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    print(f\" success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a9895-0c0b-4d04-b200-729fbd7ba87d",
   "metadata": {},
   "source": [
    "## 3.7 - Evaluate fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aad91a54-6809-4920-b63d-0fd396247d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4361e19efc614c4ebf5d0e883b820c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca07b858843400fb0c2ff8926fdeba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      " {'final_train_loss': 0.014721373096108437, 'final_train_accuracy': 0.9979, 'final_train_f1': 0.997899789978998, 'final_train_precision': 0.997999599919984, 'final_train_recall': 0.9978, 'final_train_roc_auc': 0.9979, 'final_train_brier_score': 0.0021, 'final_train_runtime': 92.1231, 'final_train_samples_per_second': 108.55, 'final_train_steps_per_second': 13.569, 'epoch': 10.0}\n",
      "\n",
      "Test/Eval Metrics:\n",
      " {'final_test_loss': 0.8467128872871399, 'final_test_accuracy': 0.865, 'final_test_f1': 0.86404833836858, 'final_test_precision': 0.8701825557809331, 'final_test_recall': 0.858, 'final_test_roc_auc': 0.865, 'final_test_brier_score': 0.135, 'final_test_runtime': 18.4041, 'final_test_samples_per_second': 108.672, 'final_test_steps_per_second': 13.584, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# if evaluating immediately after fine-tuning\n",
    "#   Note: for current step, not using separate dataset for eval/test\n",
    "train_metrics = trainer.evaluate(eval_dataset=tokenized_datasets['train'], metric_key_prefix='final_train')\n",
    "test_metrics = trainer.evaluate(eval_dataset=tokenized_datasets['test'], metric_key_prefix='final_test')\n",
    "\n",
    "print(\"Training Metrics:\\n\", train_metrics)\n",
    "print(\"\\nTest/Eval Metrics:\\n\", test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "618da4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1.0 | Training loss:  0.4222\n",
      "Epoch:   2.0 | Training loss:  0.3368\n",
      "Epoch:   3.0 | Training loss:  0.2612\n",
      "Epoch:   4.0 | Training loss:  0.1799\n",
      "Epoch:   5.0 | Training loss:  0.1647\n",
      "Epoch:   6.0 | Training loss:  0.1314\n",
      "Epoch:   7.0 | Training loss:  0.0875\n",
      "Epoch:   8.0 | Training loss:  0.0524\n",
      "Epoch:   9.0 | Training loss:  0.0298\n",
      "Epoch:  10.0 | Training loss:  0.0199\n"
     ]
    }
   ],
   "source": [
    "for log_item in trainer.state.log_history:\n",
    "    training_loss = log_item.get('loss', None)\n",
    "    if (training_loss is not None):\n",
    "        print(f\"Epoch: {log_item['epoch']:>5.1f}\", end=\" \")     # \"Epoch:   1.0 \"\n",
    "        print(f\"| Training loss: {training_loss:>7.4f}\")        # \"| Training loss:  0.1234\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd52d7bb",
   "metadata": {},
   "source": [
    "## 3.8 - Output key data as JSON"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0636eacb",
   "metadata": {},
   "source": [
    "### 3.8.1 - Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0d9958b-4320-403c-a6d4-1e5c3c5b6030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3542d485bdd4575bab57c5009d73d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make predictions\n",
    "predictions = trainer.predict(tokenized_datasets['test'])\n",
    "\n",
    "# predictions output is a subclass of NamedTuple, so to save to JSON we convert to a dict first\n",
    "#   Note: according to python docs, the leading underscore is to avoid name conflicts, not as the usual \"discouraged from use\" meaning\n",
    "#   Source: https://docs.python.org/3.10/library/collections.html#collections.somenamedtuple._asdict\n",
    "predictions_dict = predictions._asdict()\n",
    "\n",
    "predictions_dict['run_name'] = run_name\n",
    "predictions_dict['predictions'] = predictions_dict['predictions'].tolist()\n",
    "predictions_dict['label_ids'] = predictions_dict['label_ids'].tolist()\n",
    "\n",
    "# sort the keys\n",
    "dict_order = ['run_name', 'metrics', 'predictions', 'label_ids']\n",
    "predictions_dict = {key: predictions_dict[key] for key in dict_order}\n",
    "\n",
    "# save predictions to `output_dir`\n",
    "predictions_filename = trainer.args.output_dir + '/predictions.json'\n",
    "with open(predictions_filename, mode='w', encoding='utf-8') as fp:\n",
    "    json.dump(predictions_dict, fp, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b3456e6",
   "metadata": {},
   "source": [
    "### 3.8.2 - Fine Tuning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bd361f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record all the fine-tuning parameters in a dict to be saved as JSON later (not used in actual fine-tuning code)\n",
    "fine_tune_params_dict = {\n",
    "    'run_name': run_name,\n",
    "    'output_dir_name': output_dir_name,\n",
    "    'content_column': content_column,\n",
    "    'class_column': class_column,\n",
    "    'pk_column': pk_column,\n",
    "    'pretrained_model': {\n",
    "        'name': pretrained_model['name'],\n",
    "        'tokenizer': pretrained_model['tokenizer'].__name__,\n",
    "        'model': pretrained_model['model'].__name__,\n",
    "    },\n",
    "    'common_tokenizer_args': common_tokenizer_args,\n",
    "    'common_model_args': common_model_args,\n",
    "    'common_train_args': common_train_args,\n",
    "    'max_tweets_per_class': max_tweets_per_class,\n",
    "    'sampling_random_seed': sampling_random_seed,\n",
    "    'train_test_random_seed': train_test_random_seed,\n",
    "    'test_fraction': test_fraction,\n",
    "    'encoding_duration': str(time_encoding),\n",
    "    'training_duration': str(time_training),\n",
    "}\n",
    "\n",
    "# save fine tuning params to `output_dir`\n",
    "fine_tune_params_filename = trainer.args.output_dir + '/fine_tune_params.json'\n",
    "with open(fine_tune_params_filename, mode='w', encoding='utf-8') as fp:\n",
    "    json.dump(fine_tune_params_dict, fp, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9eccb55f",
   "metadata": {},
   "source": [
    "### 3.8.3 - Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64ce7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics: list = trainer.state.log_history   # same info as what is output at the end of each training epoch\n",
    "\n",
    "# save metrics to `output_dir`\n",
    "model_metrics_filename = trainer.args.output_dir + '/model_metrics.json'\n",
    "with open(model_metrics_filename, mode='w', encoding='utf-8') as fp:\n",
    "    json.dump(model_metrics, fp, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a6b46cf",
   "metadata": {},
   "source": [
    "### 3.8.4 - Tweets used for fine tuning\n",
    "\n",
    "We'll want to exclude these tweets from being used in downstream training/evaluation tasks. Each tweet has a unique `tweet_id` value, stored conceptually as a 64-bit integer but physically as a string. We noted down the `tweet_id` values in Section 3.2 so let's store them to a JSON file here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "814254fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_id_list = df_view[pk_column].to_list()\n",
    "\n",
    "fine_tune_tweet_ids_filename = trainer.args.output_dir + '/fine_tune_tweet_ids.json'\n",
    "with open(fine_tune_tweet_ids_filename, mode='w', encoding='utf-8') as fp:\n",
    "    json.dump(tweet_id_list, fp, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "217a5e37",
   "metadata": {},
   "source": [
    "## 3.9 - Zip Model\n",
    "\n",
    "Based on code from: https://realpython.com/python-zipfile/#creating-populating-and-extracting-your-own-zip-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "497fda76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"../data/models/\"\n",
    "zip_filename = pathlib.Path(model_directory, f\"{run_name}.zip\")\n",
    "\n",
    "dir_to_zip = pathlib.Path(trainer.args.output_dir)\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, mode=\"w\", compression=zipfile.ZIP_DEFLATED, compresslevel=6) as archive:\n",
    "    for file_path in dir_to_zip.rglob(\"*\"):\n",
    "        archive.write(file_path, arcname=file_path.relative_to(dir_to_zip))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m103"
  },
  "kernelspec": {
   "display_name": "tf290_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "65a6dd32d71ed4a2e5ac9ab3f52d3aeee49f01a00467a63b19dc274a1d27154b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
