{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add0cef4-a4de-47c5-a9a0-e6b818d6fa2d",
   "metadata": {},
   "source": [
    "# Tweet Turing Test: Detecting Disinformation on Twitter  \n",
    "\n",
    "|          | Group #2 - Disinformation Detectors                     |\n",
    "|---------:|---------------------------------------------------------|\n",
    "| Members  | John Johnson, Katy Matulay, Justin Minnion, Jared Rubin |\n",
    "| Notebook | `xx_fine_tuner.ipynb`                                   |\n",
    "| Purpose  | A notebook to fine-tune BERT models.                    |\n",
    "\n",
    "(todo: description)\n",
    "\n",
    "Based on tutorial from: https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a7349-cf9c-461b-aebd-de5542e2ded8",
   "metadata": {},
   "source": [
    "# 1 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c0673b-7027-407e-8435-e29090105df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports from Python standard library\n",
    "import os\n",
    "\n",
    "# imports requiring installation\n",
    "#   connection to Google Cloud Storage\n",
    "from google.cloud import storage            # pip install google-cloud-storage\n",
    "from google.oauth2 import service_account   # pip install google-auth\n",
    "\n",
    "#  data science packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 🤗 (huggingface) packages\n",
    "import evaluate\n",
    "from datasets import Dataset, ClassLabel\n",
    "from transformers import AutoTokenizer, BertTokenizerFast, BertweetTokenizer, DistilBertTokenizerFast, RobertaTokenizerFast, XLMRobertaTokenizerFast\n",
    "from transformers import AutoModelForSequenceClassification, BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7494318-b0ee-4452-80fa-c959050a6c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports from tweet_turing.py\n",
    "import tweet_turing as tur      # note - different import approach from prior notebooks\n",
    "\n",
    "# imports from tweet_turing_paths.py\n",
    "from tweet_turing_paths import local_data_paths, local_snapshot_paths, gcp_data_paths, \\\n",
    "    gcp_snapshot_paths, gcp_project_name, gcp_bucket_name, gcp_key_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eee18cdf-9d6c-4315-90ad-3e470718b1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pandas options\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7124b0f-b27f-450a-ba49-f0560c5b38b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Local or Cloud?\n",
    "\n",
    "Decide here whether to run notebook with local data or GCP bucket data\n",
    " - if the working directory of this notebook has a \"../data/\" folder with data loaded (e.g. working on local computer or have data files loaded to a cloud VM) then use the \"local files\" option and comment out the \"gcp bucket files\" option\n",
    " - if this notebook is being run from a GCP VM (preferrably in the `us-central1` location) then use the \"gcp bucket files\" option and comment out the \"local files\" option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d579d5d-f289-4b49-a1aa-f72f04d503b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# option: local files\n",
    "local_or_cloud: str = \"local\"   # comment/uncomment this line or next\n",
    "\n",
    "# option: gcp bucket files\n",
    "#local_or_cloud: str = \"cloud\"   # comment/uncomment this line or previous\n",
    "\n",
    "# don't comment/uncomment for remainder of cell\n",
    "if (local_or_cloud == \"local\"):\n",
    "    data_paths = local_data_paths\n",
    "    snapshot_paths = local_snapshot_paths\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    data_paths = gcp_data_paths\n",
    "    snapshot_paths = gcp_snapshot_paths\n",
    "else:\n",
    "    raise ValueError(\"Variable 'local_or_cloud' can only take on one of two values, 'local' or 'cloud'.\")\n",
    "    # subsequent cells will not do this final \"else\" check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b206dfc-7e39-49a1-a48d-71a4cb46e863",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell only needs to run its code if local_or_cloud==\"cloud\"\n",
    "#   (though it is harmless if run when local_or_cloud==\"local\")\n",
    "gcp_storage_client: storage.Client = None\n",
    "gcp_bucket: storage.Bucket = None\n",
    "\n",
    "if (local_or_cloud == \"cloud\"):\n",
    "    gcp_storage_client = tur.get_gcp_storage_client(project_name=gcp_project_name, key_file=gcp_key_file)\n",
    "    gcp_bucket = tur.get_gcp_bucket(storage_client=gcp_storage_client, bucket_name=gcp_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7ee9f-857e-4c69-bd3f-803a970c3199",
   "metadata": {},
   "source": [
    "# 2 - Load Dataset\n",
    "\n",
    "Starting with the ten-percent sample with NLP-preprocessing completed from notebook **`04_nlp_preprocess.ipynb`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47e19c3b-cab5-4e4c-a6b8-950f4c0dbc58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note this cell requires package `pyarrow` to be installed in environment\n",
    "parq_filename: str = \"data_sample_ten_percent_NLP_preprocessed.parquet.gz\"\n",
    "parq_path: str = f\"{snapshot_paths['parq_snapshot']}{parq_filename}\"\n",
    "\n",
    "if (local_or_cloud == \"local\"):\n",
    "    df = pd.read_parquet(parq_path, engine='pyarrow')\n",
    "elif (local_or_cloud == \"cloud\"):\n",
    "    df = tur.get_gcp_object_from_parq_as_df(bucket=gcp_bucket, object_name=parq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "448511ca-787d-4dc8-90d7-8a67332bc1fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external_author_id</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>region</th>\n",
       "      <th>language</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>updates</th>\n",
       "      <th>post_type</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>...</th>\n",
       "      <th>has_url</th>\n",
       "      <th>emoji_text</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>class</th>\n",
       "      <th>following_ratio</th>\n",
       "      <th>class_numeric</th>\n",
       "      <th>RUS_lett_count</th>\n",
       "      <th>content_demoji</th>\n",
       "      <th>content_no_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23785050</td>\n",
       "      <td>radiowoody</td>\n",
       "      <td>To live dangerously on Friday the 13th, we're doing the radio show from the UNLUCKIEST place on earth! The @TennesseeTitans Locker Room!</td>\n",
       "      <td>Nashville Tennessee</td>\n",
       "      <td>en</td>\n",
       "      <td>2585</td>\n",
       "      <td>5710</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-12-13 10:03:43+00:00</td>\n",
       "      <td>Verified</td>\n",
       "      <td>0.452635</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>To live dangerously on Friday the 13th, we're doing the radio show from the UNLUCKIEST place on earth! The @TennesseeTitans Locker Room!</td>\n",
       "      <td>To live dangerously on Friday the 13th, we're doing the radio show from the UNLUCKIEST place on earth! The @TennesseeTitans Locker Room!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59020162</td>\n",
       "      <td>matthewpouliot</td>\n",
       "      <td>@legsanity I like it. Almost like a free Gio. Pujols is still about as good of a bet as Gonzalez the rest of the way.</td>\n",
       "      <td>Florida</td>\n",
       "      <td>en</td>\n",
       "      <td>999</td>\n",
       "      <td>12637</td>\n",
       "      <td>0</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-04-26 20:13:58+00:00</td>\n",
       "      <td>Verified</td>\n",
       "      <td>0.079047</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@legsanity I like it. Almost like a free Gio. Pujols is still about as good of a bet as Gonzalez the rest of the way.</td>\n",
       "      <td>@legsanity I like it. Almost like a free Gio. Pujols is still about as good of a bet as Gonzalez the rest of the way.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1656024374</td>\n",
       "      <td>IMISSOBAMA</td>\n",
       "      <td>Man servants can have a good purpose as long as they come with cash and don't touch me ever.</td>\n",
       "      <td>United States</td>\n",
       "      <td>en</td>\n",
       "      <td>473</td>\n",
       "      <td>760</td>\n",
       "      <td>4122</td>\n",
       "      <td>RETWEET</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-12-24 13:12:00+00:00</td>\n",
       "      <td>Troll</td>\n",
       "      <td>0.621551</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Man servants can have a good purpose as long as they come with cash and don't touch me ever.</td>\n",
       "      <td>Man servants can have a good purpose as long as they come with cash and don't touch me ever.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  external_author_id          author  \\\n",
       "0           23785050      radiowoody   \n",
       "1           59020162  matthewpouliot   \n",
       "2         1656024374      IMISSOBAMA   \n",
       "\n",
       "                                                                                                                                    content  \\\n",
       "0  To live dangerously on Friday the 13th, we're doing the radio show from the UNLUCKIEST place on earth! The @TennesseeTitans Locker Room!   \n",
       "1                     @legsanity I like it. Almost like a free Gio. Pujols is still about as good of a bet as Gonzalez the rest of the way.   \n",
       "2                                              Man servants can have a good purpose as long as they come with cash and don't touch me ever.   \n",
       "\n",
       "                region language  following  followers  updates   post_type  \\\n",
       "0  Nashville Tennessee       en       2585       5710        2         NaN   \n",
       "1              Florida       en        999      12637        0  replied_to   \n",
       "2        United States       en        473        760     4122     RETWEET   \n",
       "\n",
       "   is_retweet  ... has_url emoji_text emoji_count              publish_date  \\\n",
       "0         0.0  ...       0         []           0 2013-12-13 10:03:43+00:00   \n",
       "1         0.0  ...       0         []           0 2015-04-26 20:13:58+00:00   \n",
       "2         1.0  ...       0         []           0 2016-12-24 13:12:00+00:00   \n",
       "\n",
       "      class following_ratio  class_numeric RUS_lett_count  \\\n",
       "0  Verified        0.452635              0              0   \n",
       "1  Verified        0.079047              0              0   \n",
       "2     Troll        0.621551              1              0   \n",
       "\n",
       "                                                                                                                             content_demoji  \\\n",
       "0  To live dangerously on Friday the 13th, we're doing the radio show from the UNLUCKIEST place on earth! The @TennesseeTitans Locker Room!   \n",
       "1                     @legsanity I like it. Almost like a free Gio. Pujols is still about as good of a bet as Gonzalez the rest of the way.   \n",
       "2                                              Man servants can have a good purpose as long as they come with cash and don't touch me ever.   \n",
       "\n",
       "                                                                                                                           content_no_emoji  \n",
       "0  To live dangerously on Friday the 13th, we're doing the radio show from the UNLUCKIEST place on earth! The @TennesseeTitans Locker Room!  \n",
       "1                     @legsanity I like it. Almost like a free Gio. Pujols is still about as good of a bet as Gonzalez the rest of the way.  \n",
       "2                                              Man servants can have a good purpose as long as they come with cash and don't touch me ever.  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96506d5c-cd41-4382-9518-55132bfad5d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 362314 entries, 0 to 362313\n",
      "Data columns (total 24 columns):\n",
      " #   Column              Non-Null Count   Dtype              \n",
      "---  ------              --------------   -----              \n",
      " 0   external_author_id  362314 non-null  string             \n",
      " 1   author              362314 non-null  string             \n",
      " 2   content             362314 non-null  string             \n",
      " 3   region              344249 non-null  string             \n",
      " 4   language            362314 non-null  category           \n",
      " 5   following           362314 non-null  uint64             \n",
      " 6   followers           362314 non-null  uint64             \n",
      " 7   updates             362314 non-null  uint64             \n",
      " 8   post_type           154729 non-null  category           \n",
      " 9   is_retweet          362314 non-null  float64            \n",
      " 10  account_category    362314 non-null  category           \n",
      " 11  tweet_id            362314 non-null  string             \n",
      " 12  tco1_step1          219778 non-null  string             \n",
      " 13  data_source         362314 non-null  category           \n",
      " 14  has_url             362314 non-null  int64              \n",
      " 15  emoji_text          362314 non-null  object             \n",
      " 16  emoji_count         362314 non-null  int64              \n",
      " 17  publish_date        362314 non-null  datetime64[ns, UTC]\n",
      " 18  class               362314 non-null  category           \n",
      " 19  following_ratio     362314 non-null  float64            \n",
      " 20  class_numeric       362314 non-null  int8               \n",
      " 21  RUS_lett_count      362314 non-null  int64              \n",
      " 22  content_demoji      362314 non-null  object             \n",
      " 23  content_no_emoji    362314 non-null  object             \n",
      "dtypes: category(5), datetime64[ns, UTC](1), float64(2), int64(3), int8(1), object(3), string(6), uint64(3)\n",
      "memory usage: 390.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9b1aca6-46d1-472d-8b33-4c830be6aa65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Verified', 'Troll']\n",
       "Categories (2, object): ['Troll', 'Verified']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "679eb1f2-e964-4e2c-8476-2522a5b77649",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_demoji</th>\n",
       "      <th>content_no_emoji</th>\n",
       "      <th>emoji_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>232301</th>\n",
       "      <td>RT DebAlwaystrump: Congress looking CORRUPT♨ 2 America😠thinks we are STUPID⤵ MUELLER is a DIRTY COP🚨 Russia was 4 … https://t.co/sVXXXBIjSh</td>\n",
       "      <td>RT DebAlwaystrump: Congress looking CORRUPT:hot springs: 2 America:angry face:thinks we are STUPID:right arrow curving down: MUELLER is a DIRTY COP:police car light: Russia was 4 … https://t.co/sVXXXBIjSh</td>\n",
       "      <td>RT DebAlwaystrump: Congress looking CORRUPT 2 Americathinks we are STUPID MUELLER is a DIRTY COP Russia was 4 … https://t.co/sVXXXBIjSh</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203651</th>\n",
       "      <td>RT Laura_K69: GOOD LUCK!! 🤣🤣🤣🤣 https://t.co/pc4FYSEscE</td>\n",
       "      <td>RT Laura_K69: GOOD LUCK!! :rolling on the floor laughing::rolling on the floor laughing::rolling on the floor laughing::rolling on the floor laughing: https://t.co/pc4FYSEscE</td>\n",
       "      <td>RT Laura_K69: GOOD LUCK!!  https://t.co/pc4FYSEscE</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32502</th>\n",
       "      <td>TRIGGER WARNING: Beware before reading my new @Townhallcom column, \"Dating Tips For Prominent Democrats.\" It yields maximum kurtness.  💥🔥💥🔥💥🔥💥🤷🏼‍♂️🔥💥🔥💥🔥💥  https://t.co/9EGBRNlw1j</td>\n",
       "      <td>TRIGGER WARNING: Beware before reading my new @Townhallcom column, \"Dating Tips For Prominent Democrats.\" It yields maximum kurtness.  :collision::fire::collision::fire::collision::fire::collision::man shrugging: medium-light skin tone::fire::collision::fire::collision::fire::collision:  https://t.co/9EGBRNlw1j</td>\n",
       "      <td>TRIGGER WARNING: Beware before reading my new @Townhallcom column, \"Dating Tips For Prominent Democrats.\" It yields maximum kurtness.    https://t.co/9EGBRNlw1j</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186365</th>\n",
       "      <td>RT @BNPPARIBASOPEN: How does @EVesnina001 lose her #BNPPO17 🏆 midway through her champion's press conference?\n",
       "\n",
       "🎥😂➡️ https://t.co/RbDT7nlRt7</td>\n",
       "      <td>RT @BNPPARIBASOPEN: How does @EVesnina001 lose her #BNPPO17 :trophy: midway through her champion's press conference?\\n\\n:movie camera::face with tears of joy::right arrow: https://t.co/RbDT7nlRt7</td>\n",
       "      <td>RT @BNPPARIBASOPEN: How does @EVesnina001 lose her #BNPPO17  midway through her champion's press conference?\\n\\n https://t.co/RbDT7nlRt7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303189</th>\n",
       "      <td>RT @funsized411: 💯💯💯💯💯 RT @TheMisterMarcus: I'm okay with Duke losing even if it ruins my bracket.</td>\n",
       "      <td>RT @funsized411: :hundred points::hundred points::hundred points::hundred points::hundred points: RT @TheMisterMarcus: I'm okay with Duke losing even if it ruins my bracket.</td>\n",
       "      <td>RT @funsized411:  RT @TheMisterMarcus: I'm okay with Duke losing even if it ruins my bracket.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                   content  \\\n",
       "232301                                         RT DebAlwaystrump: Congress looking CORRUPT♨ 2 America😠thinks we are STUPID⤵ MUELLER is a DIRTY COP🚨 Russia was 4 … https://t.co/sVXXXBIjSh   \n",
       "203651                                                                                                                              RT Laura_K69: GOOD LUCK!! 🤣🤣🤣🤣 https://t.co/pc4FYSEscE   \n",
       "32502   TRIGGER WARNING: Beware before reading my new @Townhallcom column, \"Dating Tips For Prominent Democrats.\" It yields maximum kurtness.  💥🔥💥🔥💥🔥💥🤷🏼‍♂️🔥💥🔥💥🔥💥  https://t.co/9EGBRNlw1j   \n",
       "186365                                         RT @BNPPARIBASOPEN: How does @EVesnina001 lose her #BNPPO17 🏆 midway through her champion's press conference?\n",
       "\n",
       "🎥😂➡️ https://t.co/RbDT7nlRt7   \n",
       "303189                                                                                  RT @funsized411: 💯💯💯💯💯 RT @TheMisterMarcus: I'm okay with Duke losing even if it ruins my bracket.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                  content_demoji  \\\n",
       "232301                                                                                                              RT DebAlwaystrump: Congress looking CORRUPT:hot springs: 2 America:angry face:thinks we are STUPID:right arrow curving down: MUELLER is a DIRTY COP:police car light: Russia was 4 … https://t.co/sVXXXBIjSh   \n",
       "203651                                                                                                                                            RT Laura_K69: GOOD LUCK!! :rolling on the floor laughing::rolling on the floor laughing::rolling on the floor laughing::rolling on the floor laughing: https://t.co/pc4FYSEscE   \n",
       "32502   TRIGGER WARNING: Beware before reading my new @Townhallcom column, \"Dating Tips For Prominent Democrats.\" It yields maximum kurtness.  :collision::fire::collision::fire::collision::fire::collision::man shrugging: medium-light skin tone::fire::collision::fire::collision::fire::collision:  https://t.co/9EGBRNlw1j   \n",
       "186365                                                                                                                       RT @BNPPARIBASOPEN: How does @EVesnina001 lose her #BNPPO17 :trophy: midway through her champion's press conference?\\n\\n:movie camera::face with tears of joy::right arrow: https://t.co/RbDT7nlRt7   \n",
       "303189                                                                                                                                             RT @funsized411: :hundred points::hundred points::hundred points::hundred points::hundred points: RT @TheMisterMarcus: I'm okay with Duke losing even if it ruins my bracket.   \n",
       "\n",
       "                                                                                                                                                        content_no_emoji  \\\n",
       "232301                           RT DebAlwaystrump: Congress looking CORRUPT 2 Americathinks we are STUPID MUELLER is a DIRTY COP Russia was 4 … https://t.co/sVXXXBIjSh   \n",
       "203651                                                                                                                RT Laura_K69: GOOD LUCK!!  https://t.co/pc4FYSEscE   \n",
       "32502   TRIGGER WARNING: Beware before reading my new @Townhallcom column, \"Dating Tips For Prominent Democrats.\" It yields maximum kurtness.    https://t.co/9EGBRNlw1j   \n",
       "186365                          RT @BNPPARIBASOPEN: How does @EVesnina001 lose her #BNPPO17  midway through her champion's press conference?\\n\\n https://t.co/RbDT7nlRt7   \n",
       "303189                                                                     RT @funsized411:  RT @TheMisterMarcus: I'm okay with Duke losing even if it ruins my bracket.   \n",
       "\n",
       "        emoji_count  \n",
       "232301            4  \n",
       "203651            4  \n",
       "32502            14  \n",
       "186365            4  \n",
       "303189            5  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['emoji_count'] > 3, ['content', 'content_demoji', 'content_no_emoji', 'emoji_count']].sample(5, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2534d6-e6f7-4698-ada0-3781ef116bc4",
   "metadata": {},
   "source": [
    "# 3 - Choose Dataset Fields and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f8aea-48f4-4e0a-8dac-0b326522b38a",
   "metadata": {},
   "source": [
    "## 3.1 - Set Args\n",
    "\n",
    "To make the subsequent encoding/training code more modular, set as many args as we can within this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b44ca1a8-709d-4990-a2e8-273ed17ef576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# descriptive name for this fine-tuning run\n",
    "run_name = \"distilbert-base-unc-2k - 20feb2023\"\n",
    "\n",
    "# where to store the output of the model (as a subfolder of ../data/models/)\n",
    "output_dir_name = 'distilbert-base-unc-2k'\n",
    "\n",
    "# which columns from dataframe will be used\n",
    "content_column = 'content_no_emoji'\n",
    "class_column = 'class_numeric'          # Assumes: 0=authentic, 1=troll\n",
    "pk_column = 'tweet_id'                  # Used to identify which tweets were used for fine tuning and exclude them from later testing\n",
    "\n",
    "# create choices for pre-trained model\n",
    "pretrained_models = {\n",
    "    'bert-base-uncased': {\n",
    "        'name': 'bert-base-uncased',          # https://huggingface.co/bert-base-uncased\n",
    "        'tokenizer': BertTokenizerFast,\n",
    "        'model': BertForSequenceClassification,\n",
    "    },\n",
    "    'distilbert-base-uncased': {                    # https://huggingface.co/distilbert-base-uncased\n",
    "        'name': 'distilbert-base-uncased',\n",
    "        'tokenizer': DistilBertTokenizerFast,\n",
    "        'model': BertForSequenceClassification,\n",
    "    },\n",
    "    'roberta-base': {                               # https://huggingface.co/roberta-base\n",
    "        'name': 'roberta-base',\n",
    "        'tokenizer': RobertaTokenizerFast,          # note: roberta-base is case sensitive\n",
    "        'model': BertForSequenceClassification,\n",
    "    },\n",
    "    'vinai/bertweet-base': {                        # https://huggingface.co/vinai/bertweet-base\n",
    "        'name': 'vinai/bertweet-base',\n",
    "        'tokenizer': BertweetTokenizer,             # note: bertweet-base is case sensitive\n",
    "        'model': BertForSequenceClassification,\n",
    "    },\n",
    "    'Twitter/twhin-bert-base': {                    # https://huggingface.co/Twitter/twhin-bert-base\n",
    "        'name': 'Twitter/twhin-bert-base',\n",
    "        'tokenizer': XLMRobertaTokenizerFast,       # twhin-bert's pre-training tokenizer is 'xlm-roberta-base' according to https://arxiv.org/pdf/2209.07562v1.pdf\n",
    "        'model': BertForSequenceClassification,\n",
    "    },\n",
    "}\n",
    "\n",
    "# select the pre-trained model\n",
    "pretrained_model_choice = 'bert-base-uncased'\n",
    "pretrained_model = pretrained_models[pretrained_model_choice]\n",
    "\n",
    "# these are passed on to tokenizer object as keyword args\n",
    "common_tokenizer_args = {\n",
    "    'padding': 'max_length', \n",
    "    'truncation': True, \n",
    "    'return_tensors': 'pt', \n",
    "    'max_length': 256,\n",
    "}\n",
    "\n",
    "# these are passed on to model object as keyword args\n",
    "common_model_args = {\n",
    "    'num_labels': 2,\n",
    "    #'output_hidden_states': True,\n",
    "}\n",
    "\n",
    "# these are passed on to trainer object as keyword args\n",
    "#   docs: https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments\n",
    "common_train_args = {\n",
    "    #### model output\n",
    "    'run_name': run_name,\n",
    "    'output_dir': f'../data/models/{output_dir_name}',\n",
    "    'save_strategy': 'epoch',\n",
    "    'save_total_limit': 1,\n",
    "    #### training hyperparams\n",
    "    'num_train_epochs': 5,\n",
    "    # 'per_device_train_batch_size': 16,\n",
    "    # 'per_device_eval_batch_size': 16,\n",
    "    # 'warmup_steps': 500,\n",
    "    # 'weight_decay': 0.01,\n",
    "    #### evaluation during training\n",
    "    # 'evaluation_strategy': 'steps',\n",
    "    # 'eval_steps': 100,\n",
    "    'evaluation_strategy': 'epoch',\n",
    "    'logging_strategy': 'epoch',\n",
    "    'log_level': 'warning',\n",
    "}\n",
    "\n",
    "# maximum tweets (per class) used for fine tuning but not including evaluation (set to None for no limit)\n",
    "#  e.g. If this value is 5000, a maximum of 5000 troll and 5000 authentic tweets will be used\n",
    "#       for a total of 10,000 tweets used for fine tuning. If `test_fraction` is 0.2, then 2,000 additional\n",
    "#       tweets will be used for testing the fine tuned model, so 12,000 total tweets will be ingested.\n",
    "max_tweets_per_class = 1000     # assumed to be less than total number of tweets per class in `df` (or else pandas yells)\n",
    "sampling_random_seed = 42\n",
    "\n",
    "# for train/test split\n",
    "train_test_random_seed = 3    # for reproducability, and \"the number of the counting shall be three\"\n",
    "test_fraction = 0.20          # within range (0.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ed522-c079-43f1-8354-b0e9cee13eb2",
   "metadata": {},
   "source": [
    "## 3.2 - Convert Pandas Dataframe to 🤗 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80f06a07-4e98-4484-b57d-a6ca104ace46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd285381fd1348f698dc62fa04a71ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'On #MuslimWomensDay, these women are empowering themselves and fighting back against Islamophobia. https://t.co/Y5NXaTHjZi',\n",
       " 'label': 1,\n",
       " 'tweet_id': '846549212082909184'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for model summary we can track how long it took to encode and train\n",
    "time_encoding = None\n",
    "time_training = None\n",
    "\n",
    "# create a view (not a copy) of dataframe\n",
    "if (max_tweets_per_class is None):\n",
    "    df_view = df[[content_column, class_column]]\n",
    "else:\n",
    "    n_tweets = int(max_tweets_per_class * (1.0 + test_fraction))     # \"gross up\" the number of tweets ingested (see section 3.1 above)\n",
    "\n",
    "    df_view = pd.concat(\n",
    "        [\n",
    "            df.loc[df[class_column] == 1, [content_column, class_column, pk_column]].sample(n=n_tweets, random_state=sampling_random_seed),\n",
    "            df.loc[df[class_column] == 0, [content_column, class_column, pk_column]].sample(n=n_tweets, random_state=sampling_random_seed)\n",
    "        ], \n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "# convert to 🤗 Dataset object\n",
    "dataset = Dataset.from_pandas(df_view) \\\n",
    "            .rename_columns({content_column: \"text\", class_column: \"label\"}) \\\n",
    "            .cast_column(\"label\", ClassLabel(names=['authentic', 'troll']))\n",
    "\n",
    "# check results\n",
    "assert (dataset.features['label'].str2int('authentic') == 0) and (dataset.features['label'].str2int('troll') == 1), 'class labels mismatched'\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e06bc37-5ce9-4bf0-90d8-c13f5fc5ec01",
   "metadata": {},
   "source": [
    "## 3.3 - Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2cac37a-a997-4564-85ea-a6677b5b6b47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'tweet_id'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'tweet_id'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if (max_tweets_per_class is None):\n",
    "    test_size = test_fraction\n",
    "else:\n",
    "    test_size = int(max_tweets_per_class * test_fraction * 2)   # \"2\" for our two classes\n",
    "\n",
    "dataset_split = dataset.train_test_split(\n",
    "    test_size=test_size,\n",
    "    shuffle=True,\n",
    "    seed=train_test_random_seed,\n",
    "    stratify_by_column='label'\n",
    ")\n",
    "\n",
    "# check output\n",
    "dataset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef6dcbe-52de-481b-9793-c3ff35c7f7bc",
   "metadata": {},
   "source": [
    "## 3.4 - Tokenize / Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "186a18b1-af51-41be-aeba-0cdcb2d3924b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the tokenizer to prepare text for model\n",
    "#tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "tokenizer = pretrained_model['tokenizer'].from_pretrained(pretrained_model['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a2e8536-27a8-4c74-8127-dcc5b02ad3e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a tokenizer function\n",
    "def tokenize_function(examples):    # todo -> convert to pure function\n",
    "    return tokenizer(examples['text'], **common_tokenizer_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ebc8519-f873-47dc-9e5f-7f44c9e13aa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd837766b9a45228f54bdba8623bd43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05df9f6a7264e2c83618549bd02db33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_encoding_start = pd.Timestamp.now()\n",
    "\n",
    "# encode the training and test sets\n",
    "#tokenized_datasets = dataset_split.map(tokenize_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "tokenized_datasets = dataset_split.map(tokenize_function, batched=True)\n",
    "\n",
    "time_encoding_stop = pd.Timestamp.now()\n",
    "time_encoding = time_encoding_stop - time_encoding_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e70390c7-fcb5-4854-87f2-1a8c3f505e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 days 00:00:00.839275\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'tweet_id', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'tweet_id', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(time_encoding)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639126b-38f2-4716-8582-7cb074dd28a8",
   "metadata": {},
   "source": [
    "## 3.5 - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f73d471-1726-4c7c-b6f7-3a7c19f63781",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     pretrained_model_name,\n",
    "#     **extra_model_args,\n",
    "# )\n",
    "\n",
    "model = pretrained_model['model'].from_pretrained(\n",
    "    pretrained_model['name'],\n",
    "    **common_model_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ea71de9-cc92-4ecf-9332-3b8ec0f70b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    **common_train_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "261b4f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup training / evaluation metric\n",
    "#   Docs: https://huggingface.co/docs/evaluate/package_reference/main_classes#evaluate.combine\n",
    "#   Each of these metrics corresponds to a script from huggingface, below are the links for each script.\n",
    "#       accuracy:   https://huggingface.co/spaces/evaluate-metric/accuracy\n",
    "#       f1:         https://huggingface.co/spaces/evaluate-metric/f1\n",
    "#       precision:  https://huggingface.co/spaces/evaluate-metric/precision\n",
    "#       recall:     https://huggingface.co/spaces/evaluate-metric/recall\n",
    "metric_list = ['accuracy', 'f1', 'precision', 'recall']\n",
    "\n",
    "metric = evaluate.combine(evaluations=metric_list)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3320b855-e785-425b-8c4a-bef53d0b462c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Justin\\.envs\\tf290_env\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3287ae0334b54db8aa46462b0603ade5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5135, 'learning_rate': 4e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f723d25aa8ed413485d8cb2da0e0ff82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.49622851610183716, 'eval_accuracy': 0.795, 'eval_f1': 0.8093023255813954, 'eval_precision': 0.7565217391304347, 'eval_recall': 0.87, 'eval_runtime': 3.7264, 'eval_samples_per_second': 107.342, 'eval_steps_per_second': 13.418, 'epoch': 1.0}\n",
      "{'loss': 0.3693, 'learning_rate': 3e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef7b5ca84754d7eabfe06e6994b6a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5828663110733032, 'eval_accuracy': 0.805, 'eval_f1': 0.8133971291866028, 'eval_precision': 0.7798165137614679, 'eval_recall': 0.85, 'eval_runtime': 3.7277, 'eval_samples_per_second': 107.306, 'eval_steps_per_second': 13.413, 'epoch': 2.0}\n",
      "{'loss': 0.2102, 'learning_rate': 2e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247db4f9b2394addb87834c4191d4f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.798835813999176, 'eval_accuracy': 0.825, 'eval_f1': 0.8356807511737089, 'eval_precision': 0.7876106194690266, 'eval_recall': 0.89, 'eval_runtime': 3.6724, 'eval_samples_per_second': 108.921, 'eval_steps_per_second': 13.615, 'epoch': 3.0}\n",
      "{'loss': 0.071, 'learning_rate': 1e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce71af71aaf46d69e40803991c1e0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9535841941833496, 'eval_accuracy': 0.815, 'eval_f1': 0.8140703517587939, 'eval_precision': 0.8181818181818182, 'eval_recall': 0.81, 'eval_runtime': 3.7126, 'eval_samples_per_second': 107.742, 'eval_steps_per_second': 13.468, 'epoch': 4.0}\n",
      "{'loss': 0.0237, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eeef64f70224257bafbb80884cd86d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0114675760269165, 'eval_accuracy': 0.825, 'eval_f1': 0.8167539267015708, 'eval_precision': 0.8571428571428571, 'eval_recall': 0.78, 'eval_runtime': 3.6425, 'eval_samples_per_second': 109.815, 'eval_steps_per_second': 13.727, 'epoch': 5.0}\n",
      "{'train_runtime': 338.8163, 'train_samples_per_second': 29.515, 'train_steps_per_second': 3.689, 'train_loss': 0.2375522357940674, 'epoch': 5.0}\n",
      "0 days 00:05:39.287998\n"
     ]
    }
   ],
   "source": [
    "time_training_start = pd.Timestamp.now()\n",
    "\n",
    "# setup the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# execute the training\n",
    "trainer.train()\n",
    "\n",
    "time_training_stop = pd.Timestamp.now()\n",
    "time_training = time_training_stop - time_training_start\n",
    "\n",
    "print(time_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a3fc9-b859-4657-b052-c111441e78ab",
   "metadata": {},
   "source": [
    "## 3.6 - Save fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96d4890f-d58f-4041-941f-45450005b4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../data/models/distilbert-base-unc-2k\n",
      "Configuration saved in ../data/models/distilbert-base-unc-2k\\config.json\n",
      "Model weights saved in ../data/models/distilbert-base-unc-2k\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()    # defaults to self.args.output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a9895-0c0b-4d04-b200-729fbd7ba87d",
   "metadata": {},
   "source": [
    "## 3.7 - Evaluate fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aad91a54-6809-4920-b63d-0fd396247d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0788fad7af08410eba1b6005c1192994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7843887805938721,\n",
       " 'eval_accuracy': 0.815,\n",
       " 'eval_f1': 0.809278350515464,\n",
       " 'eval_precision': 0.8870056497175142,\n",
       " 'eval_recall': 0.7440758293838863,\n",
       " 'eval_runtime': 3.9316,\n",
       " 'eval_samples_per_second': 101.741,\n",
       " 'eval_steps_per_second': 12.718,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if evaluating immediately after fine-tuning\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0d9958b-4320-403c-a6d4-1e5c3c5b6030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 400\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b2a9c7a7ba4705ad0d2ba2ed2fab7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "536e511d-0687-4387-b40d-4103a9d9a7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.8962348699569702,\n",
       " 'test_accuracy': 0.8,\n",
       " 'test_runtime': 3.8037,\n",
       " 'test_samples_per_second': 105.162,\n",
       " 'test_steps_per_second': 13.145}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "93dc7b2e-be12-44a7-bf28-0d0bf18765f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../data/models/dist-test1/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"../data/models/dist-test1\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ../data/models/dist-test1/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ../data/models/dist-test1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# reload model, if evaluation is being performed separately from training\n",
    "model_dir = \"../data/models/dist-test1\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1df5ba-fffc-4f10-993a-9c2ad795a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - setup eval for re-loaded model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8cd4eb-89fe-47df-9940-e2db3034cb49",
   "metadata": {},
   "source": [
    "TODO: setup way to archive the saved model files.\n",
    "\n",
    "For now:\n",
    "`tar -czvf dist-test1.tar.gz --exclude='*checkpoint*' dist-test1`"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m103"
  },
  "kernelspec": {
   "display_name": "tf290_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "65a6dd32d71ed4a2e5ac9ab3f52d3aeee49f01a00467a63b19dc274a1d27154b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
